
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../s1_computational_graphs/">
      
      
        <link rel="next" href="../s3_immediate_gpu_computation/">
      
      
      <link rel="icon" href="../../figures/nebula.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.3.2">
    
    
      
        <title>S2 - Introduction to GPU's - The Real-Timers Guide to the Computational Galaxy</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.30068a00.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
        html.glightbox-open { overflow: initial; height: 100%; }
        .gslide-title { margin-top: 0px; user-select: text; }
        .gslide-desc { color: #666; user-select: text; }
        .gslide-image img { background: white; }
        
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
            </style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#2-intro-to-gpus" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="The Real-Timers Guide to the Computational Galaxy" class="md-header__button md-logo" aria-label="The Real-Timers Guide to the Computational Galaxy" data-md-component="logo">
      
  <img src="../../figures/nebula.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            The Real-Timers Guide to the Computational Galaxy
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              S2 - Introduction to GPU's
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/absorensen/the-guide" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="The Real-Timers Guide to the Computational Galaxy" class="md-nav__button md-logo" aria-label="The Real-Timers Guide to the Computational Galaxy" data-md-component="logo">
      
  <img src="../../figures/nebula.png" alt="logo">

    </a>
    The Real-Timers Guide to the Computational Galaxy
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/absorensen/the-guide" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../acknowledgements/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Acknowledgements
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m0_introduction/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M0 - Introduction
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_3">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            M0 - Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m0_introduction/s0_intro_to_computing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S0 - Introduction to the Computing Landscape
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m0_introduction/s1_intro_to_rust/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S1 - Introduction to Rust
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m0_introduction/s2_basic_concepts_in_rust/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S2 - Basic Concepts in Rust
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m0_introduction/s3_less_basic_concepts_in_rust/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S3 - 3️⃣ Less Basic Concepts in Rust
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m0_introduction/s4_exercises/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S4 - 4️⃣ Exercises
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M1 - Memory Hierarchies
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_4">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            M1 - Memory Hierarchies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s0_soft_memory_hierarchies/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S0 - Soft Memory Hierarchies
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s1_computational_graphs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S1 - Computational Graphs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    S2 - Introduction to GPU's
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    S2 - Introduction to GPU's
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gpu-hardware" class="md-nav__link">
    GPU Hardware
  </a>
  
    <nav class="md-nav" aria-label="GPU Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer" class="md-nav__link">
    Transfer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#threads-warps-and-blocks" class="md-nav__link">
    Threads, Warps and Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-memory-hierarchy" class="md-nav__link">
    GPU Memory Hierarchy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-introducing-wgpu-and-wgsl" class="md-nav__link">
    3️⃣ Introducing wgpu and wgsl
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-basic-gpu-programming" class="md-nav__link">
    3️⃣ Basic GPU Programming
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-remove-the-loop-where-you-say" class="md-nav__link">
    3️⃣ Remove the loop where, you say?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-coalesced-accessing-and-strides" class="md-nav__link">
    3️⃣ Coalesced Accessing and Strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-divergence-overlap-and-occupancy" class="md-nav__link">
    3️⃣ Divergence, Overlap and Occupancy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-shared-memory-and-synchronization" class="md-nav__link">
    3️⃣ Shared Memory and Synchronization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-further-reading" class="md-nav__link">
    5️⃣ Further Reading
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s3_immediate_gpu_computation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S3 - Immediate GPU Computation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s4_building_a_computational_graph/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S4 - Building a Computational Graph
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s5_computational_graph_compilers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S5 - Computational Graph Compilers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../s6_exercises/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S6 - 4️⃣ Exercises
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m2_concurrency/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M2 - Concurrency
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_5">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            M2 - Concurrency
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s0_data_parallelism/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S0 - Data Parallelism
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s1_threads/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S1 - Threads
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s2_locks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S2 - Locks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s3_atomics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S3 - Atomics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s4_message_passing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S4 - Message Passing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s5_async/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S5 - Async
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s6_events/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S6 - Events
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s7_more_gpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S7 - 3️⃣🔜 More GPU
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s8_branchless_programming/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S8 - 3️⃣🔜 Branchless Programming
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s9_parallel_graphs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S9 - 3️⃣🔜 Parallel Graphs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m2_concurrency/s10_exercises/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S10 - 4️⃣ Exercises
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m3_types/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M3 - Types
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_6">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            M3 - Types
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s0_integers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S0 - Integers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s1_floats/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S1 - Floats
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s2_energy_efficiency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S2 - Energy Efficiency
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s3_bitwise_operations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S3 - Bitwise Operations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s4_bit_tricks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S4 - 3️⃣ Bit Tricks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s5_compression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S5 - 3️⃣ Compression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m3_types/s6_cooperative_matrices/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S6 - 🧬3️⃣ Cooperative Matrices
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m4_optimization/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M4 - 3️⃣ Optimization
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_7">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            M4 - 3️⃣ Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s0_measuring_performance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S0 - Measuring Performance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s1_profilers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S1 - Profilers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s2_neural_network_training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S2 - 🧬3️⃣🔜 Neural Network Training
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s3_neural_network_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S3 - 🧬3️⃣ Neural Network Inference
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s4_path_tracer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S4 - 🧬3️⃣🔜 Path Tracer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s5_point_cloud_renderer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S5 - 🧬3️⃣🔜 Point Cloud Renderer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../m4_optimization/s6_exercises/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S6 - 🧬4️⃣ Exercises
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m5_tips_and_tricks/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M5 - 3️⃣ Tips and Tricks
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            M5 - 3️⃣ Tips and Tricks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../m6_projects/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    M6 - 🧬4️⃣ Projects
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            M6 - 🧬4️⃣ Projects
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gpu-hardware" class="md-nav__link">
    GPU Hardware
  </a>
  
    <nav class="md-nav" aria-label="GPU Hardware">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer" class="md-nav__link">
    Transfer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#threads-warps-and-blocks" class="md-nav__link">
    Threads, Warps and Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-memory-hierarchy" class="md-nav__link">
    GPU Memory Hierarchy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-introducing-wgpu-and-wgsl" class="md-nav__link">
    3️⃣ Introducing wgpu and wgsl
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-basic-gpu-programming" class="md-nav__link">
    3️⃣ Basic GPU Programming
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-remove-the-loop-where-you-say" class="md-nav__link">
    3️⃣ Remove the loop where, you say?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-coalesced-accessing-and-strides" class="md-nav__link">
    3️⃣ Coalesced Accessing and Strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-divergence-overlap-and-occupancy" class="md-nav__link">
    3️⃣ Divergence, Overlap and Occupancy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-shared-memory-and-synchronization" class="md-nav__link">
    3️⃣ Shared Memory and Synchronization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-further-reading" class="md-nav__link">
    5️⃣ Further Reading
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="2-intro-to-gpus">2️⃣ Intro to GPU's</h1>
<p>Now, I'll just give you a quick introduction to GPU's as the next section is about immediate mode
GPU computation.</p>
<p>GPU's are fairly ubiquitous at this point. They started off as purely for graphics, but
around 2008, enough researchers had tinkered with workarounds to use them for general
computing, that Nvidia put out CUDA, opening up GPU's for more general usage. GPU's still do lots
of graphics, but the opaque black box parts are increasingly opened and even graphics API's such as OpenGL, Vulkan,
Metal and DirectX have opened up. With modern graphics API's you don't even necessarily need a graphics output
to use them. You can just use the pure compute capabilities. This guide won't get into graphics, except
for the graphics specialization.</p>
<p>Ok, so anyways, GPU's are pretty hot stuff right now as the software support becomes deeper and deeper,
the hardware increasingly has hardware support for neural network specific operations and ChatGPT has
increased the hype and demand for AI to exasperating levels.</p>
<p>You can think of the GPU as an expansion of the memory hierarchies we have been examining earlier.
It is not running in lock step, and you have to program more things explicitly, while also changing
your mindset about how programming works. Memory transfers to and from the CPU and GPU will be
relatively explicit, you have explicit control of a part of the L1 cache, you have start programming
in a warp oriented fashion and if-statements become quite dangerous.</p>
<p>If the CPU, with its numerous cores is like a team of highly skilled specialists building a car, sure,
they can build an amazing car, they can adapt to changing circumstances quite well, they can act independently,
then the GPU is like a factory. Each path and process has to be carefully optimized, they might each only deal with
a very small piece each and people have to work in lockstep. But. Their throughput is unmatched.</p>
<p>At 3️⃣ I will go into more detail as to how to actually write GPU code, but the guide is set up using
Rust and a GPU API abstraction layer called <a href="https://wgpu.rs/">wgpu</a>. You don't need to understand how it works
right now, but it means that you should be able to run all code, including GPU code, on your platform, even if
it's made by Apple or AMD.</p>
<p>In general, I will be using terminology specific to the compute part of the graphics API's and I will
keep to <code>wgpu</code> and <code>wgsl</code> terminology. You might see significant differences in terminology
if you follow up this guide with some <code>CUDA</code> programming.</p>
<h2 id="gpu-hardware">GPU Hardware</h2>
<p>First off, when dealing with the GPU, you will have to manipulate the GPU from the CPU with commands
like "allocate this much memory", "transfer this memory from the CPU to GPU", "execute this shader/kernel" and
"synchronize". These are all done in whatever language you are writing in on the CPU side, except for the
actual program the GPU has to run. This is distinct from the GPU API, some GPU API's even accept shaders
written in multiple shading languages, as they can either be transpiled (translated) from one language to
another, or they can be compiled to an intermediate representation, such as SPIR-V, which they can then ingest.</p>
<p>But once we have built up all of these commands, at least if they are non-blocking, as in the CPU program won't
advance until the command has completed, we have to actually submit them to the GPU. We do this with a
synchronization. The commands may/may not have already been submitted, but if you call a synchronization
function, the CPU-side code will block and wait until any and all submitted commands have executed on the GPU
and the GPU sends the all-clear signal in return. Imagine you are at a horse track. You have to give instructions
to a jockey on a race horse. You stand on the periphery of the big oval race track. You tell the jockey to make
some adjustment and do a lap. The horse first has to accelerate and then once it nears you, it slows down and you can
talk again. What would be more efficient was, if you could leave notes for the jockey to pick up
whenever he was coming around and the horse could just continue at speed. In some API's the GPU
can just be set in motion and then whenever you have a change to the loop it is running, adjust
or change. Or you can set work in motion and come back at a later time, checking whether the work might be done.</p>
<h3 id="transfer">Transfer</h3>
<p>When transferring memory, you should have the following model in mind, nothing gets transferred without a staging
area. When transferring from CPU to GPU, at least in the CUDA programming model, it will pin an area in memory.
That memory won't be movable until it is unpinned. You basically transfer some memory from say, a vector you
want transferred to the GPU, to this pinned memory staging area. That pinned memory area means the GPU
can work in peace without interruptions. In CUDA, if you don't explicitly do it, CUDA will create a pinned memory
area and do it for you. If you do it yourself and optimize this process you are likely to see around 2x improvement
in transfer speed. The same thing happens on the GPU, a staging area visible from the CPU is where the transferred
memory is stored, and then moved from the controlled area to the rest of GPU memory, where the GPU is free to do
what it wants with it, without interruptions and guarantees.</p>
<h3 id="threads-warps-and-blocks">Threads, Warps and Blocks</h3>
<p>Threads are sort of like a CPU core, except a CPU core is a physical entity, whereas a thread is more like
a set of variables (think back to the stack and function calls) which is following its own set of instructions.
Thread 1 could be running program A with various states in registers and local variables X. It makes a call
to something expensive, like a cache-missing memory access. While waiting, thread 1 is swapped for thread 2.
Its state is of course saved, but thread 2's program B and state Y are swapped in for it to do some work.
This keeps the CPU core itself occupied with work.</p>
<p>Threads on a GPU, will usually be executing the SAME program, unless several calls are overlapped, but let's
just focus on you having called a single operation. In that case all of your threads will launch, running
the same program. They might however, go down different branches (think if-statements!), but this more expensive
on the GPU and CPU, and should in general be avoided as much as possible. Each thread will have its own local
variables. Threads on a GPU are launched in groups. Depending on the platform and the API they will be
called something different. In wgpu, which is what we will be using, it is called a workgroup, while
in CUDA terminology it is called a warp. On Nvidia GPU's it will be at most 32 threads per workgroup
and on AMD it will be at most 64 threads. The "at most" might seem a bit weird, but there is something called
register pressure. All of the execution units that can run those 32 or 64 threads at the same time, share
a lot of the same physical memory, so if your program uses lots of memory, you might have to decrease
the amount of threads to have enough memory to run your program.</p>
<p>Anyways. Once you decided to write a matrix-matrix multiplication shader, you need to figure out which threads
are gonna go where. In that case, I would begin by launching 1 thread for every output element.</p>
<p>When programming for a GPU you have some maximum amount of threads you can launch. This is usually
defined in three dimensions. Yes! You can define these threads in three dimensions. It doesn't actually
have much of an effect, but it makes sense to tailor how you launch threads to your problem area.
If you are performing image processing or matrix multiplication, by all means, launch a 2D grid.
If you are summing an abitrary list of numbers, a single dimension will probably suffice.</p>
<p>So, we should launch a 2D grid, matching the output elements of our problem. Next up,
how do know which thread does what work? Each thread will usually begin its program
by asking built-in variables, which thread it is. This can be which thread it is within its
own workgroup, or it could be globally. Once it knows that, it should usually check whether
it is within legal bounds of the problem. We almost always want n^2 threads in our workgroup,
and it wouldn't be very flexible if the problem size always had to match exactly.
So usually, you should launch too many threads and then have an if-statement following
the thread ID calculation. If within acceptable range, do work, otherwise, don't do work.</p>
<p>It cannot be assumed that all work groups are running concurrently. The GPU might need to launch
waves of work groups because there aren't enough physical execution units.
As such, we can only synchronize between threads inside the warp.</p>
<h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3>
<p>The memory hierarchy on a GPU looks a lot like the memory hierarchy on the CPU. Here it is exemplified by the
Nvidia H100, which is a very expensive data center GPU and most definitely not the one residing in your laptop.
But the bandwidth (how much data per second can be transferred) internally on the card is a lot higher than on the CPU.
All of the streaming multiprocessors share the L2 cache and each streaming multiprocessor shares an L1 cache. On
Nvidia GPU's the streaming multiprocessor is a number of, in this case 4, units which can each execute a
work group, or in Nvidia terminology, a warp.</p>
<figure>
<p><a class="glightbox" href="../../figures/Full-H100-GPU-with-144-SMs-1024x457.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/Full-H100-GPU-with-144-SMs-1024x457.png" width="700" /></a></p>
<figcaption>
The layout of a H100 GPU. Note that connectivity to the memory (HBM3) is on the left and right sides.
<a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">
Image credit </a>
</figcaption>
</figure>
<p>Take some time to study these two diagrams and think about how data moves first from the CPU,
to the GPU's main memory, then to the L2 cache, then to the streaming multiprocessor which needs its L1 cache
until it finally is loaded up into the registers of the 32x4 threads executing on different, but adjacent, segments
of the same data.</p>
<figure>
<p><a class="glightbox" href="../../figures/H100-Streaming-Multiprocessor-SM-625x869.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/H100-Streaming-Multiprocessor-SM-625x869.png" width="400" /></a></p>
<figcaption>
The layout of a single Streaming Multiprocessor. It can execute 4 work groups or warps at a time.
<a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">
Image credit </a>
</figcaption>
</figure>
<p>The threads accumulate their data into their own registers until they are done and write the
result to main memory. The CPU waits for the GPU to be finished, until the GPU is, transfers to the CPU and
signals that it is finished.</p>
<p>It's not always as clear cut, though. If you are using a laptop, you probably have an integrated graphics card.
The CPU and GPU coexist and share the same memory. There may be sections where there is higher bandwidth than
just normal CPU-based memory, but overall the integrated GPU has access to the same memory the CPU has.
This makes for faster transfers, but probably slower overall computation. This has become quite useful
recently with most consumer grade GPU's having around 8 GB of memory and locally run neural networks
like diffusion models easily being able to use more than that. A desktop GPU with more than 16GB of RAM would
probably still outperform an integrated graphics card with 16GB of RAM available, but it would be very expensive.</p>
<hr />
<h2 id="3-introducing-wgpu-and-wgsl">3️⃣ Introducing wgpu and wgsl</h2>
<p>The guide will for all GPU purposes make use of the graphics library wgpu, but only the compute parts.
wgpu is based on the WebGPU spec, which is supposed to be the new web GPU API, as well as not being particularly
creative with their naming, the actual support in browsers for WebGPU is nascent. Chrome supports if you fiddle
with some settings, but for most systems, especially if you aren't actually running in a browser, wgpu
will default to using different, more powerful backends. For example, at the time of writing this,
I am using an HP laptop, with an Intel integrated graphics card running Windows 10. Whenver I run a program
with wgpu, wgpu tells me it has chosen Vulkan as my current backend. We could of course just write Vulkan,
but it would be a bit more complicated, as Vulkan is slightly more low-level than wgpu, but it would also
be more powerful. But attaining ultimate performance isn't the purpose of the guide. It's to get as many
people as possible started as soon as possible. It has to run on an Apple computer and it has to be easy to
install. So, wgpu it is. While any API which has to cover as many platforms as wgpu does will usually be hampered
by the lowest common denominator, it is possible to query wgpu for hardware support for various features, such
as fp16. While wgpu is still quite new, it has some exciting features on the way, such as a hardware accelerated
ray tracing extension.</p>
<p>The default shading language (the language you use to write the code the GPU will run) is wgsl, which
was defined along with the WebGPU specification. It is possible to use other shading languages, such
as glsl and hlsl, which also have more info and general documentation, but because of the increased code
complexity in building the files to SPIR-V and then ingesting them, I elected to just use what was simplest.</p>
<p>We can add wgpu to a project by going into the <code>Cargo.toml</code> file in the root directory,
and under <code>[dependencies]</code> write the line <code>wgpu = "*"</code>. It will pull down the latest version of wgpu.
You can of course also get a specific version of it, such as <code>wgpu = "0.16.3"</code>.</p>
<h2 id="3-basic-gpu-programming">3️⃣ Basic GPU Programming</h2>
<p>GPU programming, as has previously been mentioned, has two major elements. Host (CPU) code and device (GPU)
code. We'll start off with the basics of the host code and then move on the GPU code. Just enough
for you to be able to read the following sections and understand what is going on in this entire module,
as it doesn't go into the finer details of GPU programming, but is centered around a GPU-oriented paradigm.</p>
<p>The rest of this section will be make use of the code location at <code>m1_memory_hierarchies/code/gpu_add/</code> or
<a href="https://github.com/absorensen/the-guide/tree/main/m1_memory_hierarchies/code/gpu_add">online</a>.
Make sure to go and actually read the code. It is full of comments! And they're made just for you!
If you want to learn more about wgpu you can visit <a href="https://sotrh.github.io/learn-wgpu/">Learn Wgpu</a>.</p>
<p>Be sure to read through the code! Do this before you read the rest of this section, which will go into greater
detail.</p>
<p>Starting in the <code>main</code> function, first we initialize the environment logger with <code>env_logger::init()</code>.
This will get us more helpful feedback from wgpu. This should only happen once in your code, so by putting it
as the very first line, we should be sure that it shouldn't need to happen anywhere else.</p>
<p>Next up, we call <code>pollster::block_on(self_test())</code>. The <code>self_test</code> function, is a function I made, and
use elsewhere to make sure your system is compatible and to print the system info so you can see what GPU is being
found and what backend is being used. <code>pollster::block_on</code> allows us to call asynchronous code from a normal
function. If you don't remember what asynchronous means, just think of it as being non-blocking. Meaning, we can
launch an asynchronous function and just continue on to the next line of code. But the way we do this is different
depending on whether we are inside a normal function or an <code>async</code> function. An <code>async</code> function definition
example - <code>pub async fn self_test() -&gt; bool {</code>.</p>
<p>If we are in a normal function and we call an <code>async</code> function, we have to wait for it to complete. As in, block
on the function call, which is of course <code>pollster::block_on()</code>. Inside the <code>async</code> function itself it can
either block on async function calls by using <code>await</code> - such as <code>let result = async_function().await;</code> or
you can store what is known as a future. We could set in motion the loading of a number of files, and then once we
were done and actually genuinely NEEDED to use the files for something, <code>await</code> on the future. The <code>async</code>
function, when called from a normal function also returns a future, but we can't use <code>.await</code> on it.</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">pub</span><span class="w"> </span><span class="k">async</span><span class="w"> </span><span class="k">fn</span> <span class="nf">load_four_files</span><span class="p">(</span><span class="n">path_a</span>: <span class="kp">&amp;</span><span class="kt">str</span><span class="p">,</span><span class="w"> </span><span class="n">path_b</span>: <span class="kp">&amp;</span><span class="kt">str</span><span class="p">,</span><span class="w"> </span><span class="n">path_c</span>: <span class="kp">&amp;</span><span class="kt">str</span><span class="p">,</span><span class="w"> </span><span class="n">path_d</span>: <span class="kp">&amp;</span><span class="kt">str</span><span class="p">)</span><span class="w"> </span>-&gt; <span class="p">(</span><span class="n">File</span><span class="p">,</span><span class="w"> </span><span class="n">File</span><span class="p">,</span><span class="w"> </span><span class="n">File</span><span class="p">,</span><span class="w"> </span><span class="n">File</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_future_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_file_async</span><span class="p">(</span><span class="n">path_a</span><span class="p">);</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_future_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_file_async</span><span class="p">(</span><span class="n">path_b</span><span class="p">);</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_future_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_file_async</span><span class="p">(</span><span class="n">path_c</span><span class="p">);</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_future_d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">load_file_async</span><span class="p">(</span><span class="n">path_d</span><span class="p">);</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">file_future_a</span><span class="p">.</span><span class="k">await</span><span class="p">;</span><span class="w"> </span><span class="c1">// Block on the future</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">file_future_b</span><span class="p">.</span><span class="k">await</span><span class="p">;</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">file_future_c</span><span class="p">.</span><span class="k">await</span><span class="p">;</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">file_d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">file_future_d</span><span class="p">.</span><span class="k">await</span><span class="p">;</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="w">    </span><span class="p">(</span><span class="n">file_a</span><span class="p">,</span><span class="w"> </span><span class="n">file_b</span><span class="p">,</span><span class="w"> </span><span class="n">file_c</span><span class="p">,</span><span class="w"> </span><span class="n">file_d</span><span class="p">)</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="p">}</span>
</span></code></pre></div>
<p>Ok, so why do we need <code>async</code> when dealing with the GPU? In some cases, we don't care about synchronization.
We just want to keep transferring data to the GPU as fast as we can get it, the GPU might output to the display
or we might get some data transferred back, but if we are doing this in a real-time setting, we might not care
to synchronize, as in block, and we just need things when they are ready. Anything to do with gpu - <code>async</code>
will be involved. At least in Rust.</p>
<p>Let's move on. We set up our CPU-side data. This is a simple vector addition, and I elected to make the data
in a way that was easily verifiable as correct for humans. Input A and B are just vectors of 32-bit floats
with values equal to their index. The correct result in the output vector should of course be double the
index value then.</p>
<p>Finally, we call <code>initialize_gpu()</code> and block on it. Let's go into that function!</p>
<p>First we get an <code>Instance</code>. The <code>Instance</code> is a wgpu context which we will use to get <code>Adapter</code> and
<code>Surface</code>. The <code>Adapter</code> corresponds to your GPU. We specifically request the adapter with high performance.
If you are on a system with more than one GPU, such as a laptop with an integrated GPU, which shares memory with
the CPU and a more powerful dedicated GPU, it should try to get access to the dedicated GPU. We also request
<code>None</code> for <code>compatible_surface</code>. Surfaces are what you would render to if you were doing graphics. Think
of an image with extra steps, which you could show on your display. If we don't need to do graphics, not having one
is less work. It also means we can run on data center GPU's, which might not even have a display port. So we just
get the <code>Adapter</code>. We use the <code>Adapter</code> to get <code>Device</code>, which will be our handle to the GPU
from now on. Whereas the <code>Adapter</code> is more of a raw connection, which we can't do much with. The
<code>Device</code> is a handle that has some guaranteed features. The <code>Adapter</code> tells us what features we can get.
Once those features are guaranteed, it is much easier for wgpu to open up for more functionality with
the <code>Device</code>. We actually don't need the <code>Adapter</code> after we get the device,
but I keep it around in the GPUHandles for you to tinker around with auto-complete to see what it can
do. We do need the <code>Device</code> though. We also need the <code>Queue</code>. The <code>Queue</code> is where we
can submit the work we want the GPU to do.</p>
<p>Note that when defining our <code>DeviceDescriptor</code> for getting a <code>Device</code> that lives up to our needs
our current requested <code>features</code> is <code>wgpu::Features::empty()</code>. We just want the absolute basics.
But we could request, or at least see whether we could get them, features like 16-bit floating point support.</p>
<p>Now back to the <code>main</code> function!</p>
<p>We now have our bundled GPU-related handles. Now we calculate how many threads we need to launch for our
problem size. <code>let element_count: usize = 100;</code>, so we need to launch AT LEAST 100 threads if each thread
only processes one element of our problem. Which it does, in our simplified case. Given that we would like to fill
up our work groups, I have elected to use 32 threads per work group. <code>let block_size: usize = 32;</code>.
Given that the register pressure is likely very low for our shader, this should be no problem. Finally, we
calculate how many blocks to launch. This simple calculation is found all of the place when doing
GPGPU programming. <code>let launch_blocks: u32 = ((element_count + block_size - 1) / block_size) as u32;</code>.
The basic premise is that we add one element less than the full work group size and then use integer division
to make sure we always have at least as many threads as we need. In the worst case of a work group size of 32,
we will have a work group at the very end of the vectors with 31 idle threads.</p>
<p>Next up, we compile our shader code <code>add_vectors.wgsl</code> with <code>.create_shader_module()</code>.
Compiling shaders is quite expensive, so if you are programming a bigger system than this, you might want to
save the compiled code or do it as part of your build step.
When running you can load it from disk as needed. Once we have compiled
our shader code we can create a compute pipeline with a specific entry point. The entry point is just the function
that will be called when dispatching the shader call later on. Once we have a <code>ComputePipeline</code> we can
begin doing our bind group layouts. In CUDA you can pass device side pointers to your CUDA functions
when dispatching. Or phrased differently, when using CUDA you can pass along memory addresses for buffers you have
explicitly allocated on the GPU. When using the graphics APIs the most basic thing to do, if you are
not going bindless, which is... well, don't worry about it, is to use bindings. There is a certain amount of bind
slots available in a shader depending on the API and perhaps the system. What can be a bit tricky is the binding
slot you declare on the CPU for buffer X, has to match the exact binding slot in the shader. E.g. if you bound
your input buffer to binding slot 0 on the CPU, it has to be bound to binding slot 0 in your shader code.
Additionally, the compiler will complain if you don't use that buffer in the shader. Finally, you can have multiple
sets of bindings in the same shader. These are called bind groups and each has N binding slots.</p>
<p>When I created the <code>GPUVector</code>s earlier, the <code>new</code> function allocated a storage buffer, which is visible
to the shader and transferred the contents of the given vector to the GPU. This can be done more effectively, but
it's a nice and easy way to start things off. We don't have to keep track of whether we remembered to transfer
our data to the GPU or not, which makes sure we don't use initialized data. In the case of the output vector, we
have also allocated a <code>staging_buffer</code> to more explicitly transfer data back to the CPU. This <code>Buffer</code>
has also been flagged as readable from the CPU.</p>
<p>The <code>storage_buffer</code>s we have created, when creating the <code>GPUVector</code>s from before, can be bound. I
add these binding references to a vector and send them to a convenience function <code>create_bind_group()</code>, which
binds the array of bindings in order. Do note that we don't specify what can and cannot be done at this step. It
was specified at the creation of the <code>storage_buffer</code>s and it will be specificed locally in the binding of
the buffers in the shader.</p>
<p>Once we have our bindings set up, we create a <code>CommandEncoder</code>, which we get from <code>Device</code>.
The command encoder is a buffer of commands. We can add stuff like render and compute operations,
they are sort of like a collection of operations and state, and transfer operations. The command encoder
needs to be finished, before it is submitted to the queue. Remember the <code>Queue</code> we got earlier?
This is what it was for. We submit finished <code>CommandEncoder</code>s to our <code>Queue</code>, which submits
the jobs to the GPU. For this specific program we add two commands to the <code>CommandEncoder</code>. We
dispatch our compute shader, enclosed in a <code>ComputePass</code> and launch the appropriate number of threads.
Note also the <code>label</code> field. This field permeates wgpu usage. It is mostly for debugging. It helps us
identify what is causing an issue. Once we have finished our <code>ComputePass</code>,
due to it going out of scope, we add a transfer operation. We use the <code>staging_buffer</code> on our <code>output</code>
vector, to read the output back to the CPU. Then we finish our <code>CommandEncoder</code> and submit it
to the <code>Queue</code>.</p>
<p>We then setup a <code>oneshot_channel</code>. Don't worry too much about this. It is a connection which can only be used
for sending data once. We map the <code>staging_buffer</code> and send its data using the sender/receiver pair. Once
we have done this <code>map_async</code> call, we wait for the GPU to be finish all operations currently in its queue.
Once it has finished we block on the receiver. Until the receiver sends the <code>Ok</code> signal, we wait. Once we get
it we retrieve the data. This is raw data in bytes, <code>u8</code>, which we recast to the type we know it is, which in
this case is <code>f32</code>. We do a bit of clean up, and don't you know it, that's the program!</p>
<figure>
<p><a class="glightbox" href="../../figures/intro_to_gpu_vector_add_results.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/intro_to_gpu_vector_add_results.png" width="600" /></a></p>
<figcaption>
Adding our two vectors, it should be easily verifiable that it is correct.
</figcaption>
</figure>
<p>Maybe now might be a good time to go back to the code and try to run through it again.</p>
<h2 id="3-remove-the-loop-where-you-say">3️⃣ Remove the loop where, you say?</h2>
<p>When writing GPU programs, you should usually start writing a CPU-based version. Once that works, you have
something to verify your GPU program against. Often the part of your program that you want to offload to the GPU,
will have loops. For example, in a vector addition snippet you might have -</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="k">for</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">0</span><span class="o">..</span><span class="n">ouput</span><span class="p">.</span><span class="n">len</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input_b</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="p">}</span>
</span></code></pre></div>
<p>When transferring your program to a GPU shader, as a way to get comfortable with thinking about this sort of
parallelism, you should start with writing a single threaded version on the GPU. You can do this by dispatching
a single thread <code>cpass.dispatch_workgroups(1, 1, 1);</code>. It WILL be slower than the CPU
version, but it allows you to get all of the transfers and synchronizations out of the way first. Once you have
done that, and you have verified that it works, mind you, you can start adding, or rather removing dimensions.
You do that by removing one of the for-loops in your code and replacing it with added dimensionality in your shader
dispatch. So in your first version of your vector addition shader, it might look like this sketch (don't know if
it compiles) -</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="o">@</span><span class="n">compute</span><span class="w"> </span><span class="o">@</span><span class="n">workgroup_size</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="k">fn</span> <span class="nf">main</span><span class="p">(</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">global_invocation_id</span><span class="p">)</span><span class="w"> </span><span class="n">global_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">thread_id</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">global_id</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">thread_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">var</span><span class="w"> </span><span class="n">index</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">u</span><span class="p">;</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">element_count</span><span class="p">;</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="n">u</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="w">            </span><span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input_b</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="p">}</span>
</span></code></pre></div>
<p>When that works, you can begin thinking about how to remove that pesky loop. You do that by removing a dimension
in your shader, but adding one in your dispatch and then making accomodations in your shader. We can take that
and transform it by instead dispatching more 1D threads: <code>cpass.dispatch_workgroups(launch_blocks, 1, 1);</code>.
Then we change the shader to have each thread work on a single element -</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="o">@</span><span class="n">compute</span><span class="w"> </span><span class="o">@</span><span class="n">workgroup_size</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="k">fn</span> <span class="nf">main</span><span class="p">(</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">global_invocation_id</span><span class="p">)</span><span class="w"> </span><span class="n">global_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">thread_id</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">global_id</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">thread_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">element_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">thread_id</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_a</span><span class="p">[</span><span class="n">thread_id</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input_b</span><span class="p">[</span><span class="n">thread_id</span><span class="p">];</span><span class="w">        </span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="p">}</span>
</span></code></pre></div>
<p>If there had been more dimensions we could have continued expanding and removing dimensionality. We can continue
until the third dimension, usually you can launch less threads in the third dimension than in the first two. You
also have to remember to check whether the thread is outside of the valid range for each dimension. You
should always look up your graphics cards and GPU API to see how many threads you can launch. You might have to
break it into several passes. It's not actually quite this simple, as, well you remember how we learned stride
had a negative impact on performance earlier? Well, that is not quite the same on GPU's.</p>
<h2 id="3-coalesced-accessing-and-strides">3️⃣ Coalesced Accessing and Strides</h2>
<p>Because of the way threads and work groups share memory on a GPU, and each thread executing the same line of
code at the same time, if thread A calls for memory at indices 0, 1, 2, 3 and thread B, which is right next to it
in the same work group, calls for indices 4, 5, 6, 7, they will be asking for two different cache lines at the
same time. Imagine the whole work group doing this at the same time. They will all be waiting, while
requesting different cache lines. What is normally faster, is if, given a work group size of 32,
thread A calls for indices 0, 32, 64 and 96, with thread B calling for indices 1, 33, 65 and 97. This allows for
the work group to call for a minimum of cache lines in lock step and each getting a piece of the cache line.
This is called <em>coalesced accessing</em> and if you ever say that to a GPGPU programmer, you will see a faint smile on
their face. Think of a jigsaw puzzle, where the pieces are slowly being adjusted.
Eventually, they all snap into place. All of the pieces fit exactly right.</p>
<p>Here's a small example, if we for some reason were intent on turning our vector addition shader into
2D matrix addition, but we were deadset on keeping the thread grid for our dispatch one dimensional we
could do something like this -</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">const</span><span class="w"> </span><span class="n">BLOCK_SIZE</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="mi">32</span><span class="n">u</span><span class="p">;</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="o">@</span><span class="n">compute</span><span class="w"> </span><span class="o">@</span><span class="n">workgroup_size</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="k">fn</span> <span class="nf">main</span><span class="p">(</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">global_invocation_id</span><span class="p">)</span><span class="w"> </span><span class="n">global_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">thread_id</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">global_id</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">thread_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">first_dimension_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="w">            </span><span class="n">var</span><span class="w"> </span><span class="n">index</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">thread_id</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="w">            </span><span class="n">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">second_dimension_count</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="w">            </span><span class="n">index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="w">        </span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a><span class="w">            </span><span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input_b</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a><span class="p">}</span>
</span></code></pre></div>
<p>Again, not verified/compiled code. But hold on for a second! We have to remember that there are other work groups
too. We can't necessarily just stride through the single dimension in the same way. We would be reprocessing
elements that had already been processed by a different work group. What we could do instead would be to step
along the rows instead.</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="o">@</span><span class="n">compute</span><span class="w"> </span><span class="o">@</span><span class="n">workgroup_size</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="k">fn</span> <span class="nf">main</span><span class="p">(</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">global_invocation_id</span><span class="p">)</span><span class="w"> </span><span class="n">global_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">thread_id</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">global_id</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">thread_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">first_dimension_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a><span class="w">            </span><span class="n">var</span><span class="w"> </span><span class="n">index</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">thread_id</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="w">            </span><span class="n">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">second_dimension_count</span><span class="p">;</span><span class="w"> </span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="w">            </span><span class="n">index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">dimensions</span><span class="p">.</span><span class="n">first_dimension_count</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a><span class="w">        </span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
</span><span id="__span-5-13"><a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a><span class="w">            </span><span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">input_b</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
</span><span id="__span-5-14"><a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-5-15"><a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a><span class="w">    </span><span class="p">}</span>
</span><span id="__span-5-16"><a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a><span class="p">}</span>
</span></code></pre></div>
<p>In other cases, using a stride of the work group size can work as well. In this case, stepping along the rows
made better sense, but keep thinking in these terms, implement different versions and test them! It's the
only way to be sure! Once you have made a couple of different versions and done simple timing you can always
add in a profiler, m4 has got you covered!</p>
<h2 id="3-divergence-overlap-and-occupancy">3️⃣ Divergence, Overlap and Occupancy</h2>
<p>One statement I tried to sweep under the rug in the last section was - "each thread in a work group executes in lock step".
It is highly desirable for a work group to have each thread executing in lock step. That is each thread is executing the
same line in your program. If you have branches, like if-statements, some threads might execute path A and some threads
might execute path B. This will lead to divergence. Divergence will result in the group of threads A executing
while group of threads B will wait until A is finished, and then executed. Finally, they might join again.</p>
<figure>
<p><a class="glightbox" href="../../figures/work_group_divergence.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/work_group_divergence.png" width="700" /></a></p>
<figcaption>
An if-statement causes a work group to diverge into two.
<a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">
Image credit </a>
</figcaption>
</figure>
<p>As you can imagine, this expands the timeline of executing the code compared to a non-diverging execution.
But if you were within a workgroup where all threads take the same branch there wouldn't be an issue.
Thankfully, recent hardware takes less of a performance hit when work groups diverge.</p>
<p>Once you have most of your work groups not diverging, are you sure your threads aren't just sitting around
waiting? Whenever a thread wants to load a piece of data all the way from memory, it can take quite a long
while to retrieve. If however, you have dispatched enough work, the threads waiting around for memory
can be swapped out for another work group which might be able to do some work, once this work group has time
for a nap, like when it is also requesting data from memory, the first work group can be swapped back in,
when the requested data has, hopefully, arrived. Without this overlap, GPU programs are likely to seem
a lot slower than they need to be. If however, you launch a lot more threads than there are physical
execution units, you are likely to see this overlap resulting in higher occupancy. The higher the occupancy
the more time a physical execution unit spends on doing actual work and not just stalling until everything
is ready. So you can either launch a lot of independent work, or use a lot of elements in your data.
Like really big matrices!</p>
<p>In machine learning terms, if you have pipelined and made your computational graph
relatively independent, you might see a big increase in occupancy by using less layers and make the ones left very
wide.</p>
<h2 id="3-shared-memory-and-synchronization">3️⃣ Shared Memory and Synchronization</h2>
<p>Just two final pieces are missing before we go back to memory hierarchies. Shared memory and synchronization.
GPU's have more programmable pieces of the memory hiearchy, such as sharing directly between threads, sharing
between work groups and more, but WGSL just has the primitives for shared memory, which is the only one
I will present for you. Shared memory is a programmable section of the L1 cache. If a cache miss, resulting
in retrieving data all the way from memory costs 100's of cycles, quite often somewhere around 250-300,
accessing data from shared memory costs around 10 cycles. This is very useful if each piece of data is
accessed more than once. It could for example be overlaps in convolutions or storing preliminary results
in shared memory for the work group to finally reduce the results internally in the workgroup, before one
of the threads writes the final result to global memory.</p>
<p>Typically using shared memory, you will first see a section where each thread loads one or more pieces
of data into shared memory, followed by a synchronization primitive. This synchronization primitive
is available in wgsl and is called <code>workgroupBarrier();</code>. It is available in most shader languages,
although it will likely be named something else. It is a barrier ensuring that all threads in the
workgroup will stall and wait until each thread has signalled that it is ready to proceed. This
is very handy when you are loading data into shared memory for reuse between the threads. A small
example snippet -</p>
<div class="language-rust highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">var</span><span class="o">&lt;</span><span class="n">workgroup</span><span class="o">&gt;</span><span class="w"> </span><span class="n">shared_data</span>: <span class="nc">array</span><span class="o">&lt;</span><span class="kt">f32</span><span class="p">,</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="o">&gt;</span><span class="p">;</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="o">@</span><span class="n">compute</span><span class="w"> </span><span class="o">@</span><span class="n">workgroup_size</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="k">fn</span> <span class="nf">workgroup_phase</span><span class="p">(</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">workgroup_id</span><span class="p">)</span><span class="w"> </span><span class="n">group_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="w">    </span><span class="o">@</span><span class="n">builtin</span><span class="p">(</span><span class="n">local_invocation_id</span><span class="p">)</span><span class="w"> </span><span class="n">local_id</span>: <span class="nc">vec3</span><span class="o">&lt;</span><span class="kt">u32</span><span class="o">&gt;</span><span class="p">,</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="w">    </span><span class="n">var</span><span class="w"> </span><span class="n">tid</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">local_id</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">group_id</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="n">u</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="w">        </span><span class="c1">// In this first section we can use all 32 threads</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span class="w">        </span><span class="n">var</span><span class="w"> </span><span class="n">elements_left</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">sum_uniform</span><span class="p">.</span><span class="n">block_count</span><span class="p">;</span>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span class="w">        </span><span class="n">var</span><span class="w"> </span><span class="n">i</span>: <span class="kt">u32</span> <span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span class="w">        </span><span class="n">var</span><span class="w"> </span><span class="n">sum_value</span>: <span class="kt">f32</span> <span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a><span class="w">        </span><span class="c1">// How do we handle the odd case?</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">elements_left</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a><span class="w">            </span><span class="n">sum_value</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span class="w">            </span><span class="n">elements_left</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span><span id="__span-6-18"><a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a><span class="w">            </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span><span id="__span-6-19"><a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-6-20"><a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="mi">0</span><span class="n">u</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">elements_left</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-21"><a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span class="w">            </span><span class="k">if</span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">elements_left</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</span><span id="__span-6-22"><a id="__codelineno-6-22" name="__codelineno-6-22" href="#__codelineno-6-22"></a><span class="w">                </span><span class="n">sum_value</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span id="__span-6-23"><a id="__codelineno-6-23" name="__codelineno-6-23" href="#__codelineno-6-23"></a><span class="w">            </span><span class="p">}</span>
</span><span id="__span-6-24"><a id="__codelineno-6-24" name="__codelineno-6-24" href="#__codelineno-6-24"></a><span class="w">        </span><span class="p">}</span>
</span><span id="__span-6-25"><a id="__codelineno-6-25" name="__codelineno-6-25" href="#__codelineno-6-25"></a>
</span><span id="__span-6-26"><a id="__codelineno-6-26" name="__codelineno-6-26" href="#__codelineno-6-26"></a><span class="w">        </span><span class="n">shared_data</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum_value</span><span class="p">;</span>
</span><span id="__span-6-27"><a id="__codelineno-6-27" name="__codelineno-6-27" href="#__codelineno-6-27"></a><span class="w">        </span><span class="n">workgroupBarrier</span><span class="p">();</span>
</span><span id="__span-6-28"><a id="__codelineno-6-28" name="__codelineno-6-28" href="#__codelineno-6-28"></a><span class="w">    </span><span class="p">}</span>
</span></code></pre></div>
<p>As usual, this code isn't very well tested and there might be some cases where it isn't fully functional,
but you can see the primitives for declaring shared memory, accessing it and synchronizing.
Now back to the memory hierarchies!</p>
<h2 id="5-further-reading">5️⃣ Further Reading</h2>
<p><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/lectures/08_mem_hierarchy.pdf">The GPU Memory Hierarchy</a>,
<a href="http://meseec.ce.rit.edu/551-projects/spring2015/3-2.pdf">GPU Memory Hierarchy</a>,
<a href="http://dlsys.cs.washington.edu/pdf/lecture5.pdf">GPU Programming</a>,
<a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">Hopper Architecture In-Depth</a>
and <a href="https://gfxcourses.stanford.edu/cs149/fall22/lecture/gpuarch/">GPU architecture and CUDA Programming</a>.
The last entry is highly recommended.</p>
<p>A slightly more detailed explanation of
<a href="https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod14DataXfer/Mod14DataXfer.pdf">asynchronous memory transfers</a>
for GPUs.</p>
<p>If you want to learn more about wgpu, this is the most used tutorial -
<a href="https://sotrh.github.io/learn-wgpu/">Learn Wgpu</a>.</p>
<p>To learn more about
<a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory">optimzing shaders with shared memory</a>.</p>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 16, 2023</span>
      
        <br>
        Created:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">July 3, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../s1_computational_graphs/" class="md-footer__link md-footer__link--prev" aria-label="Previous: S1 - Computational Graphs" rel="prev">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                S1 - Computational Graphs
              </div>
            </div>
          </a>
        
        
          
          <a href="../s3_immediate_gpu_computation/" class="md-footer__link md-footer__link--next" aria-label="Next: S3 - Immediate GPU Computation" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                S3 - Immediate GPU Computation
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:aboso@dtu.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/absorensen" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/anders-bo-sorensen/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.indexes", "content.code.copy", "navigation.footer", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
    
  <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>