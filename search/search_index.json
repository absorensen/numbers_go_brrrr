{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Don't Panic!","text":"<p>A guide designed for both deep learners and systems programmers. Meant to be followed several times at deepening levels. The material is comprised of 6 modules.</p> <ul> <li>Intro to the course, the different ways to use the material, intro to Rust and wgpu.</li> <li>Memory hierarchies and computational graphs</li> <li>Parallelization, interactivity, events and GUIs</li> <li>Types, energy usage and inference, quantization, sparsity and pruning of neural networks</li> <li>Introduction to profiling, optimization use cases on topics such as model training and quantization, graphics, computer vision</li> <li>How to create real time systems, frameworks for the different fields and project proposals</li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Try rust-nexttest to solve the testing issue</li> <li>Find the right benchmarking and performance tools (blessed.rs)</li> <li>Look into friendlier error handling? Perhaps logging instead of panicing to get students used to logging. Introduce anyhow for better error handling?</li> <li>Loom</li> <li>Come up with a different name for levels 1/2/3/4, also should the levels be described in a matrix?</li> <li>Should there be an introduction to basic computer architecture somewhere?</li> </ul>"},{"location":"#references-and-additional-reading","title":"References and additional reading","text":"<p>High Performance Machine Learning High Performance Machine Learning Flash Attention Branchless Programming The Rust Programming Language Learn wgpu Install Rust wgpu ShaderToy Inigo Quilez ORB-SLAM ORB-SLAM2 Z-order curves Linearised Trees on the GPU Vivienne Sze - Energy Efficient AI Visual Computing - Stanford Parallel Computing - Stanford Rust Profiling RenderDoc Book of Shaders Scratchapixel Ray Tracing in One Weekend Physically Based Rendering Crafting Interpreters Programming Rust Godbolt Advent of Code </p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<ul> <li>Nicki Skafte Detlefsen</li> <li>Lars Kai Hansen</li> <li>Pioneer Centre for AI</li> <li>Mark Bo Jensen</li> <li>Mathias Gammelmark</li> <li>Anders Jess Pedersen</li> <li>Jens Egholm Pedersen</li> </ul>"},{"location":"overview/","title":"The Guide","text":"<p>Welcome to the internet, have a look around! Anything you can think of can be found!</p>"},{"location":"m0_introduction/","title":"Introduction","text":"<p>Hello there! If you are reading this you might have been enticed by promises of performance and other some such advanced black magic, but first, a digression...</p> <p>There are so many things to keep track of as a modern day programmer and most systems hide these things from the user. You call something called... <code>?numba?</code> and annotate a few functions and it magically makes your code faster. You use something called <code>?Py?...?Torch?...</code> and it seems really slow for some reason. You're sure you have a GPU in your machine, but it's still slow. <code>PyTorch</code> has something called a profiler, but you don't know how it works and you don't know what the hell <code>DtoHMemCpy</code> even is. It can be hard to reason about what is going on inside these black boxes. On top of that you might not be able to find a tutorial or a guide to talk you through all of the stuff you don't know that you don't know. As scary as it can sometimes seem to get your hands dirty and take on what might seem an insurmountable obstacle, not doing so can have wide reaching consequences.</p> <p>With the recent mainstreamification (is that a real word?) of AI systems it used to be an elephant in the room that no one seemed to talk about, as the overlap in the Venn diagram between people interested in making computers do stuff real fast and the people doing deep learning was quite small. That overlap is getting bigger, but not swiftly enough.</p> <p>The speed of execution of a program is approximately correlated to the energy consumption of that program. Until we use 100% green, renewable, energy in all of computing we have a shared responsibility to at the very least practice some modicum of restraint and sensibility in our resource consumption. Taken to the limit by putting large scale machine learning, with its massive energy consumption for both training and inference, in everything, without necessarily generating value comensurate to the expended resources, is an irresponsible use of resources.</p> <p></p>  Image credit  <p>If someone trains a deep learning model for two weeks on eight huge data center GPUS in a cluster, it is their responsibility that that training process is fairly well optimized, and that all data is responsibly retrieved, such that that training does not have to run again because of sloppiness.</p> <p>And thus stops the finger pointing!</p> <p>Optimizing code, especially on systems you might share with others both means that you can get your results faster, but that others can have use of the system in a reasonable time as well. If you are making large models, optimizing them to be smaller also results in corporations with profits less than the GDP of a small country can actually train and run inference with your model, increasing the democratization of your work and its reach. If its able to run on a consumer grade desktop - even better!</p> <p>This guide was made to be an iterative process, taking you by the hand, speaking to you at the level at which you are following it, trying not to overwhelm you. Reading that back, it could sound a bit condescending, but it basically means that the types of concepts you are assumed to know about will gradually increase with each level. Due to the guide gradually introducing certain concepts, jumping around the material is not recommended. The guide also acknowledges that some people have different interests. As such, portions of the guide will be tailored to people who like deep learning, people who like computer vision, or computer graphics or other some such. You are more than welcome to read and do all of it, but no one says you have to do anything. If you just follow along the path that is most relevant to you, you will be just fine. The guide does contain code, sometimes just small snippets, but also frameworks in which most of a module will take place.</p> <p>Most importantly - Don't Panic! The guide is here for you! And now for something completely different... practicalities!</p>"},{"location":"m0_introduction/#specializations","title":"Specializations","text":"<p>Throughout the guide there are sections and exercises prefixed by 'S'. These exercises and topics are meant to be up to you to follow. If you are mostly interested in deep learning, by all means only read and do the sections and exercises which are relevant to deep learning. Which section and exercise is relevant to which specialization will be explained in the each section. The currently supported specializations are deep learning, computer graphics, computer vision and cognitive signal processing.</p>"},{"location":"m0_introduction/#levels","title":"Levels","text":"<p>The guide's way of doing things is to iteratively expand the curriculum and the depth at which concepts are described. You can follow the guide iteratively by doing it multiple times, each time advancing in level or you can jump right in to the relevant level.</p>"},{"location":"m0_introduction/#level-1","title":"Level 1","text":"<p>This level is for people unsure about investing the time and effort to do level 2. People are busy, and inherently looking to maximize the value given their invested time. Just for those people each module has beem boiled down to approximately 2 pages of reading. Reading all of the material should take at most an afternoon and is comprised of the bottom of the main page of each module. Basically, you could stop reading once you are done with this \"Level 1\" header and just click each \"MX - XXXX\" on the left, read that page until the end, then click on the next \"MX - XXXX\" title and read that until the end and you would be done. That does not include \"*M5 - Projects\", that one is only relevant for levels 3 and 4. Happy reading!</p>"},{"location":"m0_introduction/#level-2","title":"Level 2","text":"<p>At this level you might be a comfortable programmer in Python, you might be a researcher, or you might just be warming up for level 3. In most cases you might not be all that comfortable with lower level languages, such as C, C++ or Rust. It is expected that you checkout the repository and try out the code on your own laptop. It is expected that you might change a few variables here and there, but not much more than that. Don't worry, it does not require an Nvidia GPU to run on your laptop. There will be Rust code, but it will be as simplified Rust code as possible to just focus on making your learning as easy as possible. If you are a systems programmer, you should be able to move through this level rapidly. This level does not take into account any module, section, page or info box with a '*' in front of the name. These constitute level 3. You are still welcome to read them of course, but the language and code are a bit more advanced and it might be a bit too much if you are still working on the basics.</p>"},{"location":"m0_introduction/#level-3","title":"Level 3","text":"<p>This level is made up of all the material from level 2, all of the sections with a title prefixed by a '*' and the relevant section to your specialization in any section prefixed by 'S'. The only thing not in level 3, is any exercise section. At this level it is expected that you have experience with C/C++/Rust and that you have tried programming a GPU or that you have previously done level 2 and are up for a challenge. If you haven't done any of these things, you'll be ok, but it might take significant effort on your part.</p>"},{"location":"m0_introduction/#level-4","title":"Level 4","text":"<p>At this level everything at level 3 is expected, as well as you doing most or all of the exercises. This is for doing a course version or if you REALLY want to learn all the guide has to offer.</p>"},{"location":"m0_introduction/#how-to-use-the-materials-as-a-teacher","title":"How to use the materials as a teacher","text":"<p>If you are a teacher who wants to make use of this material, feel free to use the course site. The course focuses on teaching real-time systems for deep learners and visual systems programmers. It allocates half of 15 days to going through the material in the guide and the other half to making a project relevant to the students' specialization, which is the contents of module 5. It is designed to move through lots of different topics very quickly with a handful of varying exercises. The student is to then reach back and explore the topics relevant to their project in greater detail. The breadth of topics is quite wide, and each student shouldn't be expected to pass an exam in every topic. In most cases they might remember that a thing exists and that they can search for it. The thing they hopefully all learn is how to reason about performance, systems design and that getting your hands dirty can be both fun and invigorating.</p>"},{"location":"m0_introduction/s0_intro_to_computing/","title":"Introduction to the Computing Landscape","text":"<p>If you are new to programming, or perhaps have been able to get by using scripting languages only, you might not have been introduced to the other options. Some of the concepts presented here lay the foundations for the choices dictating the rest of the guide. Though the guide has made some clearly defined choices about which tools to use, you should at all times use the right tool for the job. Not only in which language or framework you might choose, but in how you put together and design your systems using those tools. Part of the guide's strategy is to introduce you to quite a lot of tools and concepts, also known as the learn what to Google strategy, and then going into greater detail about core concepts and concepts especially relevant to your specialization. The guide will introduce concepts that aid some programs in producing faster and results than others. An important factor is limitations. Usually, the word limitations carries a negative connotation, very few people think less freedom sounds enticing, but in computing limitations can be a wonderful thing to have and set. Especially, once you are past the first prototype. In some cases, even when prototyping.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#scripting-languages","title":"Scripting Languages","text":"<p>Chill out, take things one line of code at a time. Scripting languages aren't compiled, but run one line at a time. This leaves the system unable to look beyond the current line of code, unless you add a compiler to the mix, whic usually takes a look at all of your code.</p> <p>Python is likely the scripting language you are most familiar with. Very popular due to its apparent ease-of-use. Quite slow in its raw form. The main advantage of vanilla python is the ability to glue together a number of libraries written in other languages. In time, improvements have been made, such as putting type hints into your code SUCH AS, which helps catch errors and gives more informative function definitions.</p> <p>In general, if other people reading your code must read the high-quality comments, that you definitely remembered to write... right?, then you are laying the foundation of a codebase that will frustrate people, probably including yourself. Python is easy to write at first, but the lack of a compiler can leave your long running code to fail just at the moment it is about to save the results because your forgot you were trying to save the wrong type.</p> <p>Python does have additional tools you can use to compile it. This allows for additional verification and performance improvements, but without additional limitations and indications of your intention, it might not be possible to optimize your code as much as a language which leaves things less open to interpretation.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compilers","title":"Compilers","text":"<p>A compiler processes the given code in one or more steps. In some steps it might verify that all of your code is correct, it might perform transform and optimize your code, it might change it into different representations like byte code or machine code. Some compilers strictly function ahead-of-time in an operation like <code>some_compiler -compile my_file.code</code> and output a runnable executable, specifically for your type of machine. This is usually done once before running your code and then only when changes are made. This is called ahead-of-time compilation (AOT). Most compilers require additional constraints to transform and improve your code. Usually, you can also give your compiler additional commands to tell it how to compile. It could be things like \"please optimize my code to have a smaller executable size\" or \"please show me all warnings as errors\".</p> <p>Imagine you ask someone to go get you some milk every Thursday at 12. An unreasonably pedantic person (engineer) might be ready at 12 every Thursday and ask you what type of milk you would like today. It seems annoying and strange. You know what type of milk you like, the pedantic person should know what type of milk you like. That bastard! If you instead asked for X brand skim milk delivered at 12 every Thursday, the pedantic person might even try to optimize the process before the delivery day. If it was milk with a long expiration day, they could buy it in bulk and just have it ready for you. That unreasonably pedantic person is the compiler of whatever programming language you are using. It will go far to help you, it just doesn't perform well in ambivalent circumstances. Compilers are genereally not allowed to guess in a way that might functionally alter your code, such as reducing the level of precision.</p> <p>The languages in the compiled languages are all designed with at least one compiler, usually compiling to byte code or machine code. However, it is possible to write a compiler after the fact. Cython is one such compiler. It benefits quite a bit from having the user perform additional annotations of their python code, allowing for a decent speedup.</p> <p>Other compilers act Just-In-Time (JIT). Just as you want to run your code it will compile it. While this seems a bit weird, why not just compile it once and for all, this can allow the compiler to optimize the program specifically for your machine. The Java HotSpot VM even tries to optimize your code as it runs. If allowed to become a long-running process it can swap byte code for compiled machine code. In general, JIT compilers increase the startup time of your code, afterall, it has to compile it, just like the AOT compiler. Some JIT compilers save the compilation artifacts (the outputs of the compilation process) for later to merely reload it, but that won't help you much while you are developing your code. Some libraries and frameworks such as numba perform JIT compilation of your annotated code to optimize the performance.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compiled-languages","title":"Compiled Languages","text":"<p>In some languages like C, C++ and Rust, machine code is the outcome. That machine code can be quite platform specific, both because of the operating system and the hardware, and is binary. 1's and 0's! These three languages are not garbage collected (more on that later).</p> <p>Another quite popular language is Go, which also compiles to machine code, but is garbage collected. Julia has more of a scientific/numerical focus, but features garbage collection, JIT compilation and can use either a runtime or compile to a standalone binary.</p> <p>Other languages like Java and C# compile to something called bytecode, which can then be interpreted by a process virtual machine. Thus all Java programs compile to the same bytecode, regardless of whether it's supposed to run on a Mac or Windows platform. The bytecode is then interpreted, sometimes optimized as well, at runtime by a virtual machine written for that specific platform.</p> <p>Javascript is a just-in-time compiled language running on most web pages. It can occassionally have a reasonable speed due to the optimizing runtime. Heavy development has tuned the widely used V8 runtime to improve Javascripts performance. Writing Javascript can seem easy, but the amount of things you are allowed to do, but shouldn't, can make it an arduous experience once the errors start piling up. The advantage is highly portable code, because everything is essentially just a string... including numbers.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#the-guide-and-languages","title":"The Guide and Languages","text":"<p>As you can probably see in the column on the left... the guide will be using Rust from here on out. If you read the section on GPU programming, you will see there are no easy, one-size-fits-all, solutions. Thankfully, the guide has clear goals and limitations. To help you get familiar with new topics, we only need reasonable performance and for all the code to be runnable on most laptops. After all, it's not much fun playing around with things on someone else's computer. Most importantly, the setup process should be easy and not make you want to stress-eat the contents of your entire fridge when going through the installation process. As such the guide will mainly use Rust and the GPU API wgpu. The guide will in all cases that do not require graphics output only concern itself with pure computation through wgpu, which makes setup quite a bit simpler. wgpu is an abstraction layer that runs whatever GPU API it finds best suitable on your system. Having exact control and the absolute best performance isn't as important for the guide as allowing as many people to participate and learn as possible. After all, if it doesn't work on your laptop/desktop, you can't really play around and have fun with it!</p>"},{"location":"m0_introduction/s0_intro_to_computing/#gpu-apis-and-languages","title":"*GPU APIs and Languages","text":"<p>GPUs are some of the most readily available accelerators. Originally made for graphics, since around 2008 using them for general computation has been in focus as well. All graphics API's now also support general computation. Usually it will be called a compute shader. Shader is the common name for a GPU program. If running CUDA or OpenCL, it is called a kernel. The guide will mostly focus on the pure compute parts of GPU APIs, except for the graphics specialization. Thus it will be assumed that if you are interested in the graphics specialization you might already have done a graphics course or a tutorial such as LearnOpenGL or Learn Wgpu. It is worth noting that a compute shader using a graphics-based API, such as Vulkan, can perform just as well as an implementation in a compute-only API, such as CUDA. One example of this is VkFFT. A GPU API is all the stuff that you have to write in your code that is not the function itself that you want to run. It could be calls like creating a connection to the GPU, allocating memory on the GPU, transferring the contents of a buffer to the memory you just allocated on the GPU or launching your shader/kernel and transferring the results back to the CPU. The GPU languages themselves vary with the APIs. Some APIs, such as Vulkan, can take an intermediate representation called SPIR-V, this allows the user to write in any shading language, or even Rust in one case, as long as it is compiled to SPIR-V. Usually a shading language will look a lot like C/C++, but have its own distinct rules. You can't always make the same assumptions.</p> <p>The rest of this section is an overview of the various available GPU APIs.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#web-apis","title":"Web APIs","text":"<p>An often used strategy for making your programs as widely available as possible, is to use web-based techonology. Whatever browser you, or the end user is using supports some GPU APIs. For a long time it has been WebGL, which is a subset of OpenGL. WebGL has a version 2.0, which was finally supported by all major browsers not too long ago. The 2.0 version brought support for compute shaders with it. The modern newcomer is WebGPU which has a way of doing things that more closely resembles modern APIs such as Vulkan, DirectX 12 and Metal. It is not widely supported in browsers, outside of developer modes. Until then, the wgpu abstraction can be used. It has an API which follows the WebGPU specification, with some optional extensions for more features, but under the hood it uses whatever API it deems best for the current platform. Once the support for WebGPU becomes widespread, it can merely choose to run using WebGPU instead. In general, you will find that most frameworks or APIs which have to support a lot of things will be centered around the lowest common denominator. However, tools such as Vulkan and wgpu do allow you to query the system you are on for support of an extension, which does allow access to specialized features. You may however, end up with several versions of some elements of your code, based on whether some feature is there or not.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#platform-specific-apis","title":"Platform-Specific APIs","text":"<p>Some GPU APIs are specific to specific operating systems. DirectX11 and DirectX12 targets Windows and XBox platforms, while Metal targets Apple devices. The guide won't concern itself too much with these. DirectX11 is somewhat similar to OpenGL, while DirectX12 and Metal are from the same, more low-level, generation as Vulkan. Metal however, seems to be a bit less low-level compared to DirectX12 and Vulkan.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#cross-platform-apis","title":"Cross-Platform APIs","text":"<p>OpenGL and Vulkan are cross platform. OpenGL hasn't seen any updates for a while. Vulkan on the other hand is a low level, but generally popular API. It puts a lot of responsibility on to the programmer, but works on Windows and Linux, as well as Intel, Nvidia and AMD GPUs. It even works fairly decently on Apple devices thanks to MoltenVK Another cross-platform tool is wgpu, mentioned earlier. It is also the one that will be used in the guide for GPU code.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compute-apis","title":"Compute APIs","text":"<p>Some GPU APIs are not for graphics, such as CUDA OpenCL. OpenCL is cross-platform (works on all GPUs), as well as compiling to FPGAs, DSPs and parallelized CPU code. On the other hand CUDA is just for Nvidia GPUs. CUDA is widely used in scientific computing and mostly dominates academia. Both CUDA and OpenCL have their kernels written in a specialized version of C++.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#shader-languages","title":"Shader Languages","text":"<p>Shader languages are languages specifically tailored for the combined graphics/compute APIs. Graphics APIs have some specific functionality which the language has to support. Usually you will find support for small vectors and matrices (up to 4x4) and various types you might not find on the CPU such as fp16. They will also usually have something called textures, bindings, samplers and built-in variables. You don't need to worry about that very much in the guide. GLSL, HLSL, WGSL and MSL are all shading languages developed for graphics APIs. OpenGL, DirectX, WebGPU and Metal, respectively. GLSL is also the main language of Vulkan, but HLSL is also seeing rising popularity. Lately, the tooling for cross compiling and running the same shaders on different graphics APIs has become a lot better. Shaders can be compiled to SPIR-V, an intermediate representation, sort of like the byte code we discussed earlier. This allows the platform independent SPIR-V to be translated to the specific instructions the GPU the code is actually run on. One tool for compiling shaders is naga.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#domain-specific-languages-and-frameworks","title":"*Domain Specific Languages and Frameworks","text":"<p>Shading languages all benefit from limitations and specializations from being specifically for graphics on a GPU. Another form of limitation is domain specific languages and frameworks. One such framework you might know of is Pytorch. You are generally supposed to formulate your neural network as a graph and not just a sequence of operations. This allows PyTorch to have a clearer picture of what it is you want to do. It can check that all dimensions fit before running the training loop and it can optimize the process. Taking things even further PyTorch even has its own compiler from version 2.0.  </p> <p>Another way of achieving speedy results in a flexible format is retrofitting an existing language, in this case Python, with a slightly different language. taichi combines a domain specific language to JIT compile highly performant code, which can also run graphics, to whatever platform you are running on. It can do this because of increased requirements of the user. Namely, annotating their code and setting limitations. Halide on the other hand restricts itself to be a AOT- or JIT-compiled language embedded in C++ made specifically for working with images and tensors.</p> <p>Futhark is a language made specifically for replacing the parts of your code which need to be fast. As such it is not a general language and can make opinionated choices which allows it to generate more performant programs.</p>"},{"location":"m0_introduction/s1_intro_to_rust/","title":"Introducing Rust","text":"<p>Why use Rust for the guide? Say you've decided you want to eat 35 burgers. Python is the friend that helps you order them delivered. C++ is the friend who says 'do whatever you want'. Rust on the other hand, is the friend who stops you and then recommends you a salad. That may be annoying at times, especially if you were just craving 35 entire burgers, but it is what is best for you. Since the guide is not a beginners introduction to programming, and we will be introducing, at times fairly advanced, concepts, having a language that keeps you on the straight and narrow, even if it seems pedantic and like it is getting in your way, is a genuine advantage. If the Rust compiler accepts your code without any unsafe sections, it is probably going to work. Another point in Rust's favor was the easy setup and use on Windows, Linux and macOS. The setup time needed to be less than 10 minutes and the chosen language needed an easy-to-use, preferably integrated, package manager which didn't cause too many versioning issues. The options considered were C, C++ and Rust. C and C++ contained too many footguns and required the use of an external package manager and the use of header files and build systems. Rust takes care of all that with cargo. cargo can help you run and test your code, as well as helping downloading and building all of the dependencies in the <code>Cargo.toml</code> file, which you will find in the root of each code project. The guide was not supposed to be a guide for learning either of those languages. Rust's very helpful compiler is likely to be a boon for guiding users towards sensible code. The process of having to program in such a memory focused, modern, compiled language will turn what is otherwise an implicit, unspoken process inside out, forcing the user to think about what good code is, where is my memory, which thread has access to which data, and so on.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#setup","title":"Setup","text":"<p>To use the code in the course, as well as doing the exercises, first of all, you should have git and Rust installed.</p> <ul> <li>Install git</li> <li>Install Rust</li> </ul> <p>Once you have installed both, ensure they are properly installed by calling <code>git --version</code> and <code>cargo --version</code> in your terminal of choice. Your Rust version should be at least 1.68. At the least, the code in the guide won't function without support for the 2021 or newer version of Rust.  </p> <ul> <li>git clone the guides repository <code>git clone https://github.com/absorensen/the-real-timers-guide-to-the-computational-galaxy.git</code></li> <li>In your terminal navigate into the folder that just appeared</li> <li>Navigate to <code>m0_introduction/code/is_my_system_ready/</code></li> <li>In the command line write <code>cargo run</code>. This might take a while as cargo will now download and build some dependencies, and then compile and run the code in debug mode.</li> <li>For IDE, the guide has been developed with VS Code with the extensions rust-analyzer, CodeLLDB, Rust Syntax, WGSL, wgsl-analyzer and optionally Dracula For Rust Theme.</li> </ul>"},{"location":"m0_introduction/s1_intro_to_rust/#projects","title":"Projects","text":"<p>Projects are how a Rust codebase is organized. A project can contain subprojects, but the guide won't use this. Navigating to your commandline and writing <code>cargo new my_project</code> will create a project with a <code>Cargo.toml</code> file and directory named <code>src</code> in the root. Inside the <code>src</code> directory there will be a file named <code>main.rs</code>. The <code>Cargo.toml</code> is a file describing the name of your project and its dependencies, which will be empty to begin with. But if you write some dependencies, next time you build the code with <code>cargo build</code> or <code>cargo run</code>, with or without the <code>--release</code> flag, cargo will download and build all of the dependencies. By default cargo will look in the <code>src</code> directory. All of the files with the <code>.rs</code> suffix are Rust source files.</p> <p>Entering <code>cargo run</code> will compile and run your code in debug mode, which means it will be easier to step through the code and getting better error messages. It will also result in significantly less compilation time, but slower run time. If you add <code>cargo run --release</code> it will compile in release mode. Compilation will take longer, the code will run faster, but debugging will be harder.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#frequent-commands","title":"Frequent commands","text":"<p>To save on space, especially for some of the smaller projects where you just need to run a command or two, write <code>cargo clean</code> once you are done to remove all of the relevant dependencies.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#testing","title":"Testing","text":"<p>Cargo can also handle unit testing of code. It requires that the project is split into an application and a library part, allowing the test to just test the library. In practice what is usually recommended is to just have a very small function in your application which tells your library to start running your code.</p> <p>Navigate to the project at <code>m0_introduction/code/how_to_test/</code> to see how a typical project is setup. Try running the commands <code>cargo run</code>, followed by <code>cargo test</code>. See how the code ran with <code>cargo run</code>, but <code>cargo test</code> actually told you which functions weren't living up to expectations? Setting up these unit tests is also how the guide tells you that the code you have been asked to write is correct. When doing exercises, keep going until all of the tests pass!</p> <p></p>  Fix the values in test_function_c() and run cargo test again!  <p>Now be sure to take a minute to look at the files involved in a project and read all of the comments! Sometimes you need more info as to what went wrong with the test. There's a fix for that on Windows.</p> <p>On some computers the GPU tests will currently fail unless being run with <code>cargo test -- --test-threads=1</code> as the tests are running concurrently. All of the tests requiring GPU access will try to grab the GPU without sharing resources. Even then it might fail. You can just try a few more times or try to run tests individually.  </p>"},{"location":"m0_introduction/s1_intro_to_rust/#clippy","title":"*Clippy","text":"<p>Clippy is cargo's tool for giving suggestions for improving your code and making it more akin to idiomatic Rust. The guide has most code conformant to Clippy's suggestions, however the guide chooses to diverge where making the code simpler and easier to understand for people who have never programmed Rust before is a priority. Clippy's messages are very informative and a good learning experience. It is recommended that you use Clippy in your own code. It is as simple as calling <code>cargo clippy</code>.</p> <p>Running it on the <code>how_to_test</code> project, Clippy returns the following message -</p> <p></p>  The guide elects not to fix this, because the return statement was put there to make a point."},{"location":"m0_introduction/s1_intro_to_rust/#rustfmt","title":"*rustfmt","text":"<p>rustfmt is a formatter for Rust. Surprise! You can install it by running <code>rustup component add rustfmt</code> in a terminal. From then on you can run commands like <code>cargo fmt</code>, which automatically changes the code in your current crate (subproject, or the entire project if you are standing in the root).</p>"},{"location":"m0_introduction/s1_intro_to_rust/#fix","title":"*fix","text":"<p>fix is a tool for taking as many of those pesky compiler warnings as possible, and fixing your code for you. You just enter <code>cargo fix</code>.</p>"},{"location":"m0_introduction/s2_basic_concepts_in_rust/","title":"Basic Concepts in Rust","text":"<p>The following chapter is an absolutely barebones introduction to concepts in Rust which you will need to understand to read the guide's code. If you would like a more thorough introduction to Rust, there is a number of nice tutorials available.</p> <p>The real contents of this section is the project in <code>m0_introduction/code/basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/","title":"*Less Basic Concepts in Rust","text":"<p>The real contents of this section is the project in <code>m0_introduction/code/less_basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found</p> <p>online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#supplementary-comments","title":"Supplementary Comments","text":"<p>In this section, we'll take you through a few addendums, which aren't as much about a specific language construct, but some concepts it might help to know.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#aliasing","title":"Aliasing","text":"<p>Aliasing is a term for when two pointers or references refer to the same place in memory. That might not sound like much of a problem at first, but it allows the compiler to make optimizations. Take a look at this code -</p> <pre><code>fn compute(input: &amp;u32, output: &amp;mut u32) {\nif 10 &lt; *input {\n*output = 1;\n}\nif 5 &lt; *input {\n*output *= 2;\n}\n// remember that `output` will be `2` if `input &gt; 10`\n}\nfn compute(input: &amp;u32, output: &amp;mut u32) {\nlet cached_input: u32 = *input; // keep `*input` in a register\nif 10 &lt; cached_input {\n// If the input is greater than 10, the previous code would set the output to 1 and then double it,\n// resulting in an output of 2 (because `10&lt;&gt;` implies `5&lt;&gt;`).\n// Here, we avoid the double assignment and just set it directly to 2.\n*output = 2;\n} else if 5 &lt; cached_input {\n*output *= 2;\n}\n}\n</code></pre> <p>You can also check the Rustonomicon for a better explanation of aliasing. It is where the code snippets above are from. The code has been reformatted to preference. It may be on the more advanced side however.</p> <p>Basically, whenever you write to a value and there are multiple references to that value hidden away in different places of the memory hieararchy, such as some threads registers, or even within the same function, everything becomes invalidated. This is one of the reasons for the borrow checker adamantly enforcing that there can be multiple shared (read-only) references to a value, but only one mutable reference (read/write), and if there is a mutable reference, there cannot be any shared references to that value. If there were multiple shared references and a multiple reference it would be impossible to guarantee correctness as just when a value is retrieved from RAM by a shared reference a write to that value may have ocurred, which in order to make the sequence of operations correct might necessitate another ready, but what if it happens again? Another read! This is not what happens, you just get a program behaving \"weird\". In another case it would also mean you could not read from a value and save that value in a local variable to do a bunch of operations before writing it somewhere. It already sounds very headscratching and like you should only ever do single threaded programs. But thankfully, the borrow checker is there to keep things in check for you. One recommendation, you should try to minimize the time that a mutable reference to a value will exist.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#multiple-function-definitions-not-allowed","title":"Multiple Function Definitions Not Allowed","text":"<p>As opposed to languages like C++, you cannot have multiple functions with the same name in Rust. In C++ this is perfectly legal, and the compiler will attempt to deduce which one you mean based on the way you are calling function().</p> <pre><code>void function(int argument_a, int argument_b) {}\nvoid function(int argument_a, int argument_b, int argument_c) {}\nvoid function(int argument_a, int argument_b, float argument_c) {}\n</code></pre> <p>Rust seems to be designed in a way as to minimize the amount of ambiguity faced by the compiler (and you too). Sometimes in Rust code you will see several different constructor functions, such as build, build_from_ints, new and default. In one way, that is a pain in the ass. In another way, it's quite nice. It forces the programmer to be explicit about how the functions are different, instead of being unwritten, implicit, or 'well, you can just read the code, it's not that complicated'. If you ever think or say that. Remember this... ahem RED FLAG! Fix your stuff so people don't have to guess, it will probably make the next person to read your code hate you slightly less. Which is a good thing!</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#index-checking","title":"Index Checking","text":"<p>Whenever you access an element in an indexed collection such as a Vec:</p> <pre><code>for index in 0..data.len() {\ndo_something(data[index]);\n}\n</code></pre> <p>Whenever you do this in Rust, there is a runtime check to make sure this index is not outside of the memory of the vector. This does have some performance cost, but unless you are absolutely sure this processing is happening in a hot region (a region of your code where a lot of time is spent), it is not recommended to try and circumvent this.</p> <pre><code>for index in 0..data.len() {\nunsafe {\ndo_something(data.get_unchecked(index));\n}\n}\n</code></pre> <p>It requires an unsafe region, which is a region in your code where you tell the compiler to allow you to do some things it would otherwise not allow you to, and call the function get_unchecked(index). An unsafe region does not turn off all checking, but in general, if you are at the level of reading the guide, you don't need it and we won't be talking about it more. If you really want to read more about unsafe, the Rustonomicon is the defacto standard introduction to unsafe in Rust.</p> <p>The two above functions are equivalent to</p> <pre><code>for(int index{0}; index &lt; data.size(); ++index) {\ndo_something(data.at(index));\n}\n</code></pre> <pre><code>for(int index{0}; index &lt; data.size(); ++index)  {\ndo_something(data[index]);\n}\n</code></pre> <p>Note however, that square bracket indexing is the defacto standard way of accessing an array element in both languages. This showcases a core difference between the two languages. One being safety opt-out, and another being safety opt-in.</p>"},{"location":"m0_introduction/s4_exercises/","title":"*Exercises","text":"<p>Basic Rust exercises to come.</p>"},{"location":"m1_memory_hierarchies/","title":"Memory Hierarchies, Computational Graphs and Compilers","text":"<p>Not all efficiency comes from optimizing the various computational details like multiplications, divisions and such of a function. Quite a large part of it, in fact, comes from optimizing how much you write to and read from memory. 'Which memory?' you might ask, rightfully. The answering the where, when and what of memory will be the focus of this module. We can almost always get more cores to throw at a problem, we can also, at least on the CPU, quite easily get more memory, but that does not change the amount of time it takes to get a piece of memory, only how much data we can have in memory before we have to go to a lower level, e.g. go from RAM to disk. This is even more important given the relatively slow improvement of memory over time.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#perspective","title":"Perspective","text":"<p>The further you move from simple, albeit heavy, problems such as a matrix-matrix problem to more heterogenous problems, such as training a neural network, the harder it can be to get good performance. How do you know or reason about what is where when in complex systems like PyTorch, Tensorflow, JAX, Numba and Taichi. All of these frameworks, compilers and domain specific languages have to nudge you in different directions to give them the restrictions and hints needed to let them run your code as efficiently as possible. Nudges like defining your neural network as a computational graph. If you're unsure about what a computational graph is, the basic version is that you define a bunch of operations and how they relate to each other. Like input layer, followed by linear layer, followed by ReLU. But more on that later! Other advances include PyTorch, after several attempts with various degrees of success, finally introducing a compiler for optimizing the neural network you just defined. Or the functional programming style used by JAX in conjunction with the XLA compiler.</p>"},{"location":"m1_memory_hierarchies/#memory-hierarchies","title":"Memory Hierarchies","text":"<p>So what is memory anyways? Memory in a compute context is represented in several stages, all having their own capacity and speed. In order from smallest capacity and highest speed to largest capacity and lowest speed we have the registers, the L1-L3 caches, the main memory (RAM) and the disks. The registers are the fastest and smallest of the bunch. They reside right next to the parts of the CPU that does the computations. As a rule of thumb, most of the variables you declare in the scope of your function, unless there is A LOT of variables, will be kept in registers. The caches and the main memory all work in conjunction with each other as an invisible way of speeding up the accesses to the main memory.</p> <p></p>  A simplified view of the CPU memory hierarchy.  <p>Say you load in a file from the disk. If small enough, that entire file can be kept in memory. Which is great! We could keep all of the values in a great big array which we could access, like <code>current_value = data[index]</code>. But if you just wanted to read the first 5 values in the file in a loop, it would be incredibly slow to load those 5 values over and over again all the way from memory. What happens instead is that those 5 values might have separate copies in the L3, L2 and L1 caches, perhaps even in the registers. That would speed up things greatly. Whenever we asked for the first value we would first ask the L1 cache, do you have this value? If yes, that would be a cache hit, and we would pay a small amount of time to retrieve the value. If the L1 cache did not have a valid copy of the value, we would ask the L2 cache, and so on and so on, until we reach memory. If our file was too big to fit in memory, the operating system might even virtualize (don't worry about it) the memory and go all the way to the disk or to the internet to retrieve our value. Which is just as slow as it sounds.</p> <p></p>  An example view of what CPU memory hierarchy can look like with 2 cores.  <p>To further complicate things, multicore CPU's have each CPU sharing the disk, memory and L3 cache, sometimes they also share the L2 cache with a few other CPUs. We are also at risk of each core not just reading from the same values, but what if some of them modified one or more of the 5 values? At any point in time a value loaded from memory, to L3 cache, to L2 cache, to L1 cache, to the registers of thread A, might be invalid because thread B wrote to that value. This may have updated the value in memory and in thread B's registers, L1 and L2 caches, hopefully, it also updated it in an L2 and/or L3 cache it shared with thread A, but even then we would still need to move the value from L2/L3, to thread A's L1 cache and registers for it to be valid. Which is probably not happening. Multiple threads reading from a piece of data, which one or more threads are writing to is also known as a data race. Most likely thread A will end up with a stale version of the data and will continue as if the value had never been modified. Thread A will then write its own new version of the value, or just be working off an old version, resulting in incorrect results.</p> <p></p>  An example view of what CPU memory hierarchy can look like with 8 cores.  <p>Nudging the programmer (that's you!), to better define your program, not just line-by-line, but as a whole, to constrain these sorts of contentions, is one of the myriad reasons why frameworks like PyTorch can greatly speed up your code, if you help it along.</p> <p>For a more in-depth explanation on the memory hierarchy see this chapter on Memory Hierarchy Design.</p>"},{"location":"m1_memory_hierarchies/#expanding-the-memory-hierarchy","title":"Expanding the Memory Hierarchy","text":"<p>To top it off we can expand this memory hierarchy with additional components, such as accelerators, networking and the internet! Let's start off with the GPU. It is an accelerator originally made for just computing graphics as fast as possible. It has a whole bunch of threads in it, meaning it can do very parallel work, like making every pixel of an image slightly darker. At the end of the 2000's, Nvidia saw a bunch of academics hacking the GPU to do stuff like fast fourier transforms using the fragment shader. Don't worry about what that is, but shader basically means GPU program. So Nvidia releases CUDA as a pure compute (no graphics) API for using your GPU. It only runs on Nvidia GPU's though. Transfering memory from the CPU to the GPU and back, can be a quite explicit process. Not only does the CPU need to reserve some memory for copying to the GPU, the CPU and GPU have to be synchronized which can take a while, and then the data is usually transferred across the slower (compared to memory and cache) PCIe bus. It is certainly one you should always be thinking about if you are using a GPU for your program. Neglecting transfers is one of the fastest ways to your code the slowest. The GPU also has its own memory hierarchy.</p> <p></p>  Image credit  <p>As you can see, this being representative of the Nvidia H100, there are 2 L2 caches and a whole bunch of smaller sections. Each of these smaller sections are a streaming multiprocessor (SM).</p> <p></p>  Image credit  <p>Here we have an L1 data cache and shared memory (more on shared memory later), shared between 128 threads. Each of these warps, Nvidia terminology for one of these four sections, have 32 threads with an L0 instruction cache, which is not matched for data. Additional accelerators exist, such as the neural engine featured in quite a lot of Apple products, and dedicated image and video processing hardware.</p> <p>Finally, you can even go outside of your current system. Two CPU's and eight GPU's could be tightly interconnected in a node, such as in the Nvidia DGX system. In the case of a DGX system everything is tightly interconnected with specialized hardware to minimize the time it takes to transfer data from one component to the other.</p> <p>Taking things even further we could be sending data between more than one node, requiring yet another layer of communication, which is going to be slower than communicating internally in your CPU or in your node. When running on clusters with multiple nodes, the data you work from might have to be fetched from one or more storage nodes, which keeps your data between batch jobs. Taking neural network training as an example, if your data set is small enough to keep fully on the compute node you only need to load the dataset to the compute node before you begin training. Even better, the data set can be small enough that it fits, along with your model, completely on the GPU, meaning less transfers, less communication and better performance.</p> <p>If you wanted to make things worse however, and have your local systems administrator put you on speed dial, you would fetch your entire data set from the internet every time you launched a job. I am absolutely certain no one has ever just downloaded a Hugging Face data set whenever they launched a job... The internet can in this way be thought of as yet another, even slower, component of the memory hierarchy. Not much good comes from the internet. Try to avoid it. Except for this guide of course, which is very helpful.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#wrapping-things-up","title":"Wrapping Things Up","text":"<p>Hopefully, this teaser hasn't scared you away from charging ahead and learning more about memory hierarchies and computational graphs. Memory hierarchies are at the center of getting good performance in pretty much all programs and it is worth spending some time on having at least a tenuous grasp of how to use them.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/","title":"Soft Memory Hierarchies","text":"Memory hierarchy of the AMD Athlon.  Image credit  <p>As mentioned in the module intro, the CPU's memory hierarchy is represented by a series of hardware components with different sizes and speeds. But don't fret, memory hierarchies and their hardware design subtleties won't be the primary focus of this module. This section will focus on the various programming constructs to better use the memory hierarchy. First off we will start bridging hardware and software.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#getting-to-the-pointer","title":"Getting to the Point(er)","text":"<p>One of the core mechanisms in using memory is the pointer! All it does is point to pieces of memory. Why? Because a pointer is basically just an address. Anti-climactic, I know, but as one of the core building blocks of computing, we need to take a bit of time to look at what it is. If you have ever tried programming in C, you will invariably have been introduced to the pointer. The examples in this heading will be in C, but don't worry, we won't even define an entire function. It is rife with opportunities for making trouble, to a degree where in Rust, which is made to be a reasonably safe language, you can't directly interact with a pointer unless you have an unsafe region around the pointer interaction. Yikes! On the other hand, you can get some of the most extreme performance by using raw pointers. So let's take a look!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#allocation","title":"Allocation","text":"<p>First of all, how do we get a pointer? Please note that checks for whether we have been given a valid pointer have been omitted. In the example below we get a pointer to a piece of memory which can hold up to 42 elements.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n</code></pre> <p>Let's break it down!</p> <pre><code>int element_count = 42;\n</code></pre> <p>We assign the number of elements to a variable in order to not have magic numbers.</p> <pre><code>int* integer_array;\n</code></pre> <p>This is actually bad practice. We have an uninitialized variable here. We could try and dereference the pointer, more on that in just a second, and try to access memory which we either don't have the right to access or which doesn't exist. The pointer at this point is likely to either be 0 or complete garbage. <code>int*</code> reads as \"a pointer to integers\" or \"address of one or more integers\".</p> <pre><code>integer_array = malloc(element_count * sizeof(int));\n</code></pre> <p>We ask for a memory allocation (malloc) from the operating system. What we get back is just a runtime dependent address. The address itself is what is known as a word. The size of the word dictates how much memory you can address in a system. If you have a 32-bit, 4 bytes, word and you use byte addressing, meaning one byte for every address, we can at most address 2GB of memory with a single word. If we have 64-bit words we can address more memory than we could possibly get. When you see something is a 32-bit or 64-bit operating system, this is why! It is also why we all of a sudden started using more than 2GB of RAM per computer in the 2000's. The address given by <code>malloc</code> will be different every time you run your code. Usually, any call to the operating system will be a very slow operation and should happen as little as possible. This can be stuff like writing to a terminal, accessing a file on disk, and so on. What we give malloc as an argument is the number of BYTES, as in 8-bits per element, we want. We want <code>element_count</code> elements which should each have a size of 32-bits (4 bytes). <code>sizeof(int)</code> returns 4. In total we ask for 168 bytes. <code>malloc</code> itself returns <code>void*</code>. Since C allows for implicit casting, what happens is that C, without us asking, changes the type to <code>int*</code>. Underlying it is the exact same thing. It is an address where 168 bytes allocated for us begins. What changes from <code>void*</code> to <code>int*</code> is how we dereference the pointer and what happens when we do.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#dereferencing","title":"Dereferencing","text":"<p>A pointer is a reference to another place in memory. Quite literally it is just a number. Dereferencing is a term for following the address to what it points to.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n*integer_array = 0;\n*(integer_array + 1) = 1;\ninteger_array[2] = 2;\ninteger_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>In this example there's three different ways of dereferencing shown.</p> <pre><code>*integer_array = 0;\n</code></pre> <p>In C, we use the <code>*</code> operator in front of the pointer to follow the address to the memory. The base pointer we got from <code>malloc</code> is the address of the first of the 42 elements in our memory. Another way of seeing it is that <code>integer_array</code> holds an address, let's say... 42. Our program now asks the CPU to write to the address 42, the number 0. So far so good. But then this happens.</p> <pre><code>*(integer_array + 1) = 1;\n</code></pre> <p>This is one of the myriad reasons why we needed to have an <code>int*</code>. If the address in <code>integer_array</code> is 42, to get the next integer element, we don't go to the address 43, which would just be the second byte of the first element. No, we want to go to the address 46, where the second element in the array begins. Since <code>integer_array</code> has the type <code>int*</code>, we have defined that each element is 4 bytes and we now have a STRIDE of 4 bytes. We also need to keep track of the size of our allocation close to the pointer itself, as trying to access an element outside of our allocation will be catastrophic, and likely result in a segmentation fault. So, no <code>integer_array[42]</code>. Back to the line on hand. We put our <code>integer_array</code> in a parentheses to make sure the dereferencing doesn't happen until after we have changed the address. So we increment the base pointer (42) with a stride of 4 (46), and then dereference (*) to assign a value of 1 to the second element in our array.</p> <pre><code>integer_array[2] = 2;\n</code></pre> <p>A short hand for the previous line, is this line. <code>integer_array[2]</code> is shorthand for <code>*(integer_array + 2)</code>.</p> <pre><code>integer_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>With these lines we manipulate the base pointer itself, by reassigning a value of the base address (42), incremented by 3 (54), before doing a simple dereferencing and assigning a value of 3. This is not a recommended way of doing things. How do we ensure that we always have the pointer to the base address? The least you can do is to copy the base pointer and increment that. Why?</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n</code></pre> <p>Because we need the address to give the memory back to the operating system.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#deallocation","title":"Deallocation","text":"<p>Once we are done with the section of memory we have so graciously been granted by the operating system, we should remember to return it to the operating system. If we don't we might get a memory leak, which is when our program uses more and more memory until the program is stopped or crashes. The operating system might keep track of the memory though and clean up once our less than stellar code terminates.</p> <p>In C, we can return our memory like this, using the free function.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(integer_array);\n</code></pre> <p>Spot the error?</p> <p>We had two pointers and forgot to <code>free</code> using the base pointer, <code>base_integer_array</code>. This is undefined behavior, which means that there are literally no definitions of what will happen. It is really bad. What we should have done was this.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(base_integer_array);\n</code></pre> <p>Note that <code>free</code> takes a <code>void*</code>. Our <code>int*</code> is cast, without us asking explicitly, to a <code>void*</code>. The operating system just wants an address. This allows the operating system to mark the section, denoted by the start of the section, and probably by its own record of the length. Note also that the address (42) held by <code>base_integer_array</code> is still in play. It is what is known as a 'dangling pointer'. We could try to dereference it after giving it to <code>free</code>, which is the notorious use after free. This is also undefined behavior as we try to access memory that is no longer accessible by our program. What we could do is to set <code>base_integer_array</code> and <code>integer_array</code> to new values to denote that they were invalid.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(base_integer_array);\nbase_integer_array = NULL;\ninteger_array = NULL;\n</code></pre> <p>This does not however, stop us from trying to dereference those pointers, but it does allow for a more general check to see whether the pointers are still valid.</p> <pre><code>if (base_integer_array != NULL){\nfree(base_integer_array);\n}\n</code></pre> <p>If this all seems a bit scary, that's because it is. Anytime a system depends on humans just not making any errors and being rockstars at everything, it's a dangerous system and you should be on guard.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#access-patterns","title":"Access Patterns","text":"<p>While it is import that you increase your understanding of what it takes to get valid, predictable, boring code. Which is the best kind. What the guide is most interested in is for you to write more performant code. An absolutely essential part of getting performant code is how we access the underlying memory. Yes, we can address memory a single byte at a time with byte addressing. But, whenever we ask for a byte, the memory is transported as a cache line through the memory hierarchy. As in, the L3, L2 and L1 cache all receive an entire cache line. That cache line is usually 64 bytes.</p> <p>What is in the cache line is dictated by cache line alignment. If for example you had made a struct (it's like an object, but just the data) like the one below and you elected to turn off the auto-alignment with <code>__attribute__ ((packed))</code></p> <pre><code>struct __attribute__ ((packed)) my_struct\n{ short first; // 2 bytes \nint second; // 4 bytes\n}\n</code></pre> <p>and you made allocated an array of <code>my_struct</code> like so</p> <pre><code>int element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 6\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>if you had an alignment of say, 8 bytes, the last two lines would result in 2 cache lines being retrieved.</p> <p></p>  Bad cache alignment.  <p>Which is not good. What we could do instead would be to pad our struct a little bit, which is the default behavior in C.</p> <pre><code>struct my_struct\n{ short first; // 2 bytes \nshort _pad; // 2 bytes\n// Usually in C it will fix this automatically, padding\n// every element to a multiple of a value. This could for example\n// be 4 bytes.\nint second; // 4 bytes\n}\nint element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 6\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>Then our alignment becomes this.</p> <p></p>  Better cache alignment.  <p>And we now only involve a single cache line. Which to remind you, is quite a bit smaller than the more standard 64 byte cache line. Now that we have that in place, let's take a look at some of the ways we can run through an array of values.</p> <p>Now that we have learned a bit about cache lines, we are equipped to talk actually talk about access patterns. I have made some Rust code for you which is located at <code>m1_memory_hierarchies/code/access_patterns/</code> or online.</p> <p>First off is sequential access. It is the one we usually strive for. We start at one end and go through every element until the end, from index 0 to the end. If everything is cache aligned, great! If not, the cost of not being aligned will probably be as low as it can be, when we aren't reusing any retrieved elements. If a value, say a 4-byte integer is spread across two cache lines, that specific value may have to be reconstructed which can be expensive.</p> <p>Next up is strided access. With strided access we only read every N elements. Based on the size of the stride and the size of the elements, it might result in each cache line only being used for a single element. In the implementations in the code there is both a non-wrapping and a wrapping stride implementation, meaning once we step over the end we wrap back around using a modulo operator. This is to ensure that it accesses the same amount of elements as the sequential access. With the non-wrapping stride we only access every N elements, but we also end up doing much less work.</p> <p>Finally, we have random access. This is basically the worst case scenario. We randomly select an element to access the same amount of times as the number of elements in the array.</p> <p></p>  Timing access patterns in Rust.  <p>Given that we just talked about cache lines, most of these numbers make good sense. Random access is catastrophic, wrapping strided access is bad, but most interestingly non-wrapping strided access, which actually accesses less elements than the others, is slower than sequential access for strides 2 and 3. With stride 4, where we are only accessing one fourth the elements of the sequential access pattern, we begin to get faster. But what do you know, sometimes the nice and predictable path, which might seem like we are doing more work actually runs faster. What a time to be alive!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#stacking-heaps-of-trouble","title":"Stacking Heaps of Trouble","text":"<p>If you aren't familiar with the stack and queue data structure types, this would be a good time to follow the link and familiarize yourself.</p> <p>The stack is not just a data structure, but also a core part of how all of the variables in your local scope are kept track of when the program enters into a function. The stack is a designated part of the memory allocated to your program. It starts at size 0. Once you enter a function, each local variable is pushed unto the stack. The stack generally requires that sizes are known at compile time. Once you call a function from within your function, the local variables are no longer accessible to the function you just entered, but once you return from that function, they are. When you enter that function, a pointer to where you called the function from is added to the stack and that function has its own local variables.</p> <p></p>  The call stack.  Image credit  <p>If you push enough of these frames unto the stack, you can get a stack overflow. This can for example happen if you write a recursive program that doesn't terminate. In general, using variables from the stack will be much faster than using variables from the heap. But we also can't return pointers to a stack variable as it might disappear or be overwritten at any moment.</p> <p>The heap, in this context, is not the actual data structure known as a heap. Instead it is a bunch of unstructured memory living in the same reserved space as the stack.</p> <p></p>  The stack and the heap sharing memory.  Image credit  <p>Thus if either one becomes too big they begin encroaching on the other. Everytime you ask for dynamically sized memory, it is allocated on the heap. This is a slow process and you have to remember to deallocate the memory to not get a memory leak. But the memory survives across functions now. If you remember the pointer examples from earlier - the memory segment we asked for lived on the heap, whereas the pointer (address) itself lived on the stack. We are allowed to keep the pointer on the stack because a pointer is a known size at compile time. We can also have arrays on the stack, but they generally need to have a size known at compile time. Moving a pointer from place to place, is also a lot cheaper than copying every single element of a large array every time ownership changes hands.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#dynamic-array","title":"Dynamic Array","text":"<p>The dynamic array is ubiquitous in C++ and Rust. It is quite often what we think about, when we think of arrays in those languages. C++ has <code>std::vector&lt;T&gt;</code> and Rust has <code>Vec&lt;T&gt;</code>. I highly recommend reading the first parts of the Rust Vec page. They are basically the same though and I will refer to them as vector from here on out. A dynamic array bundles up the behavior we saw earlier with the pointers, allocations and deallocations, but adds the ability to automatically create a new array that is larger (usually by a factor of 2) than the old array and move the old values over to the new array. The vector has three values. How much memory is in its allocation, <code>capacity</code>, how much of the memory is currently in use, <code>length</code>, and a pointer to the data which lives on the heap. The vector itself can live on the stack and make sure to free the memory it points to once the vector is dropped from the stack. The vector supports quite a few operations, but the core ones are <code>push</code>, <code>pop</code>, array access <code>[]</code>, <code>reserve</code> and <code>shrink_to_fit</code>.</p> <p>Let's start off though with how we allocate a vector (in Rust).</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\n</code></pre> <p>In this case we should get a completely empty vector. It will have a default <code>capacity</code>, because we didn't specify any capacity it should start with. Let's just say this <code>capacity</code> is 4. However, if we want to print the current size</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nprintln(\"{}\", data.len());\n</code></pre> <p>we would get an output of 0! We have a <code>capacity</code> of 4, but a <code>size</code> of 0. Meaning, we have 4 integers of 4 bytes each on the heap, but they are unitialized (containing garbage values), and we have not used any of them. If we however use <code>push</code> to add some actual data and then print</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\n</code></pre> <p>we would print the number 2. Now we have live, initialized values on the heap at indices 0 and 1. We can print them by accessing the values directly.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\nprintln(\"{}\", data[1]);\n</code></pre> <p>In this case we print 2, 0 and 1. Push finds the first unused index, which is conveniently indicated by the <code>size</code> value, increments <code>size</code> and puts the value into the designated index. If we pushed 5 values however, once we reached the 5th push, assuming the default capacity was 4, we would see the 5th push taking a lot of time compared to the other 4 pushes. In this case the vector would allocate a new memory segment on the heap with a size of 8, copy all of the values from elements 0-3 and then add the 5th value to the vector. Conversely, we can also use the <code>pop</code> function.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\ndata.pop();\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\n</code></pre> <p>Now we end up printing the values 1 and 0. In theory, a dynamic array should move to a smaller array at some point. Such as, when at a quarter of the reserved capacity. But in practice, Rust doesn't move to a smaller array unless explicitly asked to do so using the \u00b4\u00b4\u00b4shrink_to_fit\u00b4\u00b4\u00b4 function. In that case it will allocate and move to an array that is exactly the size of <code>size</code>, thus also making <code>capacity</code> the same. In practice, you should only do this for large arrays which are unlikely to see more elements added to it.</p> <p>But, in the case of knowing how many elements we actually we want to put in our vector, or at least an expcected minimum amount, we can just create the vector in a way where it has already reserved that amount of capcity. If you can at all do this, it is one of the easiest ways to get better performance as you remove a whole bunch of allocations, deallocations and copying. There's a variety of ways to control how allocation happens. The simplest one, if you know how many elements you want in your vector in advance, is to just create the vector with that capacity.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::with_capacity(5);\ndata.push(0);\ndata.push(1);\ndata.push(2);\ndata.push(3);\ndata.push(4);\n</code></pre> <p>In this case, we have been unambigously upfront about how many elements we will put in the vector. It was created with a <code>capacity</code> of 5 and a <code>size</code> of 0. We can also tell the vector to make sure we have a <code>capacity</code> of at least N. If it already has <code>capacity</code> to meet the minimum, nothing happens. If it doesn't it will allocate, copy and deallocate.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nlet element_count: usize = 42;\ndata.reserve(element_count);\nfor index in 0..element_count {\ndata.push(index as i32);\n}\n</code></pre> <p>There are more idiomatic ways to do this in Rust, which might also be faster, but you get the gist!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-vector","title":"The Vector","text":"<p>But, we aren't just interested in single lists of numbers, sometimes, we would even like a matrix. In Rust we can have fixed size, arrays defined like so:</p> <pre><code>let data: [i32; 2] = [0, 1];\n</code></pre> <p>If the sizes given to the array definition are constants, known at compile time, the array will be stack allocated. From what we have learned previously, the elements will be stored in memory in the order of 0 and 1. But what if we create a two-dimensional array?</p> <pre><code>let data: [[i32; 2]; 2] = [[0, 1], [2, 3]];\n</code></pre> <p>In Rust the elements will be ordered in memory 0, 1, 2, 3. But that is not a universal truth. This is called row-major ordering and is the standard layout in C, C++, Rust, Python and most modern languages. The alternative is column-major which is seen in Fortran and Matlab. In column-major ordering the elements would be ordered in memory as 0, 2, 1, 3. Basically, the memory will be most tightly packed in the innermost dimension. To iterate through a 3 dimensional vector, this triple for-loop would access the memory in order.</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = [\n[[1, 2], [3, 4],\n[[5, 6], [7, 8]]\n];\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\nfor x_index in 0..x_dimension {\nfor y_index in 0..y_dimension {\nfor z_index in 0..z_dimension {\nprintln(\"{}\", data[x_index][y_index][z_index]);\n}\n}\n}\n</code></pre> <p>Where as if Rust was favored column-major ordering the in-memory-order traversal would be</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = [\n[[1, 2], [3, 4],\n[[5, 6], [7, 8]]\n];\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\nfor z_index in 0..z_dimension {\nfor y_index in 0..y_dimension {\nfor x_index in 0..x_dimension {\nprintln(\"{}\", data[x_index][y_index][z_index]);\n}\n}\n}\n</code></pre> <p>If you think back to stride and cache lines, traversing our 3-dimensional array like the above in the actual case, where Rust is row-major, would be like the stride access we looked at earlier. We could also do this with nested vectors.</p> <pre><code>let mut data: Vec&lt;Vec&lt;i32&gt;&gt; = Vec::&lt;Vec&lt;i32&gt;&gt;::new();\ndata.push(vec![0, 1]);\ndata.push(vec![2, 3]);\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nfor x_index in 0..x_dimension {\nfor y_index in 0..y_dimension {\nprintln!(\"{}\", data[x_index][y_index]);\n}\n}\n</code></pre> <p>This is even worse though. We now have a 2-dimensional array, which is highly flexible, but we have to dereference two pointers for every access.</p> <p>There is another way of doing this with a vector, which is the way I will be using multi-dimensional arrays in this module. It involves using a single dimensional vector as if it had more dimensions.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(vec![0, 1, 2, 3]);\nlet column_count: usize = 2;\nlet row_count: usize = 2;\nfor x_index in 0..row_count {\nfor y_index in 0..column_count {\nprintln!(\"{}\", data[x_index * column_count + y_index]);\n}\n}\n</code></pre> <p>We just create a vector with as much room as we need and then access it with a bit of calculation. We've flattened our matrix and can now both have it dynamic and with arbitrary dimensions. We can even dynamically decide to see the matrix in a different way, for example by deciding to swap the number of columns and rows. The formula to access each element is to multiply the index by the dimensions that come after it and add it to the next index. For example with three dimensions <code>x</code>, <code>y</code> and <code>z</code>, the index would be calculated by </p> <pre><code>x_index * y_size * z_size + y_index * z_size + z_index\n</code></pre> <p>and for the two dimensions <code>x</code> and <code>y</code>, we would access the 2-dimensional matrix with</p> <pre><code>x_index * y_size + y_index\n</code></pre> <p>I really hope this makes sense. Once it clicks it is a very simple formula, if a bit wordy. Usually libraries will work like this under the surface but wrap it in an interface for you to simply access it like it was a multi-dimensional array.</p> <p>To wrap it up I have made a performance test of these approaches. The code doesn't match completely as we need bigger dimensions to get a good test. The code is at <code>m1_memory_hierarchies/code/the_vector/</code> or online.</p> <p>Implementing all of the methods described above in both row-major and column-major form, as well as an element-wise version, where we flatten the multidimensionality to save the administration of two of the for-loops, so we just get one for-loop running across a vector, we get the following numbers.</p> <p></p>  Access times for multidimensional arrays.  <p>The functions named Multi-Array are stack allocated instead of heap, which is why they are that fast. I was however unable to run them for 64x64x64 and 128x128x128. Rust refused citing a stack overflow. Interestingly as well, the element-wise function can be quite fast as it saves two of the for-loops. So, if you can, use element-wise. Otherwise, the row-major single vector function seemed to work the best. How much is saved by not having the two extra for-loops depends on how much work you are actually doing in each iteration. In this benchmark we do pretty much nothing.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#move-copy-clone-soldier-spy","title":"Move, Copy, Clone, Soldier, Spy","text":"<p>Clone Copy Move Vector Games  </p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-vector-reloaded","title":"*The Vector Reloaded","text":"<p>Strided Access and the transposition Permutations Jagged Arrays Sparse Arrays Using indices instead of pointers allow for predictability and seralization  </p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#smart-pointers","title":"*Smart pointers","text":"<p>Why smart pointers -&gt; Safety Smart pointers:  </p> <ul> <li>unique/box</li> <li>shared/rc/arc</li> <li>weak/weak</li> </ul>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#graphs-and-trees","title":"*Graphs and Trees","text":"<p>Graphs Trees Graphs and Trees using  </p> <ul> <li>Pointers</li> <li>Smart pointers</li> <li>Indices (static, dynamic issue getting a mutable reference to the collection in Rust)</li> </ul>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualized-memory-hierarchy","title":"*Virtualized Memory Hierarchy","text":"<p>Find a more formalized definition of virtualized memory The process' own virtual memory space (the stack and heap share the same memory) Stack/Heap Visualization Disk -&gt; Image addresses for training networks -&gt; Fat nodes/payload options Internet -&gt; Rendering on the internet, or pulling images from the internet  </p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#garbage-collectors","title":"*Garbage collectors","text":"<p>Reference counting How to still do memory leaks -&gt; cyclical references, but save some for the exercises Generational Garbage Collection Calling the GC yourself Object Pools</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#further-reading","title":"*Further Reading","text":"<p>An explanation of memory allocation, stack and heap in C</p> <p>A more rigorous explanation of the register, cache, main memory and virtual memory parts of the memory hierarchy.</p> <p>Check out the memory and cache specs for Apple's M1 series.</p> <p>For more about garbage collection in Python, more basic garbage collection in Pyton or garbage collection in Java.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/","title":"Computational Graphs","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#intro-to-computational-graphs-overview-of-immediate-graph-and-compiled-graph","title":"Intro to computational graphs - overview of immediate, graph and compiled graph","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#what-is-a-graph","title":"What is a graph?","text":"<p>Trees, graphs, pointers, very abstract</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#the-network-we-want-to-support","title":"The network we want to support","text":"<p>Linear Layer, ReLU, Softmax, input, output Linear f32 only, dtype=f32</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#whats-in-a-tensor","title":"What's in a tensor","text":"<p>Linear memory, sometimes called the raw view</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#data-dependencies-and-control-dependencies","title":"Data dependencies and control dependencies","text":"<p>Working on a graph Contatenation (multiple writes to the same node)</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#testing-the-correctness-of-the-nodes","title":"Testing the correctness of the nodes","text":"<p>Testing in Rust</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#inlining","title":"Inlining","text":"<p>What is inlining Perspective to kernel fusion</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#compiler-verifications-and-the-restrict-keyword","title":"*Compiler verifications and the restrict keyword","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#intermediate-representations","title":"*Intermediate representations","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#graph-representations","title":"*Graph representations","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#sperspective-to-render-graphs","title":"S*Perspective to render graphs","text":"<p>For graphics</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/","title":"Intro to GPU's","text":"<p>Expanding the memory hierarchy with an accelerator Why are GPU's everywhere?</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-hardware","title":"GPU Hardware","text":"<p>Threads Memory Transfer</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#programming-gpus","title":"Programming GPU's","text":"<p>Remove the loop where?</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#intro-to-wgpu","title":"Intro to wgpu","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#setup-of-wgpu","title":"*Setup of wgpu","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#shared-memory","title":"*Shared memory","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#synchronization","title":"*Synchronization","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#warp-shuffling-and-distributed-shared-memory","title":"*Warp Shuffling and Distributed Shared Memory","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#spir-v-glslhlsl","title":"*SPIR-V &amp; GLSL/HLSL","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/","title":"Immediate GPU computation","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-first-compute-node","title":"Building the first compute node","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#gpus-in-greater-detail","title":"GPU's in greater detail","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#pipelining-warp-divergence-occupancy-and-overlap","title":"Pipelining, Warp Divergence, Occupancy and Overlap","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-remaining-compute-nodes","title":"Building the remaining compute nodes","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#testing-the-whole-thing-in-immediate-mode","title":"Testing the whole thing in immediate mode","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#caching-shaders","title":"*Caching shaders","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/","title":"Building a Computational Graph","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#seeing-the-cpu-gpu-memory-hierarchies","title":"Seeing the CPU-GPU memory hierarchies","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#transfers","title":"Transfers","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-a-computational-graph_1","title":"Building a computational graph","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#testing-the-computational-graph","title":"Testing the computational graph","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/","title":"Building a Computational Graph Compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#seeing-the-gpu-memory-hierarchy-caches-shared-memory-and-ram","title":"Seeing the GPU memory hierarchy - caches, shared memory and RAM","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#graph-compilers-and-op-codes","title":"Graph compilers and OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#swapping-operators-for-fused-versions","title":"Swapping operators for fused versions","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#building-a-graph-compiler","title":"Building a graph compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#testing-the-graph-compiler","title":"Testing the graph compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#metaprogramming","title":"*Metaprogramming","text":"<p>Programs are just strings!</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#decomposing-to-op-codes","title":"*Decomposing to OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#a-toy-example-with-op-codes","title":"*A toy example with OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#additional-ideas-for-compiler-optimization-buffer-reusage-matrix-reusage","title":"*Additional ideas for compiler optimization, buffer reusage, matrix reusage","text":""},{"location":"m1_memory_hierarchies/s6_outro/","title":"Outro","text":""},{"location":"m1_memory_hierarchies/s6_outro/#comparing-the-work","title":"Comparing the work","text":"<p>CPU, immediate, immediate with shader caching, computational graph and compiled computational graph</p>"},{"location":"m1_memory_hierarchies/s6_outro/#how-does-this-relate-to-torchcompile","title":"How does this relate to torch.compile?","text":""},{"location":"m1_memory_hierarchies/s6_outro/#where-to-go-from-here","title":"Where to go from here?","text":""},{"location":"m1_memory_hierarchies/s6_outro/#further-reading","title":"*Further reading","text":"<p>Fun and hackable tensors in Rust Massively Parallel Fun with GPUs: Accelerating Tensors in Rust Compute Shader Glossary</p>"},{"location":"m1_memory_hierarchies/s7_exercises/","title":"*Exercises","text":""},{"location":"m1_memory_hierarchies/s7_exercises/#pen-paper-exercises","title":"Pen &amp; Paper Exercises","text":"<p>Speak to a classmate about your solutions. Which pointer kills the reference counting garbage collector? If the garbage collector implements cycle detection to depth X adding which pointer would break it? Weak pointers. Write out the memory of THIS sequence of vector operations. N dimensional indexing in 1D array</p>"},{"location":"m1_memory_hierarchies/s7_exercises/#programming","title":"Programming","text":"<p>Extend the computational graph with an inplace operation for the ReLU operator (only for the non-fused ReLU)</p> <p>The following list is sorted by expected complexity - do at least 1</p> <ul> <li>Implement a version of the linear layer functions which uses shared memory and tiling</li> <li>Add reusable buffers to the computational graph system</li> <li>Implement the tree reduction version of the sum function and add it to the softmax function. Also compare the single pass and the tree reduction performance graphs. Reference</li> <li>Implement a max pooling operator, as well as fusing with ReLU, in all levels and implement tests</li> <li>Implement a convolution operator, as well as fusing with ReLU, in all levels and implement tests</li> </ul>"},{"location":"m2_concepts_in_parallelism/","title":"Concepts in Parallelism","text":"<ul> <li>Threads</li> <li>Async</li> <li>Events</li> <li>Mutex</li> <li>Atomic</li> <li>Channels</li> <li>Data parallelism, work stealing - rayon</li> <li>Data parallelism, non-work stealing - crossbeam</li> <li>GPU</li> <li>*Branchless programming, branch prediction and pipelines</li> <li>*SIMD</li> <li>*Sparsity</li> <li>*Random Access and Monte Carlo (Gyro Dropout)</li> <li>*Sorting</li> <li>*Graph representations - pointers and indices</li> <li>*Trees using indices</li> <li>*Parallel work on graphs</li> </ul>"},{"location":"m2_concepts_in_parallelism/#exercise","title":"*Exercise","text":"<p>Describe the base architecture of the egui-winit-wgpu template. Expand on the template and program some things (needs suggestions) using some of the primitives introduced in the module</p>"},{"location":"m2_concepts_in_parallelism/#sexercise","title":"S*Exercise","text":"<p>Pick items worth a total of 3 points or more, write am interpretation of each item of at least 10 times the number of points lines.</p> <ul> <li>1 - Data-oriented design - Entity component systems</li> <li>1 - Array of Structs, Structs of Arrays, Auto-Vectorization</li> <li>1 - Linearized octrees</li> <li>2 - Sorting kernels in divergent workloads - Wavefront path tracing</li> <li>4 - ORB-SLAM - design and a warning about trying to code it</li> <li>4 - Nanite</li> <li>1 - PyTorch - Data-Distributed-Parallelism</li> <li>1 - PyTorch - Model-Distributed-Parallelism</li> <li>1 - PyTorch - Optimizing inference</li> <li>2 - Shadertoy</li> <li>1 - Gyro Dropout - MLSys 2022</li> <li>1 - Hierarchical Frustum Culling</li> <li>1 - SIMD optimization</li> <li>2 - Flash Attention</li> <li>2 - Custom memory allocators</li> <li>2 - JAX</li> <li>1 - Branch Prediction</li> </ul>"},{"location":"m3_types/","title":"Types","text":""},{"location":"m3_types/#types-energy-usage-and-inference-quantization-sparsity-and-pruning-of-neural-networks","title":"Types, energy usage and inference, quantization, sparsity and pruning of neural networks","text":"<ul> <li>Floats</li> <li>Float precision</li> <li>Integers</li> <li>Energy usage (Vivienne Sze and the controversial paper)</li> <li>Inference</li> <li>Quantization</li> <li>Sparsity</li> <li>Pruning</li> <li>*Fast inverse square root</li> <li>*Bit tricks</li> <li>*Basic compression</li> <li>*Batch based data processing</li> <li>*Tensor Cores</li> <li>*Using integers instead of strings in hash tables</li> </ul>"},{"location":"m3_types/#exercise","title":"*Exercise","text":"<p>Find a suitable model and inference library. Perform inference. Optimize the model and inference process. Can you do inferencing on one thread, training on another and swap in the new model? ADD SUGGESTED MODELS  </p>"},{"location":"m3_types/#additional-reading","title":"Additional Reading","text":"<p>Full-Stack, GPU-based Acceleration of Deep Learning</p>"},{"location":"m4_profilers/","title":"Profilers and Case Studies","text":""},{"location":"m4_profilers/#introduction-to-profiling-optimization-case-studies","title":"Introduction to profiling, Optimization case studies","text":"<ul> <li>Profilers (PyTorch, web, GPU, general)</li> <li>Memory bound</li> <li>Compute bound</li> <li>Timing</li> <li>Variance</li> <li>Multiple samples</li> <li>Where to measure</li> </ul>"},{"location":"m4_profilers/#specializations","title":"Specializations","text":"<ul> <li>Training a neural network</li> <li>Optimizing a neural network for inference</li> <li>Running Yolo</li> <li>Optimizing a point cloud renderer</li> <li>Optimizing a path tracer</li> </ul>"},{"location":"m4_profilers/#exercise","title":"*Exercise","text":"<p>Try out the profilers relevant to your own system with some sample programs. Now try it with some of your own code from before you started on the guide!</p>"},{"location":"m4_profilers/#sgroup-discussion-and-presentation","title":"S*Group discussion and presentation","text":"<p>Pick one of the following topics. Read and understand it, then present and discuss the topic with one or more other people. Preferably classmates.</p> <ul> <li>Packing bits for atomic operators</li> <li>Inverse depth buffers</li> <li>Bittricks, packing normals and colors</li> <li>Morton codes / Z-order curves, tiling and GPU textures</li> <li>Calculating compression precision in a lossy point cloud compression scheme</li> <li>DLSS</li> <li>Real-Time Texture Decompression and Upsampling</li> <li>2:4 sparsity with Tensor Cores</li> </ul>"},{"location":"m5_projects/","title":"Tips and Tricks in Real-Time Systems","text":"<ul> <li>Events</li> <li>Key and Mouse events</li> <li>Event Loops</li> <li>GUIs &amp; egui</li> <li>memcpy</li> <li>Check/validate everything before the hot loop</li> <li>Hot loops, event loops</li> <li>Allocations in a hot loop</li> <li>Object Pools</li> <li>System calls - hoist out of the hot loop</li> <li>Logging and printing</li> <li>Bindings - PyO3 and cxx</li> <li>Walk, don't run, testing for correctness before optimization</li> <li>Don't use abbreviations</li> <li>Don't talk moon man language to me, ya Blargon!</li> <li>Don't use postfix incrementation++</li> <li>When to care about software engineering and when to care about performance</li> <li>Don't use a string key/identifier or integer, when a type safe enum will do the job</li> <li>Hard coding types</li> <li>Cognitive load, and delaying errors to after the first draft - deliberate development vs. debugging</li> <li>Prefer stateless programming, minimize stateful programming (functional inspiration)</li> <li>Implicit casting</li> <li>Compression</li> <li>Know your system - mobile, laptop, desktop, integrated memory, which GPU</li> <li>Use version control even for solo development</li> <li>Am I copying/cloning things that don't need to be copied?</li> <li>Anything that can be immutable, should be immutable - aliasing!</li> <li>Testing and Seeding RNG's</li> <li>Faster RNG</li> <li>Timing real-time systems and how to escape or offload compute</li> <li>Multi-resolution computing for making your real-time target</li> <li>Pressure testing and worst cases</li> <li>Static/Dynamic Dispatch - dyn, enum, trait</li> <li>The Markov Chain</li> <li>If using recursion and risking stack overflow, use a loop and a queue</li> <li>If you can, always prefer ana array over more complicated structures</li> </ul>"},{"location":"m5_projects/#sprojects","title":"S*Projects","text":"<p>This whole module is for levels 3 and 4. Specifically, all of the tips and tracks are for level 3, where the projects themselves are meant for level 4.</p>"},{"location":"m5_projects/#how-to-create-real-time-systems-good-frameworks-for-the-different-fields-and-project-proposals","title":"How to create real time systems, good frameworks for the different fields and project proposals","text":"<ul> <li>Starting with a simple prototype</li> <li>Identify your components</li> <li>Single threaded correct implementation -&gt; Testing to avoid regression</li> <li>Optimize</li> </ul>"},{"location":"m5_projects/#what-makes-for-a-good-project","title":"What makes for a good project?","text":"<ul> <li>What is your concept/project?</li> <li>Which concepts from the previous material do you think are relevant to your project and why?</li> <li>Preprocessing your data?</li> <li>How do you adapt to your chosen/available platform?</li> <li>Which libraries did you choose for this problem?</li> <li>How fast did you get to your minimum viable product?</li> <li>Which steps did you take from there and why?</li> <li>How did you determine which parts of your system to optimize?</li> <li>What else would you like to do with your system?</li> </ul>"},{"location":"m5_projects/#sproject-proposals","title":"SProject Proposals","text":"<ul> <li>Virtual 3D scanner for a point cloud dataset</li> <li>EEG system</li> <li>Change the latent variables in a network using GUI, optimize the network</li> <li>Point cloud renderer</li> <li>Real-time style transfer on a web cam feed</li> <li>Rendering fractals influenced by a web cam feed</li> <li>Eye tracking -&gt; Present to screen and read from web cam -&gt; feature extraction -&gt; classifier -&gt; intervention signal -&gt; reading app (Wolfgang Fuhl, PISTOL, fixation detection)</li> <li>Bird classification from sound / Real-time classification of sound (Xeno-canto database)</li> <li>Who is talking? Real-time classification of sound</li> <li>Are you dyslexic? Eye tracking classifier</li> <li>Cognitive load tracker - Eyes &amp; pupil dilation and online estimation of signal strength (pupils vs. sound for the hearing impaired)</li> </ul>"},{"location":"m5_projects/#components-librariesframeworks","title":"Components - libraries/frameworks","text":"<p>blessed rayon egui wonnx tch winit cv ultraviolet arewelearningyet burn time chrono hifitime </p>"}]}