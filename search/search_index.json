{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Don't Panic!","text":"<p>A guide designed for both deep learners and systems programmers. Meant to be followed several times at deepening levels. The material is comprised of 6 modules.</p> <ul> <li>Intro to the course, the different ways to use the material, intro to Rust and wgpu.</li> <li>Memory hierarchies and computational graphs</li> <li>Parallelization, interactivity, events and GUIs</li> <li>Types, energy usage and inference, quantization, sparsity and pruning of neural networks</li> <li>Introduction to profiling, optimization use cases on topics such as model training and quantization, graphics, computer vision</li> <li>How to create real time systems, frameworks for the different fields and project proposals</li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Try rust-nexttest to solve the testing issue</li> <li>Find the right benchmarking and performance tools (blessed.rs)</li> <li>Look into friendlier error handling? Perhaps logging instead of panicing to get students used to logging. Introduce anyhow for better error handling?</li> <li>Loom</li> <li>Come up with a different name for levels 1/2/3/4, also should the levels be described in a matrix?</li> <li>Should there be an introduction to basic computer architecture somewhere?</li> </ul>"},{"location":"#references-and-additional-reading","title":"References and additional reading","text":"<p>High Performance Machine Learning High Performance Machine Learning Flash Attention Branchless Programming The Rust Programming Language Learn wgpu Install Rust wgpu ShaderToy Inigo Quilez ORB-SLAM ORB-SLAM2 Z-order curves Linearised Trees on the GPU Vivienne Sze - Energy Efficient AI Visual Computing - Stanford Parallel Computing - Stanford Rust Profiling RenderDoc Book of Shaders Scratchapixel Ray Tracing in One Weekend Physically Based Rendering Crafting Interpreters Programming Rust Godbolt Advent of Code </p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<ul> <li>Nicki Skafte Detlefsen</li> <li>Lars Kai Hansen</li> <li>Pioneer Centre for AI</li> <li>Mark Bo Jensen</li> <li>Mathias Gammelmark</li> <li>Anders Jess Pedersen</li> <li>Jens Egholm Pedersen</li> </ul>"},{"location":"overview/","title":"The Guide","text":"<p>Welcome to the internet, have a look around! Anything you can think of can be found!</p>"},{"location":"m0_introduction/","title":"Introduction","text":"<p>Hello there! If you are reading this you might have been enticed by promises of performance and other some such advanced black magic, but first, a digression...</p> <p>There are so many things to keep track of as a modern day programmer and most systems hide these things from the user. You call something called... <code>?numba?</code> and annotate a few functions and it magically makes your code faster. You use something called <code>?Py?...?Torch?...</code> and it seems really slow for some reason. You're sure you have a GPU in your machine, but it's still slow. <code>PyTorch</code> has something called a profiler, but you don't know how it works and you don't know what the hell <code>DtoHMemCpy</code> even is. It can be hard to reason about what is going on inside these black boxes. On top of that you might not be able to find a tutorial or a guide to talk you through all of the stuff you don't know that you don't know. As scary as it can sometimes seem to get your hands dirty and take on what might seem an insurmountable obstacle, not doing so can have wide reaching consequences.</p> <p>With the recent mainstreamification (is that a real word?) of AI systems it used to be an elephant in the room that no one seemed to talk about, as the overlap in the Venn diagram between people interested in making computers do stuff real fast and the people doing deep learning was quite small. That overlap is getting bigger, but not swiftly enough.</p> <p>The speed of execution of a program is approximately correlated to the energy consumption of that program. Until we use 100% green, renewable, energy in all of computing we have a shared responsibility to at the very least practice some modicum of restraint and sensibility in our resource consumption. Taken to the limit by putting large scale machine learning, with its massive energy consumption for both training and inference, in everything, without necessarily generating value comensurate to the expended resources, is an irresponsible use of resources.</p> <p></p>  Image credit  <p>If someone trains a deep learning model for two weeks on eight huge data center GPUS in a cluster, it is their responsibility that that training process is fairly well optimized, and that all data is responsibly retrieved, such that that training does not have to run again because of sloppiness.</p> <p>And thus stops the finger pointing!</p> <p>Optimizing code, especially on systems you might share with others both means that you can get your results faster, but that others can have use of the system in a reasonable time as well. If you are making large models, optimizing them to be smaller also results in corporations with profits less than the GDP of a small country can actually train and run inference with your model, increasing the democratization of your work and its reach. If its able to run on a consumer grade desktop - even better!</p> <p>This guide was made to be an iterative process, taking you by the hand, speaking to you at the level at which you are following it, trying not to overwhelm you. Reading that back, it could sound a bit condescending, but it basically means that the types of concepts you are assumed to know about will gradually increase with each level. Due to the guide gradually introducing certain concepts, jumping around the material is not recommended. The guide also acknowledges that some people have different interests. As such, portions of the guide will be tailored to people who like deep learning, people who like computer vision, or computer graphics or other some such. You are more than welcome to read and do all of it, but no one says you have to do anything. If you just follow along the path that is most relevant to you, you will be just fine. The guide does contain code, sometimes just small snippets, but also frameworks in which most of a module will take place.</p> <p>Most importantly - Don't Panic! The guide is here for you! And now for something completely different... practicalities!</p>"},{"location":"m0_introduction/#specializations","title":"Specializations","text":"<p>Throughout the guide there are sections and exercises prefixed by 'S'. These exercises and topics are meant to be up to you to follow. If you are mostly interested in deep learning, by all means only read and do the sections and exercises which are relevant to deep learning. Which section and exercise is relevant to which specialization will be explained in the each section. The currently supported specializations are deep learning, computer graphics, computer vision and cognitive signal processing.</p>"},{"location":"m0_introduction/#levels","title":"Levels","text":"<p>The guide's way of doing things is to iteratively expand the curriculum and the depth at which concepts are described. You can follow the guide iteratively by doing it multiple times, each time advancing in level or you can jump right in to the relevant level.</p>"},{"location":"m0_introduction/#level-1","title":"Level 1","text":"<p>This level is for people unsure about investing the time and effort to do level 2. People are busy, and inherently looking to maximize the value given their invested time. Just for those people each module has beem boiled down to approximately 2 pages of reading. Reading all of the material should take at most an afternoon and is comprised of the bottom of the main page of each module. Basically, you could stop reading once you are done with this \"Level 1\" header and just click each \"MX - XXXX\" on the left, read that page until the end, then click on the next \"MX - XXXX\" title and read that until the end and you would be done. That does not include \"*M5 - Projects\", that one is only relevant for levels 3 and 4. Happy reading!</p>"},{"location":"m0_introduction/#level-2","title":"Level 2","text":"<p>At this level you might be a comfortable programmer in Python, you might be a researcher, or you might just be warming up for level 3. In most cases you might not be all that comfortable with lower level languages, such as C, C++ or Rust. It is expected that you checkout the repository and try out the code on your own laptop. It is expected that you might change a few variables here and there, but not much more than that. Don't worry, it does not require an Nvidia GPU to run on your laptop. There will be Rust code, but it will be as simplified Rust code as possible to just focus on making your learning as easy as possible. If you are a systems programmer, you should be able to move through this level rapidly. This level does not take into account any module, section, page or info box with a '*' in front of the name. These constitute level 3. You are still welcome to read them of course, but the language and code are a bit more advanced and it might be a bit too much if you are still working on the basics.</p>"},{"location":"m0_introduction/#level-3","title":"Level 3","text":"<p>This level is made up of all the material from level 2, all of the sections with a title prefixed by a '*' and the relevant section to your specialization in any section prefixed by 'S'. The only thing not in level 3, is any exercise section. At this level it is expected that you have experience with C/C++/Rust and that you have tried programming a GPU or that you have previously done level 2 and are up for a challenge. If you haven't done any of these things, you'll be ok, but it might take significant effort on your part.</p>"},{"location":"m0_introduction/#level-4","title":"Level 4","text":"<p>At this level everything at level 3 is expected, as well as you doing most or all of the exercises. This is for doing a course version or if you REALLY want to learn all the guide has to offer.</p>"},{"location":"m0_introduction/#how-to-use-the-materials-as-a-teacher","title":"How to use the materials as a teacher","text":"<p>If you are a teacher who wants to make use of this material, feel free to use the course site. The course focuses on teaching real-time systems for deep learners and visual systems programmers. It allocates half of 15 days to going through the material in the guide and the other half to making a project relevant to the students' specialization, which is the contents of module 5. It is designed to move through lots of different topics very quickly with a handful of varying exercises. The student is to then reach back and explore the topics relevant to their project in greater detail. The breadth of topics is quite wide, and each student shouldn't be expected to pass an exam in every topic. In most cases they might remember that a thing exists and that they can search for it. The thing they hopefully all learn is how to reason about performance, systems design and that getting your hands dirty can be both fun and invigorating.</p>"},{"location":"m0_introduction/s0_intro_to_computing/","title":"Introduction to the Computing Landscape","text":"<p>If you are new to programming, or perhaps have been able to get by using scripting languages only, you might not have been introduced to the other options. Some of the concepts presented here lay the foundations for the choices dictating the rest of the guide. Though the guide has made some clearly defined choices about which tools to use, you should at all times use the right tool for the job. Not only in which language or framework you might choose, but in how you put together and design your systems using those tools. Part of the guide's strategy is to introduce you to quite a lot of tools and concepts, also known as the learn what to Google strategy, and then going into greater detail about core concepts and concepts especially relevant to your specialization. The guide will introduce concepts that aid some programs in producing faster and results than others. An important factor is limitations. Usually, the word limitations carries a negative connotation, very few people think less freedom sounds enticing, but in computing limitations can be a wonderful thing to have and set. Especially, once you are past the first prototype. In some cases, even when prototyping.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#scripting-languages","title":"Scripting Languages","text":"<p>Chill out, take things one line of code at a time. Scripting languages aren't compiled, but run one line at a time. This leaves the system unable to look beyond the current line of code, unless you add a compiler to the mix, whic usually takes a look at all of your code.</p> <p>Python is likely the scripting language you are most familiar with. Very popular due to its apparent ease-of-use. Quite slow in its raw form. The main advantage of vanilla python is the ability to glue together a number of libraries written in other languages. In time, improvements have been made, such as putting type hints into your code SUCH AS, which helps catch errors and gives more informative function definitions.</p> <p>In general, if other people reading your code must read the high-quality comments, that you definitely remembered to write... right?, then you are laying the foundation of a codebase that will frustrate people, probably including yourself. Python is easy to write at first, but the lack of a compiler can leave your long running code to fail just at the moment it is about to save the results because your forgot you were trying to save the wrong type.</p> <p>Python does have additional tools you can use to compile it. This allows for additional verification and performance improvements, but without additional limitations and indications of your intention, it might not be possible to optimize your code as much as a language which leaves things less open to interpretation.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compilers","title":"Compilers","text":"<p>A compiler processes the given code in one or more steps. In some steps it might verify that all of your code is correct, it might perform transform and optimize your code, it might change it into different representations like byte code or machine code. Some compilers strictly function ahead-of-time in an operation like <code>some_compiler -compile my_file.code</code> and output a runnable executable, specifically for your type of machine. This is usually done once before running your code and then only when changes are made. This is called ahead-of-time compilation (AOT). Most compilers require additional constraints to transform and improve your code. Usually, you can also give your compiler additional commands to tell it how to compile. It could be things like \"please optimize my code to have a smaller executable size\" or \"please show me all warnings as errors\".</p> <p>Imagine you ask someone to go get you some milk every Thursday at 12. An unreasonably pedantic person (engineer) might be ready at 12 every Thursday and ask you what type of milk you would like today. It seems annoying and strange. You know what type of milk you like, the pedantic person should know what type of milk you like. That bastard! If you instead asked for X brand skim milk delivered at 12 every Thursday, the pedantic person might even try to optimize the process before the delivery day. If it was milk with a long expiration day, they could buy it in bulk and just have it ready for you. That unreasonably pedantic person is the compiler of whatever programming language you are using. It will go far to help you, it just doesn't perform well in ambivalent circumstances. Compilers are genereally not allowed to guess in a way that might functionally alter your code, such as reducing the level of precision.</p> <p>The languages in the compiled languages are all designed with at least one compiler, usually compiling to byte code or machine code. However, it is possible to write a compiler after the fact. Cython is one such compiler. It benefits quite a bit from having the user perform additional annotations of their python code, allowing for a decent speedup.</p> <p>Other compilers act Just-In-Time (JIT). Just as you want to run your code it will compile it. While this seems a bit weird, why not just compile it once and for all, this can allow the compiler to optimize the program specifically for your machine. The Java HotSpot VM even tries to optimize your code as it runs. If allowed to become a long-running process it can swap byte code for compiled machine code. In general, JIT compilers increase the startup time of your code, afterall, it has to compile it, just like the AOT compiler. Some JIT compilers save the compilation artifacts (the outputs of the compilation process) for later to merely reload it, but that won't help you much while you are developing your code. Some libraries and frameworks such as numba perform JIT compilation of your annotated code to optimize the performance.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compiled-languages","title":"Compiled Languages","text":"<p>In some languages like C, C++ and Rust, machine code is the outcome. That machine code can be quite platform specific, both because of the operating system and the hardware, and is binary. 1's and 0's! These three languages are not garbage collected (more on that later).</p> <p>Another quite popular language is Go, which also compiles to machine code, but is garbage collected. Julia has more of a scientific/numerical focus, but features garbage collection, JIT compilation and can use either a runtime or compile to a standalone binary.</p> <p>Other languages like Java and C# compile to something called bytecode, which can then be interpreted by a process virtual machine. Thus all Java programs compile to the same bytecode, regardless of whether it's supposed to run on a Mac or Windows platform. The bytecode is then interpreted, sometimes optimized as well, at runtime by a virtual machine written for that specific platform.</p> <p>Javascript is a just-in-time compiled language running on most web pages. It can occassionally have a reasonable speed due to the optimizing runtime. Heavy development has tuned the widely used V8 runtime to improve Javascripts performance. Writing Javascript can seem easy, but the amount of things you are allowed to do, but shouldn't, can make it an arduous experience once the errors start piling up. The advantage is highly portable code, because everything is essentially just a string... including numbers.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#the-guide-and-languages","title":"The Guide and Languages","text":"<p>As you can probably see in the column on the left... the guide will be using Rust from here on out. If you read the section on GPU programming, you will see there are no easy, one-size-fits-all, solutions. Thankfully, the guide has clear goals and limitations. To help you get familiar with new topics, we only need reasonable performance and for all the code to be runnable on most laptops. After all, it's not much fun playing around with things on someone else's computer. Most importantly, the setup process should be easy and not make you want to stress-eat the contents of your entire fridge when going through the installation process. As such the guide will mainly use Rust and the GPU API wgpu. The guide will in all cases that do not require graphics output only concern itself with pure computation through wgpu, which makes setup quite a bit simpler. wgpu is an abstraction layer that runs whatever GPU API it finds best suitable on your system. Having exact control and the absolute best performance isn't as important for the guide as allowing as many people to participate and learn as possible. After all, if it doesn't work on your laptop/desktop, you can't really play around and have fun with it!</p>"},{"location":"m0_introduction/s0_intro_to_computing/#gpu-apis-and-languages","title":"*GPU APIs and Languages","text":"<p>GPUs are some of the most readily available accelerators. Originally made for graphics, since around 2008 using them for general computation has been in focus as well. All graphics API's now also support general computation. Usually it will be called a compute shader. Shader is the common name for a GPU program. If running CUDA or OpenCL, it is called a kernel. The guide will mostly focus on the pure compute parts of GPU APIs, except for the graphics specialization. Thus it will be assumed that if you are interested in the graphics specialization you might already have done a graphics course or a tutorial such as LearnOpenGL or Learn Wgpu. It is worth noting that a compute shader using a graphics-based API, such as Vulkan, can perform just as well as an implementation in a compute-only API, such as CUDA. One example of this is VkFFT. A GPU API is all the stuff that you have to write in your code that is not the function itself that you want to run. It could be calls like creating a connection to the GPU, allocating memory on the GPU, transferring the contents of a buffer to the memory you just allocated on the GPU or launching your shader/kernel and transferring the results back to the CPU. The GPU languages themselves vary with the APIs. Some APIs, such as Vulkan, can take an intermediate representation called SPIR-V, this allows the user to write in any shading language, or even Rust in one case, as long as it is compiled to SPIR-V. Usually a shading language will look a lot like C/C++, but have its own distinct rules. You can't always make the same assumptions.</p> <p>The rest of this section is an overview of the various available GPU APIs.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#web-apis","title":"Web APIs","text":"<p>An often used strategy for making your programs as widely available as possible, is to use web-based techonology. Whatever browser you, or the end user is using supports some GPU APIs. For a long time it has been WebGL, which is a subset of OpenGL. WebGL has a version 2.0, which was finally supported by all major browsers not too long ago. The 2.0 version brought support for compute shaders with it. The modern newcomer is WebGPU which has a way of doing things that more closely resembles modern APIs such as Vulkan, DirectX 12 and Metal. It is not widely supported in browsers, outside of developer modes. Until then, the wgpu abstraction can be used. It has an API which follows the WebGPU specification, with some optional extensions for more features, but under the hood it uses whatever API it deems best for the current platform. Once the support for WebGPU becomes widespread, it can merely choose to run using WebGPU instead. In general, you will find that most frameworks or APIs which have to support a lot of things will be centered around the lowest common denominator. However, tools such as Vulkan and wgpu do allow you to query the system you are on for support of an extension, which does allow access to specialized features. You may however, end up with several versions of some elements of your code, based on whether some feature is there or not.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#platform-specific-apis","title":"Platform-Specific APIs","text":"<p>Some GPU APIs are specific to specific operating systems. DirectX11 and DirectX12 targets Windows and XBox platforms, while Metal targets Apple devices. The guide won't concern itself too much with these. DirectX11 is somewhat similar to OpenGL, while DirectX12 and Metal are from the same, more low-level, generation as Vulkan. Metal however, seems to be a bit less low-level compared to DirectX12 and Vulkan.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#cross-platform-apis","title":"Cross-Platform APIs","text":"<p>OpenGL and Vulkan are cross platform. OpenGL hasn't seen any updates for a while. Vulkan on the other hand is a low level, but generally popular API. It puts a lot of responsibility on to the programmer, but works on Windows and Linux, as well as Intel, Nvidia and AMD GPUs. It even works fairly decently on Apple devices thanks to MoltenVK Another cross-platform tool is wgpu, mentioned earlier. It is also the one that will be used in the guide for GPU code.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compute-apis","title":"Compute APIs","text":"<p>Some GPU APIs are not for graphics, such as CUDA OpenCL. OpenCL is cross-platform (works on all GPUs), as well as compiling to FPGAs, DSPs and parallelized CPU code. On the other hand CUDA is just for Nvidia GPUs. CUDA is widely used in scientific computing and mostly dominates academia. Both CUDA and OpenCL have their kernels written in a specialized version of C++.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#shader-languages","title":"Shader Languages","text":"<p>Shader languages are languages specifically tailored for the combined graphics/compute APIs. Graphics APIs have some specific functionality which the language has to support. Usually you will find support for small vectors and matrices (up to 4x4) and various types you might not find on the CPU such as fp16. They will also usually have something called textures, bindings, samplers and built-in variables. You don't need to worry about that very much in the guide. GLSL, HLSL, WGSL and MSL are all shading languages developed for graphics APIs. OpenGL, DirectX, WebGPU and Metal, respectively. GLSL is also the main language of Vulkan, but HLSL is also seeing rising popularity. Lately, the tooling for cross compiling and running the same shaders on different graphics APIs has become a lot better. Shaders can be compiled to SPIR-V, an intermediate representation, sort of like the byte code we discussed earlier. This allows the platform independent SPIR-V to be translated to the specific instructions the GPU the code is actually run on. One tool for compiling shaders is naga.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#domain-specific-languages-and-frameworks","title":"*Domain Specific Languages and Frameworks","text":"<p>Shading languages all benefit from limitations and specializations from being specifically for graphics on a GPU. Another form of limitation is domain specific languages and frameworks. One such framework you might know of is Pytorch. You are generally supposed to formulate your neural network as a graph and not just a sequence of operations. This allows PyTorch to have a clearer picture of what it is you want to do. It can check that all dimensions fit before running the training loop and it can optimize the process. Taking things even further PyTorch even has its own compiler from version 2.0.  </p> <p>Another way of achieving speedy results in a flexible format is retrofitting an existing language, in this case Python, with a slightly different language. taichi combines a domain specific language to JIT compile highly performant code, which can also run graphics, to whatever platform you are running on. It can do this because of increased requirements of the user. Namely, annotating their code and setting limitations. Halide on the other hand restricts itself to be a AOT- or JIT-compiled language embedded in C++ made specifically for working with images and tensors.</p> <p>Futhark is a language made specifically for replacing the parts of your code which need to be fast. As such it is not a general language and can make opinionated choices which allows it to generate more performant programs.</p>"},{"location":"m0_introduction/s1_intro_to_rust/","title":"Introducing Rust","text":"<p>Why use Rust for the guide? Say you've decided you want to eat 35 burgers. Python is the friend that helps you order them delivered. C++ is the friend who says 'do whatever you want'. Rust on the other hand, is the friend who stops you and then recommends you a salad. That may be annoying at times, especially if you were just craving 35 entire burgers, but it is what is best for you. Since the guide is not a beginners introduction to programming, and we will be introducing, at times fairly advanced, concepts, having a language that keeps you on the straight and narrow, even if it seems pedantic and like it is getting in your way, is a genuine advantage. If the Rust compiler accepts your code without any unsafe sections, it is probably going to work. Another point in Rust's favor was the easy setup and use on Windows, Linux and macOS. The setup time needed to be less than 10 minutes and the chosen language needed an easy-to-use, preferably integrated, package manager which didn't cause too many versioning issues. The options considered were C, C++ and Rust. C and C++ contained too many footguns and required the use of an external package manager and the use of header files and build systems. Rust takes care of all that with cargo. cargo can help you run and test your code, as well as helping downloading and building all of the dependencies in the <code>Cargo.toml</code> file, which you will find in the root of each code project. The guide was not supposed to be a guide for learning either of those languages. Rust's very helpful compiler is likely to be a boon for guiding users towards sensible code. The process of having to program in such a memory focused, modern, compiled language will turn what is otherwise an implicit, unspoken process inside out, forcing the user to think about what good code is, where is my memory, which thread has access to which data, and so on.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#setup","title":"Setup","text":"<p>To use the code in the course, as well as doing the exercises, first of all, you should have git and Rust installed.</p> <ul> <li>Install git</li> <li>Install Rust</li> </ul> <p>Once you have installed both, ensure they are properly installed by calling <code>git --version</code> and <code>cargo --version</code> in your terminal of choice. Your Rust version should be at least 1.68. At the least, the code in the guide won't function without support for the 2021 or newer version of Rust.  </p> <ul> <li>git clone the guides repository <code>git clone https://github.com/absorensen/the-real-timers-guide-to-the-computational-galaxy.git</code></li> <li>In your terminal navigate into the folder that just appeared</li> <li>Navigate to <code>m0_introduction/code/is_my_system_ready/</code></li> <li>In the command line write <code>cargo run</code>. This might take a while as cargo will now download and build some dependencies, and then compile and run the code in debug mode.</li> <li>For IDE, the guide has been developed with VS Code with the extensions rust-analyzer, CodeLLDB, Rust Syntax, WGSL, wgsl-analyzer and optionally Dracula For Rust Theme.</li> </ul>"},{"location":"m0_introduction/s1_intro_to_rust/#projects","title":"Projects","text":"<p>Projects are how a Rust codebase is organized. A project can contain subprojects, but the guide won't use this. Navigating to your commandline and writing <code>cargo new my_project</code> will create a project with a <code>Cargo.toml</code> file and directory named <code>src</code> in the root. Inside the <code>src</code> directory there will be a file named <code>main.rs</code>. The <code>Cargo.toml</code> is a file describing the name of your project and its dependencies, which will be empty to begin with. But if you write some dependencies, next time you build the code with <code>cargo build</code> or <code>cargo run</code>, with or without the <code>--release</code> flag, cargo will download and build all of the dependencies. By default cargo will look in the <code>src</code> directory. All of the files with the <code>.rs</code> suffix are Rust source files.</p> <p>Entering <code>cargo run</code> will compile and run your code in debug mode, which means it will be easier to step through the code and getting better error messages. It will also result in significantly less compilation time, but slower run time. If you add <code>cargo run --release</code> it will compile in release mode. Compilation will take longer, the code will run faster, but debugging will be harder.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#frequent-commands","title":"Frequent commands","text":"<p>To save on space, especially for some of the smaller projects where you just need to run a command or two, write <code>cargo clean</code> once you are done to remove all of the relevant dependencies.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#testing","title":"Testing","text":"<p>Cargo can also handle unit testing of code. It requires that the project is split into an application and a library part, allowing the test to just test the library. In practice what is usually recommended is to just have a very small function in your application which tells your library to start running your code.</p> <p>Navigate to the project at <code>m0_introduction/code/how_to_test/</code> to see how a typical project is setup. Try running the commands <code>cargo run</code>, followed by <code>cargo test</code>. See how the code ran with <code>cargo run</code>, but <code>cargo test</code> actually told you which functions weren't living up to expectations? Setting up these unit tests is also how the guide tells you that the code you have been asked to write is correct. When doing exercises, keep going until all of the tests pass!</p> <p></p>  Fix the values in test_function_c() and run cargo test again!  <p>Now be sure to take a minute to look at the files involved in a project and read all of the comments! Sometimes you need more info as to what went wrong with the test. There's a fix for that on Windows.</p> <p>On some computers the GPU tests will currently fail unless being run with <code>cargo test -- --test-threads=1</code> as the tests are running concurrently. All of the tests requiring GPU access will try to grab the GPU without sharing resources. Even then it might fail. You can just try a few more times or try to run tests individually.  </p>"},{"location":"m0_introduction/s1_intro_to_rust/#clippy","title":"*Clippy","text":"<p>Clippy is cargo's tool for giving suggestions for improving your code and making it more akin to idiomatic Rust. The guide has most code conformant to Clippy's suggestions, however the guide chooses to diverge where making the code simpler and easier to understand for people who have never programmed Rust before is a priority. Clippy's messages are very informative and a good learning experience. It is recommended that you use Clippy in your own code. It is as simple as calling <code>cargo clippy</code>.</p> <p>Running it on the <code>how_to_test</code> project, Clippy returns the following message -</p> <p></p>  The guide elects not to fix this, because the return statement was put there to make a point."},{"location":"m0_introduction/s1_intro_to_rust/#rustfmt","title":"*rustfmt","text":"<p>rustfmt is a formatter for Rust. Surprise! You can install it by running <code>rustup component add rustfmt</code> in a terminal. From then on you can run commands like <code>cargo fmt</code>, which automatically changes the code in your current crate (subproject, or the entire project if you are standing in the root).</p>"},{"location":"m0_introduction/s1_intro_to_rust/#fix","title":"*fix","text":"<p>fix is a tool for taking as many of those pesky compiler warnings as possible, and fixing your code for you. You just enter <code>cargo fix</code>.</p>"},{"location":"m0_introduction/s2_basic_concepts_in_rust/","title":"Basic Concepts in Rust","text":"<p>The following chapter is an absolutely barebones introduction to concepts in Rust which you will need to understand to read the guide's code. If you would like a more thorough introduction to Rust, there is a number of nice tutorials available.</p> <p>The real contents of this section is the project in <code>m0_introduction/code/basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/","title":"*Less Basic Concepts in Rust","text":"<p>The real contents of this section is the project in <code>m0_introduction/code/less_basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found</p> <p>online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#supplementary-comments","title":"Supplementary Comments","text":"<p>In this section, we'll take you through a few addendums, which aren't as much about a specific language construct, but some concepts it might help to know.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#aliasing","title":"Aliasing","text":"<p>Aliasing is a term for when two pointers or references refer to the same place in memory. That might not sound like much of a problem at first, but it allows the compiler to make optimizations. Take a look at this code -</p> <pre><code>fn compute(input: &amp;u32, output: &amp;mut u32) {\nif 10 &lt; *input {\n*output = 1;\n}\nif 5 &lt; *input {\n*output *= 2;\n}\n// remember that `output` will be `2` if `input &gt; 10`\n}\nfn compute(input: &amp;u32, output: &amp;mut u32) {\nlet cached_input: u32 = *input; // keep `*input` in a register\nif 10 &lt; cached_input {\n// If the input is greater than 10, the previous code would set the output to 1 and then double it,\n// resulting in an output of 2 (because `10&lt;&gt;` implies `5&lt;&gt;`).\n// Here, we avoid the double assignment and just set it directly to 2.\n*output = 2;\n} else if 5 &lt; cached_input {\n*output *= 2;\n}\n}\n</code></pre> <p>You can also check the Rustonomicon for a better explanation of aliasing. It is where the code snippets above are from. The code has been reformatted to preference. It may be on the more advanced side however.</p> <p>Basically, whenever you write to a value and there are multiple references to that value hidden away in different places of the memory hieararchy, such as some threads registers, or even within the same function, everything becomes invalidated. This is one of the reasons for the borrow checker adamantly enforcing that there can be multiple shared (read-only) references to a value, but only one mutable reference (read/write), and if there is a mutable reference, there cannot be any shared references to that value. If there were multiple shared references and a multiple reference it would be impossible to guarantee correctness as just when a value is retrieved from RAM by a shared reference a write to that value may have ocurred, which in order to make the sequence of operations correct might necessitate another ready, but what if it happens again? Another read! This is not what happens, you just get a program behaving \"weird\". In another case it would also mean you could not read from a value and save that value in a local variable to do a bunch of operations before writing it somewhere. It already sounds very headscratching and like you should only ever do single threaded programs. But thankfully, the borrow checker is there to keep things in check for you. One recommendation, you should try to minimize the time that a mutable reference to a value will exist.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#multiple-function-definitions-not-allowed","title":"Multiple Function Definitions Not Allowed","text":"<p>As opposed to languages like C++, you cannot have multiple functions with the same name in Rust. In C++ this is perfectly legal, and the compiler will attempt to deduce which one you mean based on the way you are calling function().</p> <pre><code>void function(int argument_a, int argument_b) {}\nvoid function(int argument_a, int argument_b, int argument_c) {}\nvoid function(int argument_a, int argument_b, float argument_c) {}\n</code></pre> <p>Rust seems to be designed in a way as to minimize the amount of ambiguity faced by the compiler (and you too). Sometimes in Rust code you will see several different constructor functions, such as build, build_from_ints, new and default. In one way, that is a pain in the ass. In another way, it's quite nice. It forces the programmer to be explicit about how the functions are different, instead of being unwritten, implicit, or 'well, you can just read the code, it's not that complicated'. If you ever think or say that. Remember this... ahem RED FLAG! Fix your stuff so people don't have to guess, it will probably make the next person to read your code hate you slightly less. Which is a good thing!</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#index-checking","title":"Index Checking","text":"<p>Whenever you access an element in an indexed collection such as a Vec:</p> <pre><code>for index in 0..data.len() {\ndo_something(data[index]);\n}\n</code></pre> <p>Whenever you do this in Rust, there is a runtime check to make sure this index is not outside of the memory of the vector. This does have some performance cost, but unless you are absolutely sure this processing is happening in a hot region (a region of your code where a lot of time is spent), it is not recommended to try and circumvent this.</p> <pre><code>for index in 0..data.len() {\nunsafe {\ndo_something(data.get_unchecked(index));\n}\n}\n</code></pre> <p>It requires an unsafe region, which is a region in your code where you tell the compiler to allow you to do some things it would otherwise not allow you to, and call the function get_unchecked(index). An unsafe region does not turn off all checking, but in general, if you are at the level of reading the guide, you don't need it and we won't be talking about it more. If you really want to read more about unsafe, the Rustonomicon is the defacto standard introduction to unsafe in Rust.</p> <p>The two above functions are equivalent to</p> <pre><code>for(int index{0}; index &lt; data.size(); ++index) {\ndo_something(data.at(index));\n}\n</code></pre> <pre><code>for(int index{0}; index &lt; data.size(); ++index)  {\ndo_something(data[index]);\n}\n</code></pre> <p>Note however, that square bracket indexing is the defacto standard way of accessing an array element in both languages. This showcases a core difference between the two languages. One being safety opt-out, and another being safety opt-in.</p>"},{"location":"m0_introduction/s4_exercises/","title":"*Exercises","text":"<p>Basic Rust exercises to come.</p>"},{"location":"m1_memory_hierarchies/","title":"Memory Hierarchies, Computational Graphs and Compilers","text":"<p>Not all efficiency comes from optimizing the various computational details like multiplications, divisions and such of a function. Quite a large part of it, in fact, comes from optimizing how much you write to and read from memory. 'Which memory?' you might ask, rightfully. The answering the where, when and what of memory will be the focus of this module. We can almost always get more cores to throw at a problem, we can also, at least on the CPU, quite easily get more memory, but that does not change the amount of time it takes to get a piece of memory, only how much data we can have in memory before we have to go to a lower level, e.g. go from RAM to disk. This is even more important given the relatively slow improvement of memory over time.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#perspective","title":"Perspective","text":"<p>The further you move from simple, albeit heavy, problems such as a matrix-matrix problem to more heterogenous problems, such as training a neural network, the harder it can be to get good performance. How do you know or reason about what is where when in complex systems like Pytorch, Tensorflow, JAX, Numba and Taichi. All of these frameworks, compilers and domain specific languages have to nudge you in different directions to give them the restrictions and hints needed to let them run your code as efficiently as possible. Nudges like defining your neural network as a computational graph. If you're unsure about what a computational graph is, the basic version is that you define a bunch of operations and how they relate to each other. Like input layer, followed by linear layer, followed by ReLU. But more on that later! Other advances include Pytorch, after several attempts with various degrees of success, finally introducing a compiler for optimizing the neural network you just defined. Or the functional programming style used by JAX in conjunction with the XLA compiler.</p>"},{"location":"m1_memory_hierarchies/#memory-hierarchies","title":"Memory Hierarchies","text":"<p>So what is memory anyways? Memory in a compute context is represented in several stages, all having their own capacity and speed. In order from smallest capacity and highest speed to largest capacity and lowest speed we have the registers, the L1-L3 caches, the main memory (RAM) and the disks. The registers are the fastest and smallest of the bunch. They reside right next to the parts of the CPU that does the computations. As a rule of thumb, most of the variables you declare in the scope of your function, unless there is A LOT of variables, will be kept in registers. The caches and the main memory all work in conjunction with each other as an invisible way of speeding up the accesses to the main memory.</p> <p></p>  Image credit REPLACE ME  <p>Say you load in a file from the disk. If small enough, that entire file can be kept in memory. Which is great! We could keep all of the values in a great big array which we could access, like <code>current_value = data[index]</code>. But if you just wanted to read the first 5 values in the file in a loop, it would be incredibly slow to load those 5 values over and over again all the way from memory. What happens instead is that those 5 values might have separate copies in the L3, L2 and L1 caches, perhaps even in the registers. That would speed up things greatly. Whenever we asked for the first value we would first ask the L1 cache, do you have this value? If yes, that would be a cache hit, and we would pay a small amount of time to retrieve the value. If the L1 cache did not have a valid copy of the value, we would ask the L2 cache, and so on and so on, until we reach memory. If our file was too big to fit in memory, the operating system might even virtualize (don't worry about it) the memory and go all the way to the disk or to the internet to retrieve our value. Which is just as slow as it sounds.</p> <p>To further complicate things, multicore CPU's have each CPU sharing the disk, memory and L3 cache, sometimes they also share the L2 cache with a few other CPUs. We are also at risk of each core not just reading from the same values, but what if some of them modified one or more of the 5 values? At any point in time a value loaded from memory, to L3 cache, to L2 cache, to L1 cache, to the registers of thread A, might be invalid because thread B wrote to that value. This may have updated the value in memory and in thread B's registers, L1 and L2 caches, hopefully, it also updated it in an L2 and/or L3 cache it shared with thread A, but even then we would still need to move the value from L2/L3, to thread A's L1 cache and registers for it to be valid. Which is probably not happening. Multiple threads reading from a piece of data, which one or more threads are writing to is also known as a  data race. Most likely thread A will end up with a stale version of the data and will continue as if the value had never been modified. Thread A will then write its own new version of the value, or just be working off an old version, resulting in incorrect results.</p> <p>Nudging the programmer (that's you!), to better define your program, not just line-by-line, but as a whole, to constrain these sorts of contentions, is one of the myriad reasons why frameworks like Pytorch can greatly speed up your code, if you help it along.</p> <p>For a more in-depth explanation on the memory hierarchy see this chapter on Memory Hierarchy Design.</p>"},{"location":"m1_memory_hierarchies/#expanding-the-memory-hierarchy","title":"Expanding the Memory Hierarchy","text":"<p>To top it off we can expand this memory hierarchy with additional components, such as accelerators, networking and the internet! Let's start off with the GPU. It is an accelerator originally made for just computing graphics as fast as  possible. It has a whole bunch of threads in it, meaning it can do very parallel work, like making every pixel  of an image slightly darker. At the end of the 2000's, Nvidia saw a bunch of academics hacking the GPU to do  stuff like fast fourier transforms using the fragment shader. Don't worry about what that is, but shader basically means GPU program. So Nvidia releases CUDA as a pure compute (no graphics) API for using your GPU.  It only runs on Nvidia GPU's though. Transfering memory from the CPU to the GPU and back, can be a quite explicit process. Not only does the CPU need to reserve some memory for copying to the GPU, the CPU and GPU have to be synchronized which can take a while, and then the data is usually transferred across the slower (compared to memory and cache) PCIe bus. It is certainly one you should always be thinking about if you are using a GPU for your program. Neglecting transfers is one of the fastest ways to your code the slowest. The GPU also has its own memory hierarchy.</p> <p></p>  Image credit  <p>As you can see, this being representative of the Nvidia H100, there are 2 L2 caches and a whole bunch of smaller sections. Each of these smaller sections are a streaming multiprocessor (SM).</p> <p></p>  Image credit  <p>Here we have an L1 data cache and shared memory (more on shared memory later), shared between 128 threads. Each of these warps, Nvidia terminology for one of these four sections, have 32 threads with an L0 instruction cache, which is not matched for data. Additional accelerators exist, such as the neural engine featured in quite a lot of Apple products, and dedicated image and video processing hardware.</p> <p>Finally, you can even go outside of your current system. Two CPU's and eight GPU's could be tightly interconnected in a node, such as in the Nvidia DGX system. In the case of a DGX system everything is tightly interconnected with specialized hardware to minimize the time it takes to transfer data from one component to the other.</p> <p>Taking things even further we could be sending data between more than one node, requiring yet another layer of communication, which is going to be slower than communicating internally in your CPU or in your node. When running on clusters with multiple nodes, the data you work from might have to be fetched from one or more storage nodes, which keeps your data between batch jobs. Taking neural network training as an example, if your data set is small enough to keep fully on the compute node you only need to load the dataset to the compute node before you begin training. Even better, the data set can be small enough that it fits, along with your model, completely on the GPU, meaning less transfers, less communication and better performance.</p> <p>If you wanted to make things worse however, and have your local systems administrator put you on speed dial, you would fetch your entire data set from the internet every time you launched a job. I am absolutely certain no one has ever just downloaded a Hugging Face data set whenever they launched a job...  The internet can in this way be thought of as yet another, even slower, component of the memory hierarchy. Not much good comes from the internet. Try to avoid it. Except for this guide of course, which is very helpful.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#wrapping-things-up","title":"Wrapping Things Up","text":"<p>Hopefully, this teaser hasn't scared you away from charging ahead and learning more about memory hierarchies  and computational graphs. Memory hierarchies are at the center of getting good performance in pretty much all programs and it is worth spending some time on having at least a tenuous grasp of how to use them.</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/","title":"Memory Hierarchies and the CPU","text":"<p>As mentioned in the module intro, </p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#introduction-to-memory-hierarchies","title":"Introduction to memory hierarchies","text":"<p>Go back to just CPU, registers, caches and memory. Computer diagram Register Cache RAM</p> <p></p>  Image credit REPLACE ME  <p></p>  Image credit"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#pointers-heap-and-stack-dynamic-arrays","title":"Pointers, Heap and Stack, Dynamic Arrays","text":""},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#pointers-and-allocations","title":"Pointers and Allocations","text":"<p>What is a pointer What is an allocation What is freeing  </p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#heap-and-stack","title":"Heap and Stack","text":"<p>Remember the more detailed description in the level 3 section Virtualized Memory Hierarchy. Confusingly, there is also a data structure called heap. This heap is not the same.</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#the-vector","title":"The Vector","text":"<p>Column-major, row-major. We can index however we want if we keep track of it ourselves and use 1D allocations. Keep size, allocated and pointer What happens when moving What happens when cloning Look at Programming Rust</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#move-copy-clone-soldier-spy","title":"Move, Copy, Clone, Soldier, Spy","text":"<p>Relating to Vector Move Copy Clone</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#virtualized-memory-hierarchy","title":"*Virtualized Memory Hierarchy","text":"<p>The process' own virtual memory space (the stack and heap share the same memory) Disk Internet Fat nodes Paylod missing?</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#pointers-and-smart-pointers","title":"*Pointers and smart pointers","text":"<p>What's the deal What's the problem Smart pointers use cases Graphs Trees</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#garbage-collectors","title":"*Garbage collectors","text":"<p>Reference counting Object Pools</p>"},{"location":"m1_memory_hierarchies/s0_memory_hierarchies_and_the_cpu/#further-reading","title":"*Further Reading","text":"<p>An explanation of memory allocation, stack and heap in C</p> <p>A more rigorous explanation of the register, cache, main memory and virtual memory parts of the memory hierarchy.</p> <p>Checkout the memory and cache specs for Apple's M1 series.</p> <p>For more about garbage collection in Python, more basic garbage collection in Pyton or garbage collection in Java.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/","title":"Computational Graphs","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#intro-to-computational-graphs-overview-of-immediate-graph-and-compiled-graph","title":"Intro to computational graphs - overview of immediate, graph and compiled graph","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#what-is-a-graph","title":"What is a graph?","text":"<p>Trees, graphs, pointers, very abstract</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#the-network-we-want-to-support","title":"The network we want to support","text":"<p>Linear Layer, ReLU, Softmax, input, output Linear f32 only, dtype=f32</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#whats-in-a-tensor","title":"What's in a tensor","text":"<p>Linear memory, sometimes called the raw view</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#data-dependencies-and-control-dependencies","title":"Data dependencies and control dependencies","text":"<p>Working on a graph Contatenation (multiple writes to the same node)</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#testing-the-correctness-of-the-nodes","title":"Testing the correctness of the nodes","text":"<p>Testing in Rust</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#inlining","title":"Inlining","text":"<p>What is inlining Perspective to kernel fusion</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#compiler-verifications-and-the-restrict-keyword","title":"*Compiler verifications and the restrict keyword","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#intermediate-representations","title":"*Intermediate representations","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#graph-representations","title":"*Graph representations","text":""},{"location":"m1_memory_hierarchies/s1_computational_graphs/#sperspective-to-render-graphs","title":"S*Perspective to render graphs","text":"<p>For graphics</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/","title":"Intro to GPU's","text":"<p>Expanding the memory hierarchy with an accelerator Why are GPU's everywhere?</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-hardware","title":"GPU Hardware","text":"<p>Threads Memory Transfer</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#programming-gpus","title":"Programming GPU's","text":"<p>Remove the loop where?</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#intro-to-wgpu","title":"Intro to wgpu","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#setup-of-wgpu","title":"*Setup of wgpu","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#shared-memory","title":"*Shared memory","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#synchronization","title":"*Synchronization","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#warp-shuffling-and-distributed-shared-memory","title":"*Warp Shuffling and Distributed Shared Memory","text":""},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#spir-v-glslhlsl","title":"*SPIR-V &amp; GLSL/HLSL","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/","title":"Immediate GPU computation","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-first-compute-node","title":"Building the first compute node","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#gpus-in-greater-detail","title":"GPU's in greater detail","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#pipelining-warp-divergence-occupancy-and-overlap","title":"Pipelining, Warp Divergence, Occupancy and Overlap","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-remaining-compute-nodes","title":"Building the remaining compute nodes","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#testing-the-whole-thing-in-immediate-mode","title":"Testing the whole thing in immediate mode","text":""},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#caching-shaders","title":"*Caching shaders","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/","title":"Building a Computational Graph","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#seeing-the-cpu-gpu-memory-hierarchies","title":"Seeing the CPU-GPU memory hierarchies","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#transfers","title":"Transfers","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-a-computational-graph_1","title":"Building a computational graph","text":""},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#testing-the-computational-graph","title":"Testing the computational graph","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/","title":"Building a Computational Graph Compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#seeing-the-gpu-memory-hierarchy-caches-shared-memory-and-ram","title":"Seeing the GPU memory hierarchy - caches, shared memory and RAM","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#graph-compilers-and-op-codes","title":"Graph compilers and OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#swapping-operators-for-fused-versions","title":"Swapping operators for fused versions","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#building-a-graph-compiler","title":"Building a graph compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#testing-the-graph-compiler","title":"Testing the graph compiler","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#metaprogramming","title":"*Metaprogramming","text":"<p>Programs are just strings!</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#decomposing-to-op-codes","title":"*Decomposing to OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#a-toy-example-with-op-codes","title":"*A toy example with OP codes","text":""},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#additional-ideas-for-compiler-optimization-buffer-reusage-matrix-reusage","title":"*Additional ideas for compiler optimization, buffer reusage, matrix reusage","text":""},{"location":"m1_memory_hierarchies/s6_outro/","title":"Outro","text":""},{"location":"m1_memory_hierarchies/s6_outro/#comparing-the-work","title":"Comparing the work","text":"<p>CPU, immediate, immediate with shader caching, computational graph and compiled computational graph</p>"},{"location":"m1_memory_hierarchies/s6_outro/#how-does-this-relate-to-torchcompile","title":"How does this relate to torch.compile?","text":""},{"location":"m1_memory_hierarchies/s6_outro/#where-to-go-from-here","title":"Where to go from here?","text":""},{"location":"m1_memory_hierarchies/s6_outro/#further-reading","title":"*Further reading","text":"<p>Fun and hackable tensors in Rust Massively Parallel Fun with GPUs: Accelerating Tensors in Rust Compute Shader Glossary</p>"},{"location":"m1_memory_hierarchies/s7_exercises/","title":"*Exercises","text":""},{"location":"m1_memory_hierarchies/s7_exercises/#pen-paper-exercises","title":"Pen &amp; Paper Exercises","text":"<p>Speak to a classmate about your solutions. Which pointer kills the reference counting garbage collector? If the garbage collector implements cycle detection to depth X adding which pointer would break it? Weak pointers. Write out the memory of THIS sequence of vector operations. N dimensional indexing in 1D array</p>"},{"location":"m1_memory_hierarchies/s7_exercises/#programming","title":"Programming","text":"<p>Extend the computational graph with an inplace operation for the ReLU operator (only for the non-fused ReLU)</p> <p>The following list is sorted by expected complexity - do at least 1</p> <ul> <li>Implement a version of the linear layer functions which uses shared memory and tiling</li> <li>Add reusable buffers to the computational graph system</li> <li>Implement the tree reduction version of the sum function and add it to the softmax function. Also compare the single pass and the tree reduction performance graphs. Reference</li> <li>Implement a max pooling operator, as well as fusing with ReLU, in all levels and implement tests</li> <li>Implement a convolution operator, as well as fusing with ReLU, in all levels and implement tests</li> </ul>"},{"location":"m2_concepts_in_parallelism/","title":"Concepts in Parallelism","text":"<ul> <li>Threads</li> <li>Async</li> <li>Events</li> <li>Mutex</li> <li>Atomic</li> <li>Channels</li> <li>Data parallelism, work stealing - rayon</li> <li>Data parallelism, non-work stealing - crossbeam</li> <li>GPU</li> <li>*Branchless programming, branch prediction and pipelines</li> <li>*SIMD</li> <li>*Sparsity</li> <li>*Random Access and Monte Carlo (Gyro Dropout)</li> <li>*Sorting</li> <li>*Graph representations - pointers and indices</li> <li>*Trees using indices</li> <li>*Parallel work on graphs</li> </ul>"},{"location":"m2_concepts_in_parallelism/#exercise","title":"*Exercise","text":"<p>Describe the base architecture of the egui-winit-wgpu template. Expand on the template and program some things (needs suggestions) using some of the primitives introduced in the module</p>"},{"location":"m2_concepts_in_parallelism/#sexercise","title":"S*Exercise","text":"<p>Pick items worth a total of 3 points or more, write am interpretation of each item of at least 10 times the number of points lines.</p> <ul> <li>1 - Data-oriented design - Entity component systems</li> <li>1 - Array of Structs, Structs of Arrays, Auto-Vectorization</li> <li>1 - Linearized octrees</li> <li>2 - Sorting kernels in divergent workloads - Wavefront path tracing</li> <li>4 - ORB-SLAM - design and a warning about trying to code it</li> <li>4 - Nanite</li> <li>1 - PyTorch - Data-Distributed-Parallelism</li> <li>1 - PyTorch - Model-Distributed-Parallelism</li> <li>1 - PyTorch - Optimizing inference</li> <li>2 - Shadertoy</li> <li>1 - Gyro Dropout - MLSys 2022</li> <li>1 - Hierarchical Frustum Culling</li> <li>1 - SIMD optimization</li> <li>2 - Flash Attention</li> <li>2 - Custom memory allocators</li> <li>2 - JAX</li> <li>1 - Branch Prediction</li> </ul>"},{"location":"m3_types/","title":"Types","text":""},{"location":"m3_types/#types-energy-usage-and-inference-quantization-sparsity-and-pruning-of-neural-networks","title":"Types, energy usage and inference, quantization, sparsity and pruning of neural networks","text":"<ul> <li>Floats</li> <li>Float precision</li> <li>Integers</li> <li>Energy usage (Vivienne Sze and the controversial paper)</li> <li>Inference</li> <li>Quantization</li> <li>Sparsity</li> <li>Pruning</li> <li>*Fast inverse square root</li> <li>*Bit tricks</li> <li>*Basic compression</li> <li>*Batch based data processing</li> <li>*Tensor Cores</li> <li>*Using integers instead of strings in hash tables</li> </ul>"},{"location":"m3_types/#exercise","title":"*Exercise","text":"<p>Find a suitable model and inference library. Perform inference. Optimize the model and inference process. Can you do inferencing on one thread, training on another and swap in the new model? ADD SUGGESTED MODELS  </p>"},{"location":"m3_types/#additional-reading","title":"Additional Reading","text":"<p>Full-Stack, GPU-based Acceleration of Deep Learning</p>"},{"location":"m4_profilers/","title":"Profilers and Case Studies","text":""},{"location":"m4_profilers/#introduction-to-profiling-optimization-case-studies","title":"Introduction to profiling, Optimization case studies","text":"<ul> <li>Profilers (PyTorch, web, GPU, general)</li> <li>Memory bound</li> <li>Compute bound</li> <li>Timing</li> <li>Variance</li> <li>Multiple samples</li> <li>Where to measure</li> </ul>"},{"location":"m4_profilers/#specializations","title":"Specializations","text":"<ul> <li>Training a neural network</li> <li>Optimizing a neural network for inference</li> <li>Running Yolo</li> <li>Optimizing a point cloud renderer</li> <li>Optimizing a path tracer</li> </ul>"},{"location":"m4_profilers/#exercise","title":"*Exercise","text":"<p>Try out the profilers relevant to your own system with some sample programs. Now try it with some of your own code from before you started on the guide!</p>"},{"location":"m4_profilers/#sgroup-discussion-and-presentation","title":"S*Group discussion and presentation","text":"<p>Pick one of the following topics. Read and understand it, then present and discuss the topic with one or more other people. Preferably classmates.</p> <ul> <li>Packing bits for atomic operators</li> <li>Inverse depth buffers</li> <li>Bittricks, packing normals and colors</li> <li>Morton codes / Z-order curves, tiling and GPU textures</li> <li>Calculating compression precision in a lossy point cloud compression scheme</li> <li>DLSS</li> <li>Real-Time Texture Decompression and Upsampling</li> <li>2:4 sparsity with Tensor Cores</li> </ul>"},{"location":"m5_projects/","title":"Tips and Tricks in Real-Time Systems","text":"<ul> <li>Events</li> <li>Key and Mouse events</li> <li>Event Loops</li> <li>GUIs &amp; egui</li> <li>memcpy</li> <li>Check/validate everything before the hot loop</li> <li>Hot loops, event loops</li> <li>Allocations in a hot loop</li> <li>Object Pools</li> <li>System calls - hoist out of the hot loop</li> <li>Logging and printing</li> <li>Bindings - PyO3 and cxx</li> <li>Walk, don't run, testing for correctness before optimization</li> <li>Don't use abbreviations</li> <li>Don't use postfix incrementation++</li> <li>When to care about software engineering and when to care about performance</li> <li>Don't use a string key/identifier or integer, when a type safe enum will do the job</li> <li>Hard coding types</li> <li>Cognitive load, and delaying errors to after the first draft - deliberate development vs. debugging</li> <li>Prefer stateless programming, minimize stateful programming (functional inspiration)</li> <li>Implicit casting</li> <li>Compression</li> <li>Know your system - mobile, laptop, desktop, integrated memory, which GPU</li> <li>Use version control even for solo development</li> <li>Am I copying/cloning things that don't need to be copied?</li> <li>Anything that can be immutable, should be immutable - aliasing!</li> <li>Testing and Seeding RNG's</li> <li>Faster RNG</li> <li>Timing real-time systems and how to escape or offload compute</li> <li>Multi-resolution computing for making your real-time target</li> <li>Pressure testing and worst cases</li> <li>Static/Dynamic Dispatch - dyn, enum, trait</li> <li>The Markov Chain</li> <li>If using recursion and risking stack overflow, use a loop and a queue</li> <li>If you can, always prefer ana array over more complicated structures</li> </ul>"},{"location":"m5_projects/#sprojects","title":"S*Projects","text":"<p>This whole module is for levels 3 and 4. Specifically, all of the tips and tracks are for level 3, where the projects themselves are meant for level 4.</p>"},{"location":"m5_projects/#how-to-create-real-time-systems-good-frameworks-for-the-different-fields-and-project-proposals","title":"How to create real time systems, good frameworks for the different fields and project proposals","text":"<ul> <li>Starting with a simple prototype</li> <li>Identify your components</li> <li>Single threaded correct implementation -&gt; Testing to avoid regression</li> <li>Optimize</li> </ul>"},{"location":"m5_projects/#what-makes-for-a-good-project","title":"What makes for a good project?","text":"<ul> <li>What is your concept/project?</li> <li>Which concepts from the previous material do you think are relevant to your project and why?</li> <li>Preprocessing your data?</li> <li>How do you adapt to your chosen/available platform?</li> <li>Which libraries did you choose for this problem?</li> <li>How fast did you get to your minimum viable product?</li> <li>Which steps did you take from there and why?</li> <li>How did you determine which parts of your system to optimize?</li> <li>What else would you like to do with your system?</li> </ul>"},{"location":"m5_projects/#sproject-proposals","title":"SProject Proposals","text":"<ul> <li>Virtual 3D scanner for a point cloud dataset</li> <li>EEG system</li> <li>Change the latent variables in a network using GUI, optimize the network</li> <li>Point cloud renderer</li> <li>Real-time style transfer on a web cam feed</li> <li>Rendering fractals influenced by a web cam feed</li> <li>Eye tracking -&gt; Present to screen and read from web cam -&gt; feature extraction -&gt; classifier -&gt; intervention signal -&gt; reading app (Wolfgang Fuhl, PISTOL, fixation detection)</li> <li>Bird classification from sound / Real-time classification of sound (Xeno-canto database)</li> <li>Who is talking? Real-time classification of sound</li> <li>Are you dyslexic? Eye tracking classifier</li> <li>Cognitive load tracker - Eyes &amp; pupil dilation and online estimation of signal strength (pupils vs. sound for the hearing impaired)</li> </ul>"},{"location":"m5_projects/#components-librariesframeworks","title":"Components - libraries/frameworks","text":"<p>blessed rayon egui wonnx tch winit cv ultraviolet arewelearningyet burn time chrono hifitime </p>"}]}