{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Don't Panic!","text":"<p>A guide designed for both deep learners and systems programmers. Meant to be followed several times at deepening levels. The material is comprised of 6 modules.</p> <ul> <li>Intro to the course, the different ways to use the material, intro to Rust and wgpu.</li> <li>Memory hierarchies and computational graphs</li> <li>Parallelization, interactivity, events and GUIs</li> <li>Types, energy usage and inference, quantization, sparsity and pruning of neural networks</li> <li>Introduction to profiling, optimization use cases on topics such as model training and quantization, graphics, computer vision</li> <li>How to create real time systems, frameworks for the different fields and project proposals</li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Try rust-nexttest to solve the testing issue</li> <li>Find the right benchmarking and performance tools (blessed.rs)</li> <li>Look into friendlier error handling? Perhaps logging instead of panicing to get students used to logging. Introduce anyhow for better error handling?</li> <li>Loom</li> <li>Come up with a different name for levels 1/2/3/4, also should the levels be described in a matrix?</li> </ul>"},{"location":"#references-and-additional-reading","title":"References and additional reading","text":"<p>High Performance Machine Learning High Performance Machine Learning Flash Attention Branchless Programming The Rust Programming Language Learn wgpu Install Rust wgpu ShaderToy Inigo Quilez ORB-SLAM ORB-SLAM2 Z-order curves Linearised Trees on the GPU Vivienne Sze - Energy Efficient AI Visual Computing - Stanford Parallel Computing - Stanford Rust Profiling RenderDoc Book of Shaders Scratchapixel Ray Tracing in One Weekend Physically Based Rendering Crafting Interpreters </p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<ul> <li>Nicki Skafte Detlefsen</li> <li>Lars Kai Hansen</li> <li>Pioneer Centre for AI</li> <li>Mark Bo Jensen</li> <li>Mathias Gammelmark</li> <li>Anders Jess Pedersen</li> <li>Jens Egholm Pedersen</li> </ul>"},{"location":"overview/","title":"The Guide","text":"<p>Welcome to the internet, have a look around! Anything you can think of can be found!</p>"},{"location":"m0_introduction/","title":"Introduction","text":"<p>Hello there! If you are reading this you might have been enticed by promises of performance and other some such advanced black magic, but first, a digression...</p> <p>There are so many things to keep track of as a modern day programmer and most systems hide these things from the user. You call something called... <code>?numba?</code> and annotate a few functions and it magically makes your code faster. You use something caled <code>?Py?...?Torch?...</code> and it seems really slow for some reason. It has something called a profiler, but you don't know how it works and you don't know what the hell <code>HtoDMemCpy</code> even is. It can be hard to reason about what is going on inside these black boxes. On top of that you might not be able to find a tutorial or a guide to talk you through all of the stuff you don't know that you don't know. As scary as it can sometimes seem to get your hands dirty and take on what might seem an insurmountable obstacle, not doing so can have wide reaching consequences.</p> <p>With the recent mainstreamification (is that a real word?) of AI systems it used to be an elephant in the room that no one seemed to talk about, as the overlap in the Venn diagram between people interested in making computers do stuff real fast and the people doing deep learning was quite small. That overlap is getting bigger, but not swiftly enough.</p> <p>The speed of execution of a program is approximately correlated to the energy consumption of that program. Until we use 100% green, renewable, energy in all of computing we have a shared responsibility to at the very least practice some modicum of restraint and sensibility in our resource consumption. Taken to the limit by putting large scale machine learning, with its massive energy consumption for both training and inference, in everything, without necessarily generating value comensurate to the expended resources, is an irresponsible use of resources.</p> <p></p>  Image credit  <p>If someone trains a deep learning model for two weeks on eight huge data center GPUS in a cluster, it is their responsibility that that training process is fairly well optimized, and that all data is responsibly retrieved, such that that training does not have to run again for no reason other than sloppiness.</p> <p>And thus stops the finger pointing!</p> <p>Optimizing code, especially on systems you might share with others both means that you can get your results faster, but that others can have use of the system in a reasonable time as well. If you are making large models, optimizing them to be smaller also results in corporations with profits less than the GDP of a small country can actually train and run inference with your model, increasing the democratization of your work and its reach. If its able to run on a consumer grade desktop - even better!</p> <p>This guide was made to be an iterative process, taking you by the hand, speaking to you at the level at which you are following it, trying not to overwhelm you. Reading that back, it could sound a bit condescending, but it basically means that the types of concepts you are assumed to know about will gradually increase with each level. Due to the guide gradually introducing certain concepts, jumping around the material is not recommended. The guide also acknowledges that some people have different interests. As such, portions of the guide will be tailored to people who like deep learning, people who like computer vision, or computer graphics or other some such. You are more than welcome to read and do all of it, but no one says you have to do anything. If you just follow along the path that is most relevant to you, you will be just fine. The guide does contain code, sometimes just small snippets, but also frameworks in which most of a module will take place.</p> <p>Most importantly - Don't Panic! The guide is here for you! And now for something completely different... practicalities!</p>"},{"location":"m0_introduction/#specializations","title":"Specializations","text":"<p>Throughout the guide there are sections and exercises prefixed by 'S'. These exercises and topics are meant to be up to you to follow. If you are mostly interested in deep learning, by all means only read and do the sections and exercises which are relevant to deep learning. Which section and exercise is relevant to which specialization will be explained in the each section. The currently supported specializations are deep learning, computer graphics, computer vision and cognitive signal processing.</p>"},{"location":"m0_introduction/#levels","title":"Levels","text":"<p>The guide's way of doing things is to iteratively expand the curriculum and the depth at which concepts are described. You can follow the guide iteratively by doing it multiple times, each time advancing in level or you can jump right in to the relevant level.</p>"},{"location":"m0_introduction/#level-1","title":"Level 1","text":"<p>This level is for people unsure about investing the time and effort to do level 2. People are busy, and inherently looking to maximize the value given their invested time. Just for those people each module has beem boiled down to approximately 2 pages of reading. Reading all of the material should take at most an afternoon and is comprised of the bottom of the main page of each module. Basically, you could stop reading once you are done with this \"Level 1\" header and just click each \"MX - XXXX\" on the left, read that page until the end, then click on the next \"MX - XXXX\" title and read that until the end and you would be done. That does not include \"*M5 - Projects\", that one is only relevant for levels 3 and 4. Happy reading!</p>"},{"location":"m0_introduction/#level-2","title":"Level 2","text":"<p>At this level you might be a comfortable programmer in Python, you might be a researcher, or you might just be warming up for level 3. In most cases you might not be all that comfortable with lower level languages, such as C, C++ or Rust. It is expected that you checkout the repository and try out the code on your own laptop. It is expected that you might change a few variables here and there, but not much more than that. Don't worry, it does not require an Nvidia GPU to run on your laptop. There will be Rust code, but it will be as simplified Rust code as possible to just focus on making your learning as easy as possible. If you are a systems programmer, you should be able to move through this level rapidly. This level does not take into account any module, section, page or info box with a '*' in front of the name. These constitute level 3. You are still welcome to read them of course, but the language and code are a bit more advanced and it might be a bit too much if you are still working on the basics.</p>"},{"location":"m0_introduction/#level-3","title":"Level 3","text":"<p>This level is made up of all the material from level 2, all of the sections with a title prefixed by a '*' and the relevant section to your specialization in any section prefixed by 'S'. The only thing not in level 3, is any exercise section. At this level it is expected that you have experience with C/C++/Rust and that you have tried programming a GPU or that you have previously done level 2 and are up for a challenge. If you haven't done any of these things, you'll be ok, but it might take significant effort on your part.</p>"},{"location":"m0_introduction/#level-4","title":"Level 4","text":"<p>At this level everything at level 3 is expected, as well as you doing most or all of the exercises. This is for doing a course version or if you REALLY want to learn all the guide has to offer.</p>"},{"location":"m0_introduction/#how-to-use-the-materials-as-a-teacher","title":"How to use the materials as a teacher","text":"<p>If you are a teacher who wants to make use of this material, feel free to use the course site. The course focuses on teaching real-time systems for deep learners and visual systems programmers. It allocates half of 15 days to going through the material in the guide and the other half to making a project relevant to the students' specialization, which is the contents of module 5. It is designed to move through lots of different topics very quickly with a handful of varying exercises. The student is to then reach back and explore the topics relevant to their project in greater detail. The breadth of topics is quite wide, and each student shouldn't be expected to pass an exam in every topic. In most cases they might remember that a thing exists and that they can search for it. The thing they hopefully all learn is how to reason about performance, systems design and that getting your hands dirty can be both fun and invigorating.</p>"},{"location":"m0_introduction/s0_intro_to_computing/","title":"Introduction to the Computing Landscape","text":"<p>What are programs. How do we use them. Choosing the right tools for the job. Why do some programs produce faster and safer code than others. Limitations, while usually carrying a negative connotation, is a wonderful thing to have and set in computing. Will be briefly introducing some languages and concepts. By no means an expert, but planting seeds for later.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#scripting-languages","title":"Scripting Languages","text":"<p>Chill out, take things one line of code at a time. We are left without the ability to look beyond the current line of code. Easy to write things at first, but the lack of a compiler can leave your long running code to fail just at the moment it is about to save the results because your forgot you were trying to save the wrong type.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#python","title":"Python","text":"<p>The one you are most likely to be familiar with. Very popular due to its apparent ease-of-use.  Quite slow in its raw form. The main advantage of vanilla python is the ability to glue together a number of libraries written in other languages. In time, improvements have been made, such as putting type hints into your code SUCH AS, which helps catch errors and gives more informative function definitions. In general, if other people reading your code must read the high-quality comments, that you definitely remembered to write... right?, then you are laying the foundation of a codebase that will frustrate people, probably including yourself.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#javascript","title":"Javascript","text":"<p>The one running on most web pages. Can occassionally have a reasonable speed due to the optimizing runtime. Heavy development has tuned the V8 runtime. Writing Javascript can seem easy, but the amount of things you are allowed to do, but shouldn't can make it an arduous experience, once the errors start piling up. The advantage is highly portable code, because everything is essentially just a string... including numbers. The guide won't concern itself too much with Javascript.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#compiled-languages","title":"Compiled Languages","text":"<p>Java, Go, C#, C, C++, Rust</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compilers","title":"Compilers","text":"<p>Most of them require additional constraints to transform and improve your code. They are not allowed to guess in a way that might functionally alter your code, such as reducing the level of precision.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#ahead-of-time-aot","title":"Ahead-of-Time (AOT)","text":"<p>The languages in the compiled languages are all designed with at least one compiler, usually compiling to byte code or machine code. However, it is possible to write a compiler after the fact. Cython is one such compiler. It benefits quite a bit by having the user perform additional annotations of their python code, allowing for a decent speedup.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#just-in-time-jit","title":"Just-in-Time (JIT)","text":"<p>Start-up time, compilation artifacts may be saved If allowed to become a long-running process it may optimize the code while running and substitute the function for a new optimized version. Numba Java  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#gpu-specific-languages-and-apis","title":"*GPU Specific Languages and APIs","text":"<p>CUDA/OpenCL Older APIs Web languages Modern APIs (D12, Metal, Vulkan, WebGPU(wgpu))</p>"},{"location":"m0_introduction/s0_intro_to_computing/#cuda","title":"CUDA","text":"<p>Haha! Surprise! CUDA is actually programmed in C++ with some additional functions and decorators available.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#opengl","title":"OpenGL","text":""},{"location":"m0_introduction/s0_intro_to_computing/#webgl","title":"WebGL","text":""},{"location":"m0_introduction/s0_intro_to_computing/#directx11directx12metal","title":"DirectX11/DirectX12/Metal","text":"<p>Platform specific stuff</p>"},{"location":"m0_introduction/s0_intro_to_computing/#glslhlslwgsl","title":"GLSL/HLSL/WGSL","text":"<p>Graphics and compute code. Specifically for the Can be compiled to SPIR-V, an intermediate representation, sort of like the byte code we discussed earlier. This allows the platform independent SPIR-V to be translated to the specific instructions the GPU the code is actually run on.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#domain-specific-languages-and-frameworks","title":"Domain Specific Languages and Frameworks","text":"<p>We can take this concept of setting limitations even further Including GPU programming</p>"},{"location":"m0_introduction/s0_intro_to_computing/#pytorch","title":"Pytorch","text":"<p>Pytorch Has its own compiler from 2.0.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#taichi","title":"*Taichi","text":"<p>taichi</p>"},{"location":"m0_introduction/s0_intro_to_computing/#halide","title":"*Halide","text":"<p>Halide</p>"},{"location":"m0_introduction/s0_intro_to_computing/#futhark","title":"*Futhark","text":"<p>Futhark</p>"},{"location":"m0_introduction/s0_intro_to_computing/#the-guide-and-languages","title":"The guide and languages","text":"<p>As you can probably see in the column on the left... the guide will be using Rust from here on out. If you read the section on GPU programming, you will see there are no easy, one-size-fits-all, solutions. Thankfully, the guide has clear goals and limitations. To help you get familiar with new topics, we only need reasonable performance and for all the code to be runnable on most laptops. After all, it's not much fun playing around with things on someone else's computer. Most importantly, the setup process should be easy and not make you want to stress-eat the contents of your entire fridge when going through the installation process. As such the guide will mainly use Rust and wgpu.</p>"},{"location":"m0_introduction/s1_intro_to_rust/","title":"Introducing Rust","text":"<p>Why Rust? Rust has specifically been chosen as the guide needed a systems level programming language which was easy to setup and use on Windows, Linux and macOS. The setup time needed to be less than 10 minutes and the chosen language needed an easy-to-use preferably integrated package manager. The options considered were C, C++ and Rust. C and C++ contained too many footguns. The guide was not supposed to be a guide to learning either of those languages. Rust's very helpful compiler is likely to be a boon for guiding users towards sensible code. The process of having to program in such a memory focused, modern, compiled language will turn what is otherwise an implicit, unspoken process inside out, forcing the user to think about what good code is, where is my memory, which elements has access, and so on.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#setup","title":"Setup","text":""},{"location":"m0_introduction/s1_intro_to_rust/#installation","title":"Installation","text":"<ul> <li>Install Rust. A version of Rust supporting edition 2021 is needed.</li> <li>git clone this code</li> <li>In the command line write <code>cargo run --release</code>. This might take a while.</li> <li>For IDE, I prefere VS Code with the extensions rust-analyzer, CodeLLDB, Rust Syntax, WGSL, wgsl-analyzer and optionally Dracula For Rust Theme.</li> </ul>"},{"location":"m0_introduction/s1_intro_to_rust/#testing","title":"Testing","text":"<p>On some computers the GPU tests will currently fail unless being run with <code>cargo test -- --test-threads=1</code> Even then it might fail. You can just try a few more times or try to run tests individually. It is because of the queue and device being acquired several times. This is likely to happen less on bigger GPU's.  </p>"},{"location":"m0_introduction/s1_intro_to_rust/#projects","title":"Projects","text":""},{"location":"m0_introduction/s1_intro_to_rust/#project-setup","title":"Project setup","text":""},{"location":"m0_introduction/s1_intro_to_rust/#how-to-compile","title":"How to compile","text":""},{"location":"m0_introduction/s1_intro_to_rust/#frequent-commands-and-faq","title":"Frequent commands and FAQ","text":""},{"location":"m0_introduction/s1_intro_to_rust/#clippy","title":"*Clippy","text":""},{"location":"m0_introduction/s1_intro_to_rust/#fmt","title":"*fmt","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/","title":"Basic Concepts in Rust","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#types","title":"Types","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#function-definitions","title":"Function Definitions","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#the-borrow-checker","title":"The Borrow Checker","text":"<p>Shared and mutable references</p>"},{"location":"m0_introduction/s2_basic_concepts_in_rust/#move-copy-and-clone","title":"Move, Copy and Clone","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#iterators","title":"Iterators","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#option-and-result","title":"Option and Result","text":""},{"location":"m0_introduction/s2_basic_concepts_in_rust/#enums-match","title":"Enums &amp; Match","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/","title":"*Less Basic Concepts in Rust","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#iterators","title":"Iterators","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#smart-pointers","title":"Smart pointers","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#traits","title":"Traits","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#templates","title":"Templates","text":""},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#gentle-lifetimes","title":"Gentle Lifetimes","text":""},{"location":"m1_memory_hierarchies/","title":"Memory Hierarchies, Computational Graphs and Compilers","text":"<p>Why memory hierarchies and computational graphs?</p>"},{"location":"m1_memory_hierarchies/s0_intro/","title":"Intro","text":"<p>What will this module go through.</p>"},{"location":"m1_memory_hierarchies/s0_intro/#getting-started","title":"Getting Started","text":"<p>The code framework has benchmarking and functions. Most of them are variations of the same operators.</p>"},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/","title":"Memory Hierarchies and the CPU","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#introduction-to-memory-hierarchies","title":"Introduction  to memory hierarchies","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#pointers-heap-and-stack-dynamic-arrays","title":"Pointers, Heap and Stack, Dynamic Arrays","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#the-cpu-side-memory-hierarchy","title":"The CPU-side memory hierarchy","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#virtualized-memory","title":"*Virtualized memory","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#pipelines-and-branch-prediction","title":"Pipelines and branch prediction","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#inlining","title":"Inlining","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#pointers-and-smart-pointers","title":"*Pointers and smart pointers","text":""},{"location":"m1_memory_hierarchies/s1_memory_hierarchies_and_the_cpu/#garbage-collectors","title":"*Garbage collectors","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/","title":"Computational Graphs","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#intro-to-computational-graphs-overview-of-immediate-graph-and-compiled-graph","title":"Intro to computational graphs - overview of immediate, graph and compiled graph","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#the-network-we-want-to-support","title":"The network we want to support","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#whats-in-a-tensor","title":"What's in a tensor","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#data-dependencies-and-control-dependencies","title":"Data dependencies and control dependencies","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#compiler-verifications-and-the-restrict-keyword","title":"*Compiler verifications and the restrict keyword","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#testing-the-correctness-of-the-nodes","title":"Testing the correctness of the nodes","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#intermediate-representations","title":"*Intermediate representations","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#graph-representations","title":"*Graph representations","text":""},{"location":"m1_memory_hierarchies/s2_computational_graphs/#perspective-to-render-graphs","title":"*Perspective to render graphs","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/","title":"Intro to GPU's","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#gpu-hardware","title":"GPU Hardware","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#programming-gpus","title":"Programming GPU's","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#intro-to-wgpu","title":"Intro to wgpu","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#setup-of-wgpu","title":"*Setup of wgpu","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#shared-memory","title":"*Shared memory","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#synchronization","title":"*Synchronization","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#warp-shuffling-and-distributed-shared-memory","title":"*Warp Shuffling and Distributed Shared Memory","text":""},{"location":"m1_memory_hierarchies/s3_intro_to_gpus/#spir-v-glslhlsl","title":"*SPIR-V &amp; GLSL/HLSL","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/","title":"Immediate GPU computation","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#building-the-first-compute-node","title":"Building the first compute node","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#gpus-in-greater-detail","title":"GPU's in greater detail","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#pipelining-warp-divergence-occupancy-and-overlap","title":"Pipelining, Warp Divergence, Occupancy and Overlap","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#building-the-remaining-compute-nodes","title":"Building the remaining compute nodes","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#testing-the-whole-thing-in-immediate-mode","title":"Testing the whole thing in immediate mode","text":""},{"location":"m1_memory_hierarchies/s4_immediate_gpu_computation/#caching-shaders","title":"*Caching shaders","text":""},{"location":"m1_memory_hierarchies/s5_building_a_computational_graph/","title":"Building a Computational Graph","text":""},{"location":"m1_memory_hierarchies/s5_building_a_computational_graph/#seeing-the-cpu-gpu-memory-hierarchies","title":"Seeing the CPU-GPU memory hierarchies","text":""},{"location":"m1_memory_hierarchies/s5_building_a_computational_graph/#transfers","title":"Transfers","text":""},{"location":"m1_memory_hierarchies/s5_building_a_computational_graph/#building-a-computational-graph_1","title":"Building a computational graph","text":""},{"location":"m1_memory_hierarchies/s5_building_a_computational_graph/#testing-the-computational-graph","title":"Testing the computational graph","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/","title":"Building a Computational Graph Compiler","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#seeing-the-gpu-memory-hierarchy-caches-shared-memory-and-ram","title":"Seeing the GPU memory hierarchy - caches, shared memory and RAM","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#graph-compilers-and-op-codes","title":"Graph compilers and OP codes","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#swapping-operators-for-fused-versions","title":"Swapping operators for fused versions","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#building-a-graph-compiler","title":"Building a graph compiler","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#testing-the-graph-compiler","title":"Testing the graph compiler","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#metaprogramming","title":"*Metaprogramming","text":"<p>Programs are just strings!</p>"},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#decomposing-to-op-codes","title":"*Decomposing to OP codes","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#a-toy-example-with-op-codes","title":"*A toy example with OP codes","text":""},{"location":"m1_memory_hierarchies/s6_computational_graph_compilers/#additional-ideas-for-compiler-optimization-buffer-reusage-matrix-reusage","title":"*Additional ideas for compiler optimization, buffer reusage, matrix reusage","text":""},{"location":"m1_memory_hierarchies/s7_outro/","title":"Outro","text":""},{"location":"m1_memory_hierarchies/s7_outro/#comparing-the-work","title":"Comparing the work","text":"<p>CPU, immediate, immediate with shader caching, computational graph and compiled computational graph</p>"},{"location":"m1_memory_hierarchies/s7_outro/#how-does-this-relate-to-torchcompile","title":"How does this relate to torch.compile?","text":""},{"location":"m1_memory_hierarchies/s7_outro/#where-to-go-from-here","title":"Where to go from here?","text":""},{"location":"m1_memory_hierarchies/s8_exercises/","title":"*Exercises - do at least 1","text":"<ul> <li>Implement a version of the linear layer functions which uses shared memory and tiling</li> <li>Implement the tree reduction version of the sum function and add it to the softmax function. Also compare the single pass and the tree reduction performance graphs. Reference</li> <li>Implement a max pooling operator in all levels and implement tests</li> <li>Implement a convolution operator in all levels and implement tests</li> <li>Add reusable buffers to the computational graph system</li> <li>Extend the computational graph with inplace operation for the ReLU operator</li> </ul>"},{"location":"m2_concepts_in_parallelism/","title":"Concepts in Parallelism","text":"<ul> <li>Data parallelism, work stealing - rayon</li> <li>Data parallelism, non-work stealing - crossbeam</li> <li>Mutex</li> <li>Async</li> <li>Atomic</li> <li>Threads</li> <li>GPU</li> <li>Channels</li> <li>Events</li> <li>*Sparsity</li> <li>*Random Access and Monte Carlo (Gyro Dropout)</li> <li>*Branchless programming</li> <li>*SIMD</li> <li>*Sorting</li> <li>*Graph representations - pointers and indices</li> <li>*Trees using indices</li> <li>*Parallel work on graphs</li> </ul>"},{"location":"m2_concepts_in_parallelism/#exercise","title":"*Exercise","text":"<p>Describe the base architecture of the egui-winit-wgpu template. Expand on the template and program some things (needs suggestions) using some of the primitives introduced in the module</p>"},{"location":"m2_concepts_in_parallelism/#sexercise","title":"S*Exercise","text":"<p>Pick items worth a total of 3 points or more, write a 10+ lines interpretation of each item</p> <ul> <li>1 - Data-oriented design - Entity component systems</li> <li>1 - Array of Structs, Structs of Arrays, Auto-Vectorization</li> <li>1 - Linearized octrees</li> <li>2 - Sorting kernels in divergent workloads - Wavefront path tracing</li> <li>4 - ORB-SLAM - design and a warning about trying to code it</li> <li>4 - Nanite</li> <li>1 - PyTorch - Data-Distributed-Parallelism</li> <li>1 - PyTorch - Model-Distributed-Parallelism</li> <li>2 - Shadertoy</li> <li>1 - Gyro Dropout - MLSys 2022</li> <li>1 - Hierarchical Frustum Culling</li> <li>2 - Flash Attention</li> <li>2 - Custom memory allocators</li> <li>2 - JAX</li> </ul>"},{"location":"m3_types/","title":"Types","text":""},{"location":"m3_types/#types-energy-usage-and-inference-quantization-sparsity-and-pruning-of-neural-networks","title":"Types, energy usage and inference, quantization, sparsity and pruning of neural networks","text":"<ul> <li>Floats</li> <li>Float precision</li> <li>Integers</li> <li>Energy usage (Vivienne Sze and the controversial paper)</li> <li>Inference</li> <li>Quantization</li> <li>Sparsity</li> <li>Pruning</li> <li>*Fast inverse square root</li> <li>*Bit tricks</li> <li>*Basic compression</li> <li>*Batch based data processing</li> <li>*Tensor Cores</li> <li>*Using integers instead of strings in hash tables</li> </ul>"},{"location":"m3_types/#exercise","title":"*Exercise","text":"<p>Find a suitable model and inference library. Perform inference. Optimize the model and inference process. Can you do inferencing on one thread, training on another and swap in the new model? ADD SUGGESTED MODELS  </p>"},{"location":"m3_types/#sgroup-discussion-and-presentation","title":"S*Group discussion and presentation","text":"<p>Pick one of the following topics. Read and understand it, then present and discuss the topic with one or more other people. Preferably classmates.</p> <ul> <li>Packing bits for atomic operators</li> <li>Inverse depth buffers</li> <li>Bittricks, packing normals and colors</li> <li>Morton codes / Z-order curves, tiling and GPU textures</li> <li>Calculating compression precision in a lossy point cloud compression scheme</li> <li>DLSS</li> <li>Real-Time Texture Decompression and Upsampling</li> <li>2:4 sparsity with Tensor Cores</li> </ul>"},{"location":"m4_profilers/","title":"Profilers and Case Studies","text":""},{"location":"m4_profilers/#introduction-to-profiling-optimization-case-studies","title":"Introduction to profiling, Optimization case studies","text":"<ul> <li>Profilers (PyTorch, web, GPU, general)</li> </ul>"},{"location":"m4_profilers/#specializations","title":"Specializations","text":"<ul> <li>Training a neural network</li> <li>Optimizing a neural network for inference</li> <li>Running Yolo</li> <li>Optimizing a point cloud renderer</li> <li>Optimizing a path tracer</li> </ul>"},{"location":"m4_profilers/#exercise","title":"*Exercise","text":"<p>Try out the profilers relevant to your own system with some sample programs. Now try it with some of your own code from before you started on the guide!</p>"},{"location":"m5_projects/","title":"*Projects","text":"<p>This whole module is for levels 3 and 4. Specifically, all of the tips and tracks are for level 3, where the projects themselves are meant for level 4.</p>"},{"location":"m5_projects/#how-to-create-real-time-systems-good-frameworks-for-the-different-fields-and-project-proposals","title":"How to create real time systems, good frameworks for the different fields and project proposals","text":"<ul> <li>Starting with a simple prototype</li> <li>Identify your components</li> <li>Single threaded correct implementation -&gt; Testing to avoid regression</li> <li>Optimize</li> </ul>"},{"location":"m5_projects/#tips-and-tricks-in-real-time-systems","title":"Tips and tricks in real time systems","text":"<ul> <li>Events</li> <li>Key and Mouse events</li> <li>Event Loops</li> <li>GUIs &amp; egui</li> <li>memcpy</li> <li>Hot loops, event loops</li> <li>Allocations in a hot loop</li> <li>System calls - hoist out of the hot loop</li> <li>Logging and printing</li> <li>Bindings - PyO3 and cxx</li> <li>Walk, don't run, testing for correctness before optimization</li> <li>Don't use abbreviations</li> <li>Don't use postfix incrementation++</li> <li>When to care about software engineering and when to care about performance</li> <li>Don't use a string key/identifier or integer, when a type safe enum will do the job</li> <li>Hard coding types</li> <li>Cognitive load, and delaying errors to after the first draft - deliberate development vs. debugging</li> <li>Prefer stateless programming, minimize stateful programming (functional inspiration)</li> <li>Implicit casting</li> <li>Compression</li> <li>Know your system - mobile, laptop, desktop, integrated memory, which GPU</li> <li>Use version control even for solo development</li> <li>Am I copying/cloning things that don't need to be copied?</li> <li>Check/validate everything before the hot loop</li> <li>Anything that can be immutable, should be immutable - aliasing!</li> <li>Testing and Seeding RNG's</li> <li>Faster RNG</li> <li>Timing real-time systems and how to escape or offload compute</li> <li>Multi-resolution computing for making your real-time target</li> <li>Pressure testing and worst cases</li> </ul>"},{"location":"m5_projects/#components-librariesframeworks","title":"Components - libraries/frameworks","text":"<p>blessed rayon egui wonnx tch winit cv ultraviolet arewelearningyet burn </p>"},{"location":"m5_projects/#s-project-proposals","title":"S Project proposals","text":"<ul> <li>Virtual 3D scanner for a point cloud dataset</li> <li>EEG system</li> <li>Change the latent variables in a network using GUI, optimize the network</li> <li>Point cloud renderer</li> <li>Real-time style transfer on a web cam feed</li> <li>Rendering fractals influenced by a web cam feed</li> <li>Eye tracking -&gt; Present to screen and read from web cam -&gt; feature extraction -&gt; classifier -&gt; intervention signal -&gt; reading app (Wolfgang Fuhl, PISTOL, fixation detection)</li> <li>Bird classification from sound / Real-time classification of sound (Xeno-canto database)</li> <li>Who is talking? Real-time classification of sound</li> <li>Are you dyslexic? Eye tracking classifier</li> <li>Cognitive load tracker - Eyes &amp; pupil dilation and online estimation of signal strength (pupils vs. sound for the hearing impaired)</li> </ul>"},{"location":"m5_projects/#what-makes-for-a-good-project","title":"What makes for a good project?","text":"<ul> <li>What is your concept/project?</li> <li>Which concepts from the previous material do you think are relevant to your project and why?</li> <li>Preprocessing your data?</li> <li>How do you adapt to your chosen/available platform?</li> <li>Which libraries did you choose for this problem?</li> <li>How fast did you get to your minimum viable product?</li> <li>Which steps did you take from there and why?</li> <li>How did you determine which parts of your system to optimize?</li> <li>What else would you like to do with your system?</li> </ul>"}]}