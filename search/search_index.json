{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Don't Panic","text":"<p>Hello There!</p> <p>You've found a guide for learning about all the stuff needed to either program or reason about data oriented and real time systems. It will help you with things like what memory allocations are, why computational graphs used to program neural networks are a good idea, different concepts in parallelization of code, what types are, how to profile and optimize code and how to create real-time systems. All running on your laptop!</p> <p>The guide is meant to be followed several times, with every increase in level the curriculum expands and it is assumed that you have learned and understood the previously introduced concepts. It's done this way, because computer science is a bit hard. To learn more stuff, you have to know some stuff, about some stuff, but to learn more, you have to know a lot of stuff about a lot of stuff. Which section is part of which level is indicated by an emoji, like so 2\ufe0f\u20e3.</p> <p>To make things more complicated, because everyone loves complicated, there are some sections which are meant to tailor to you, the reader! These specialization sections could for example have one set of tasks for people interested in compute graphics and a different set of tasks for people interested in deep learning. These sections are indicated by this DNA emoji - \ud83e\uddec.</p> <p>Course material for a course based on this material can be found here.</p> <p>The material comprising the guide is divided into 6 modules.</p> <ul> <li>Intro to the course, the different ways to use the material, intro to Rust and wgpu.</li> <li>Memory hierarchies and computational graphs</li> <li>Concurrency</li> <li>Types, energy usage and inference, quantization, sparsity and pruning of neural networks</li> <li>Introduction to profiling, optimization use cases on topics such as model training and quantization, graphics, computer vision</li> <li>How to create real time systems, a random grab bag of tips and tricks, frameworks for the different fields and project proposals</li> </ul> <p>So let's get started!</p> <p>\ud83c\udf0c Queue Eric Idle singing while wearing a white wig \ud83c\udf0c</p>"},{"location":"TODO/","title":"TODO","text":"<ul> <li>Try rust-nexttest to solve the testing issue</li> <li>Find the right benchmarking and performance tools (blessed.rs)</li> <li>Look into friendlier error handling? Perhaps logging instead of panicing to get students used to logging. Introduce anyhow for better error handling?</li> <li>Loom</li> <li>Come up with a different name for levels 1/2/3/4, also should the levels be described in a matrix?</li> <li>Should there be an introduction to basic computer architecture somewhere?</li> </ul>"},{"location":"TODO/#emojis-for-later","title":"Emojis for later","text":"<p>\ud83c\udf0c \ud83d\udd1c 1\ufe0f\u20e3 2\ufe0f\u20e3 3\ufe0f\u20e3 4\ufe0f\u20e3 5\ufe0f\u20e3 \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb \ud83e\uddec \ud83d\udc7d \ud83e\ude90 \ud83d\ude80 \ud83d\udef0\ufe0f \ud83e\udd80 \ud83d\udd25  </p>"},{"location":"TODO/#references-and-additional-reading","title":"References and additional reading","text":"<p>High Performance Machine Learning High Performance Machine Learning Flash Attention Branchless Programming The Rust Programming Language Learn wgpu Install Rust wgpu ShaderToy Inigo Quilez ORB-SLAM ORB-SLAM2 Z-order curves Linearised Trees on the GPU Vivienne Sze - Energy Efficient AI Rust Profiling RenderDoc Book of Shaders Scratchapixel Ray Tracing in One Weekend Physically Based Rendering Crafting Interpreters Programming Rust Godbolt Advent of Code </p>"},{"location":"TODO/#im-done-but-i-want-more","title":"I'm done, but I want more!","text":"<p>Visual Computing Systems - Stanford Parallel Computing - Stanford</p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<ul> <li>Nicki Skafte Detlefsen</li> <li>Lars Kai Hansen</li> <li>Pioneer Centre for AI</li> <li>Mark Bo Jensen</li> <li>Mathias Gammelmark</li> <li>Anders Jess Pedersen</li> <li>Jens Egholm Pedersen</li> <li>Nebula icons created by Eucalyp - Flaticon</li> </ul>"},{"location":"m0_introduction/","title":"1\ufe0f\u20e3 Introduction","text":"<p>Hello there! If you are reading this you might have been enticed by promises of performance and other some such advanced black magic, but first, a digression...</p> <p>There are so many things to keep track of as a modern day programmer and most systems hide these things from the user. You call something called... <code>?numba?</code> and annotate a few functions and it magically makes your code faster. You use something called <code>?Py?...?Torch?...</code> and it seems really slow for some reason. You're sure you have a GPU in your machine, but it's still slow. <code>PyTorch</code> has something called a profiler, but you don't know how it works and you don't know what the hell <code>DtoHMemCpy</code> even is. It can be hard to reason about what is going on inside these black boxes. On top of that you might not be able to find anything to walk you through all of the stuff you don't know that you don't know. As scary as it can sometimes seem to get your hands dirty and take on what might seem an insurmountable obstacle, not doing so can have consequences.</p> <p>The speed of execution of a program is approximately correlated to the energy consumption of that program. Until we use 100% green, renewable, energy in all of computing we have a shared responsibility to at the very least practice some modicum of restraint and sensibility in our resource consumption. Taken to the limit by putting large scale machine learning, with its massive energy consumption for both training and inference, in everything without necessarily generating value comensurate to the expended resources is an irresponsible use of resources.</p> <p></p>  Image credit  <p>If someone trains a deep learning model for two weeks on eight huge data center GPUs in a cluster, it is their responsibility that that training process is fairly well optimized, and that all data is responsibly retrieved, such that that training does not have to run again because of sloppiness.</p> <p>And thus stops the finger pointing!</p> <p>Optimizing code, especially on systems you might share with others both means that you can get your results faster, but that others can have use of the system in a reasonable time as well. If you are creating large models, optimizing them to be smaller or more efficient also results in entities with profits less than the GDP of a small nation actually being able to train and run inference with your model, increasing the democratization of your work and its reach. If its able to run on a consumer grade desktop - even better!</p> <p>This guide was made to be an iterative process, taking you by the hand, speaking to you at the level at which you are following it, trying not to overwhelm you. Reading that back, it could sound a bit condescending, but it basically means that the types of concepts you are assumed to know about will gradually increase with each level. Due to the guide gradually introducing certain concepts, jumping around the material is not recommended. I also acknowledge that some people have different interests. As such, some portions of the guide will be tailored to people who are into deep learning, people who like computer vision, or computer graphics or other some such. You are more than welcome to read and do all of it, but no one says you have to do anything. If you just follow along the path that is most relevant to you, you will be just fine. The guide does contain code, sometimes just small snippets, but also frameworks in which most of a module will take place.</p> <p>Most importantly - Don't Panic! The guide is here for you! And now for something completely different... practicalities!</p>"},{"location":"m0_introduction/#specializations","title":"\ud83e\uddec Specializations","text":"<p>Throughout the guide there are sections and exercises prefixed by \ud83e\uddec. These exercises and topics are meant to be up to you to follow. If you are mostly interested in deep learning, by all means only read and do the sections and exercises which are relevant to deep learning. Which section and exercise is relevant to which specialization will be explained in each section. To begin with deep learning will be supported, but once a full first draft is ready, I will begin adding things relevant to computer graphics, computer vision and cognitive signal processing.</p>"},{"location":"m0_introduction/#12345-levels","title":"1\ufe0f\u20e32\ufe0f\u20e33\ufe0f\u20e34\ufe0f\u20e35\ufe0f\u20e3 Levels","text":"<p>The guide's way of doing things is to iteratively expand the curriculum and the depth at which concepts are described. You can follow the guide iteratively by doing it multiple times, each time advancing in level or you can jump right in to the relevant level.</p>"},{"location":"m0_introduction/#1","title":"1\ufe0f\u20e3","text":"<p>This level is for people unsure about investing the time and effort to do level 2. People are busy, and inherently looking to maximize the value gained given the time invested. Just for those people, each module has been boiled down to approximately 2 pages of reading. Reading all of the material should take at most an afternoon and is comprised of main page of each module. Basically, you could stop reading once you are done with the 1\ufe0f\u20e3 header and just click each \"MX - XXXX\" on the left, read that page until the end, then click on the next \"MX - XXXX\" title and read that until the end and you would be done. That does not include \"M5 - Projects\", that one is only relevant for 3\ufe0f\u20e3 and 4\ufe0f\u20e3. Happy reading!</p>"},{"location":"m0_introduction/#2","title":"2\ufe0f\u20e3","text":"<p>At this level you might be a comfortable programmer in Python, you might be a researcher, or you might just be warming up for level 3. In most cases you might not be all that comfortable with lower level languages, such as C, C++ or Rust. It is expected that you checkout the repository and try out the code on your own laptop. It is expected that you might change a few variables here and there to explore the performance benchmarks generated, but not much more than that. Don't worry, it does not require an Nvidia GPU to run on your laptop. There will be Rust code, \ud83e\udd80, but it will be as simplified as possible to just focus on making your learning experience as smooth as possible. If you have a systems programming background or are experienced in C/C++/Rust, you should be able to move through this level easily. This level does not take into account any module, section, page or info box with a 3\ufe0f\u20e3 or above in front of the name. You are still welcome to read them of course, but the language and code are a bit more advanced and it might be a bit too much if you are still working on the basics.</p>"},{"location":"m0_introduction/#3","title":"3\ufe0f\u20e3","text":"<p>This level is made up of all the material from 1\ufe0f\u20e3 and 2\ufe0f\u20e3. At this level it is expected that you have experience with C/C++/Rust and that you have tried programming a GPU or that you have previously done level 2 and are up for a challenge. If you haven't done any of these things, you'll be ok, but it might take significant effort on your part.</p>"},{"location":"m0_introduction/#4","title":"4\ufe0f\u20e3","text":"<p>At this level everything at 3\ufe0f\u20e3 is expected, as well as you doing a handful of the exercises. This is for doing a course version or if you REALLY want to learn all the guide has to offer.</p>"},{"location":"m0_introduction/#5","title":"5\ufe0f\u20e3","text":"<p>The secret bonus level! It is the one where you do levels 1-4 and read all the further reading sections (which are more academically rigorous). Typically these references will be to book chapters, university courses, scientific papers and in-depth blog posts.</p>"},{"location":"m0_introduction/#_1","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb","text":"<p>If you are a teacher who wants to make use of this material, feel free to use the course site. The course focuses on teaching real-time systems for deep learners and visual systems programmers. It allocates half of 15 days to go through the material in the guide and the other half to making a project relevant to the students' specialization, which is the contents of m5. It is designed to give shallow introductions to a wide range of concepts with a handful of varying exercises. The student is to then reach back and explore the topics relevant to their specialized project in greater detail. The breadth of topics is quite wide, and each student shouldn't be expected to pass an exam in every topic. In most cases they might remember that a thing exists and that they can search for it. The thing they hopefully all learn is how to reason about performance, systems design and that getting your hands dirty can be both fun and invigorating.</p>"},{"location":"m0_introduction/s0_intro_to_computing/","title":"2\ufe0f\u20e3 Introduction to the Computing Landscape","text":"<p>If you are new to programming, or perhaps have been able to get by using scripting languages only, you might not have been introduced to the other options. Some of the concepts presented here lay the foundations for the choices dictating the rest of the guide. Though I have made some clearly defined choices about which tools to use, you should at all times use the right tool for the job. Not only in which language or framework you might choose, but in how you put together and design your systems using those tools. Part of the guide's strategy is to introduce you to quite a lot of tools and concepts, also known as the \"learn what to DuckDuckGo\"-strategy, and then going into greater detail about core concepts and concepts relevant to your specialization. The guide will introduce concepts that aid some programs in producing faster results than others. An important factor is limitations. Usually, the word limitations carries a negative connotation, very few people think less freedom sounds enticing, but in computing, limitations can be a wonderful thing to have and set. Especially, once you are past the first prototype. In some cases, even when prototyping.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#scripting-languages","title":"Scripting Languages","text":"<p>Chill out, take things one line of code at a time. Scripting languages aren't compiled, but run one line at a time. This leaves the system unable to look beyond the current line of code, unless you add a compiler to the mix, whic usually takes a look at all of your code.</p> <p>Python is likely the scripting language you are most familiar with. Python is very popular due to its apparent ease-of-use, but it is quite slow in its raw form. The main advantage of vanilla Python is the ability to glue together a number of libraries written in other languages. In time, improvements have been made, such as putting type hints into your code which helps catch errors and gives more informative function definitions.</p> <p>In general, if other people reading your code must read the high-quality comments, that you definitely remembered to write... right?, then you are laying the foundation of a codebase that will frustrate people, probably including yourself. Python is easy to write at first, but the lack of a compiler can leave your long running code to fail just at the moment it is about to save the results because your forgot you were trying to save the wrong type.</p> <p>Python does have additional tools you can use to compile it. This allows for additional verification and performance improvements, but without additional limitations and indications of your intention, it might not be possible to optimize your code as much as a language which leaves things less open to interpretation.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compilers","title":"Compilers","text":"<p>A compiler processes the given code in one or more steps. In some steps it might verify that all of your code is correct, it might transform and optimize your code, it might change it into different representations like byte code or machine code. Some compilers strictly function before running it in an operation like <code>some_compiler -compile my_file.code</code> and outputs a runnable executable, specifically for your type of machine. This is usually done once before running your code and then only when changes are made. This is called ahead-of-time (AOT) compilation. Most compilers require additional constraints to transform and improve your code. Usually, you can also give your compiler additional commands to tell it how to compile. It could be things like \"please optimize my code to have a smaller executable size\" or \"please show me all warnings as errors\".</p> <p>Imagine you ask someone to go get you some milk every Thursday at 12. An unreasonably pedantic person (engineer) might be ready at 12 every Thursday and ask you what type of milk you would like today. It seems annoying and strange. You know what type of milk you like, the pedantic person should know what type of milk you like. That bastard! If you instead asked for X brand skim milk delivered at 12 every Thursday, the pedantic person might even try to optimize the process before the delivery day. If it was milk with a long expiration day, they could buy it in bulk and just have it ready for you. That unreasonably pedantic person is the compiler of whatever programming language you are using. It will go far to help you, it just doesn't perform well in ambivalent circumstances. Compilers are genereally not allowed to guess in a way that might functionally alter your code, such as reducing the level of precision.</p> <p>Most compiled languages are all designed with at least one compiler, usually compiling to byte code or machine code. However, it is possible to write a compiler after the fact. Cython is one such compiler made for compiling AOT compiling Python. It benefits quite a bit from having the user perform additional annotations of their Python code, allowing for a decent speedup.</p> <p>Other compilers act just-in-time (JIT). Just as you want to run your code it will compile it. While this seems a bit weird, why not just compile it once and for all, this can allow the compiler to optimize the program specifically for your machine. Afterall, while two machines might have similar setups, one might have a GPU and other one not. The Java HotSpot VM even tries to optimize your code as it runs. If allowed to become a long-running process it can swap byte code for compiled machine code. In general, JIT compilers increase the startup time of your code, afterall, it has to compile it, just like the AOT compiler. Some JIT compilers save the compilation artifacts (the outputs of the compilation process) for later to merely reload it, but that won't help you much while you are developing your code. Some libraries and frameworks such as numba perform JIT compilation of your annotated code to optimize the performance.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compiled-languages","title":"Compiled Languages","text":"<p>In some languages like C, C++ and Rust, machine code is the outcome. That machine code can be quite platform specific, both because of the operating system and the hardware, and is in binary. 1's and 0's! These three languages are not garbage collected (more on that later).</p> <p>Another quite popular language is Go, which also compiles to machine code, but is garbage collected. Julia has more of a scientific/numerical focus, but features garbage collection, JIT compilation and can use either a runtime or compile to a standalone binary.</p> <p>Other languages like Java and C# compile to something called bytecode, which can then be interpreted by a process virtual machine. Thus all Java programs compile to the same bytecode, regardless of whether it's supposed to run on a Mac, Linux or Windows platform. The bytecode is then interpreted, sometimes optimized as well, at runtime by a virtual machine written for that specific platform.</p> <p>Javascript is a just-in-time compiled language running on most web pages. It can occassionally have a reasonable speed due to the optimizing runtime. Heavy development has tuned the widely used V8 runtime to improve Javascripts performance. Writing Javascript can seem easy, but the amount of things you are allowed to do, but shouldn't, can make it an arduous experience once the errors start piling up. The advantage is highly portable code, because everything is essentially just a string... including numbers, which is a traumatic experience, I would prefer not to elaborate on.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#the-guide-and-languages","title":"The Guide and Languages","text":"<p>As you can probably see in the column on the left... the guide will be using Rust from here on out, with a few exceptions. C will occasionally be used for reasoning about low level stuff like pointers and memory allocations, while C++ will be used as a comparison to Rust and Python will be used for a bit of perspective. In any case it will be assumed you don't really know any of the languages except Python and that you have read the introductions to Rust in this module. If you read the section on GPU programming, you will see there are no easy, one-size-fits-all, solutions. Thankfully, the guide has clear goals and limitations. To help you get familiar with new topics, we only need reasonable performance and for all the code to be runnable on most laptops. Most importantly, the setup process should be easy and not make you want to stress-eat the contents of your entire fridge when going through the installation process. As such the guide will mainly use Rust and the GPU API wgpu. The guide will in all cases that do not require graphics output only concern itself with pure computation through wgpu, which makes setup quite a bit simpler. wgpu is an abstraction layer that runs whatever GPU API it finds best suitable on your system. Having exact control and the absolute best performance isn't as important as allowing as many people to participate and learn as possible. After all, if it doesn't work on your laptop/desktop, you can't really play around and have fun with it!</p>"},{"location":"m0_introduction/s0_intro_to_computing/#3-gpu-apis-and-languages","title":"3\ufe0f\u20e3 GPU APIs and Languages","text":"<p>GPUs are some of the most readily available accelerators. Originally made for graphics, since around 2008 using them for general computation has been fairly wide spread. All graphics API's now also support general computation. Usually, it will be called a compute shader. Shader is the common name for a GPU program. If running CUDA or OpenCL, it is called a kernel. The guide will mostly focus on the pure compute parts of GPU APIs, except for the graphics specialization. Thus it will be assumed that if you are interested in the graphics specialization you might already have done a graphics course or a tutorial such as LearnOpenGL or Learn Wgpu. It is worth noting that a compute shader using a graphics-based API, such as Vulkan, can perform just as well as an implementation in a compute-only API, such as CUDA. One example of this is VkFFT. A GPU API is all the stuff that you have to write in your code that is not the function (shaders) that you want to run. It could be calls like creating a connection to the GPU, allocating memory on the GPU, transferring the contents of a buffer to the memory you just allocated on the GPU or launching your shader/kernel and transferring the results back to the CPU. The GPU languages themselves vary with the APIs. Some APIs, such as Vulkan, can take an intermediate representation called SPIR-V, this allows the user to write in any shading language, or even Rust in one case, as long as it is compiled to SPIR-V. Usually a shading language will look a lot like C/C++, but have its own distinct rules. You can't always make the same assumptions.</p> <p>The rest of this section is an overview of the various available GPU APIs.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#web-apis","title":"Web APIs","text":"<p>An often used strategy for making your programs as widely available as possible, is to use web-based techonology. Whatever browser you, or the end user is using supports some GPU APIs. For a long time it has been WebGL, which is a subset of OpenGL. WebGL has a version 2.0, which was finally supported by all major browsers not too long ago. The 2.0 version brought support for compute shaders with it. The modern newcomer is WebGPU which has a way of doing things that more closely resembles modern APIs such as Vulkan, DirectX 12 and Metal. It is not widely supported in browsers, outside of developer modes. Until then, the wgpu abstraction can be used. It has an API which follows the WebGPU specification, with some optional extensions for more features, but under the hood it uses whatever API it deems best for the current platform. Once the support for WebGPU becomes widespread, it can merely choose to run using WebGPU instead. In general, you will find that most frameworks or APIs which have to support a lot of things will be centered around the lowest common denominator. However, tools such as Vulkan and wgpu do allow you to query the system you are on for support of an extension, which does allow access to specialized features. You may however, end up with several versions of some elements of your code, based on whether some feature is there or not.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#platform-specific-apis","title":"Platform-Specific APIs","text":"<p>Some GPU APIs are specific to specific operating systems. DirectX11 and DirectX12 targets Windows and XBox platforms, while Metal targets Apple devices. The guide won't concern itself too much with these. DirectX11 is somewhat similar to OpenGL, while DirectX12 and Metal are from the same, more low-level, generation as Vulkan. Metal however, seems to be a bit less low-level compared to DirectX12 and Vulkan.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#cross-platform-apis","title":"Cross-Platform APIs","text":"<p>OpenGL and Vulkan are cross platform. OpenGL hasn't seen any updates for a while. Vulkan on the other hand is a low level, but generally popular API. It puts a lot of responsibility on to the programmer, but works on Windows and Linux, as well as Intel, Nvidia and AMD GPUs. It even works fairly decently on Apple devices thanks to MoltenVK Another cross-platform tool is wgpu, mentioned earlier. It is also the one that will be used in the guide for GPU code.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compute-apis","title":"Compute APIs","text":"<p>Some GPU APIs are strictly not for graphics, such as CUDA and OpenCL. OpenCL is cross-platform (works on all GPUs), as well as compiling to FPGAs, DSPs and parallelized CPU code. On the other hand CUDA is just for Nvidia GPUs. CUDA is widely used in scientific computing and mostly dominates academia. Both CUDA and OpenCL have their kernels written in a specialized version of C++.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#shader-languages","title":"Shader Languages","text":"<p>Shader languages are languages specifically tailored for the combined graphics/compute APIs. Graphics APIs have some specific functionality which the language has to support. Usually you will find support for small vectors and matrices (up to 4x4) and various types you might not find on the CPU such as fp16. They will also usually have something called textures, bindings, samplers and built-in variables. You don't need to worry about that very much in the guide. GLSL, HLSL, WGSL and MSL are all shading languages developed for graphics APIs. OpenGL, DirectX, WebGPU and Metal, respectively. GLSL is the main shader language of Vulkan, but HLSL is also seeing rising popularity in that community. Lately, the tooling for cross compiling and running the same shaders on different graphics APIs has become a lot better. Shaders can be compiled to SPIR-V, an intermediate representation, sort of like the byte code we discussed earlier. This allows the platform independent SPIR-V to be translated to the specific instructions the GPU the code is actually run on. One tool for compiling shaders is naga.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#3-domain-specific-languages-and-frameworks","title":"3\ufe0f\u20e3 Domain Specific Languages and Frameworks","text":"<p>Shading languages all benefit from limitations and specializations from being specifically for graphics on a GPU. Another form of limitation is domain specific languages and frameworks. One such framework you might know of is PyTorch. You are generally supposed to formulate your neural network as a graph and not just a sequence of operations. This allows PyTorch to have a clearer picture of what it is you want to do. It can check that all dimensions fit before running the training loop and it can optimize the process. Taking things even further PyTorch even has its own compiler from version 2.0.  </p> <p>Another way of achieving speedy results in a flexible format is retrofitting an existing language, in this case Python, with a slightly different language. Taichi combines a domain specific language to JIT compile highly performant code, which can also run graphics, to whatever platform you are running on. It can do this because of increased requirements of the user. Namely, annotating their code and setting limitations. Halide on the other hand restricts itself to be a AOT- or JIT-compiled language embedded in C++ made specifically for working with images and tensors.</p> <p>Futhark is a language made specifically for replacing the parts of your code which need to be fast. As such it is not a general language and can make opinionated choices which allows it to generate more performant code.</p>"},{"location":"m0_introduction/s1_intro_to_rust/","title":"2\ufe0f\u20e3 Introducing Rust","text":"<p>Why use Rust for the guide? Say you've decided you want to eat 35 burgers. Python is the friend that helps you order them to be delivered at your door. C++ is the friend who says 'do whatever you want'. Rust on the other hand, is the friend who stops you and then recommends you a salad. That may be annoying at times, especially if you were just craving 35 entire burgers, but it is what is best for you. Since the guide is not a beginners introduction to programming, and I will be introducing at times fairly advanced concepts, having a language that keeps you on the straight and narrow, even if it seems pedantic and like it is getting in your way, is a genuine advantage. If the Rust compiler accepts your code without any unsafe sections, it is probably going to work. Another point in Rust's favor was the easy setup and use on Windows, Linux and macOS. The setup time needed to be less than 10 minutes and the chosen language needed an easy-to-use, preferably integrated, package manager which didn't cause too many versioning issues. The options considered were C, C++ and Rust. C and C++ contained too many footguns and required the use of an external package manager and the use of header files and build systems. Rust takes care of all that with cargo. cargo can help you run and test your code, as well as helping downloading and building all of the dependencies in the <code>Cargo.toml</code> file, which you will find in the root of each code project. This guide was not supposed to be a guide for learning either any these languages. Rust's very helpful compiler is likely to be a boon for guiding users towards sensible code. The process of having to program in such a memory focused, modern, compiled language will turn what is otherwise an implicit, unspoken process inside out, forcing you to think about what good code is, where is the data located in memory, which thread has access to which data, and so on.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#setup","title":"Setup","text":"<p>To use the code in the course, as well as doing the exercises, first of all, you should have git and Rust installed.</p> <ul> <li>Install git</li> <li>Install Rust</li> </ul> <p>Once you have installed both, ensure they are properly installed by calling <code>git --version</code> and <code>cargo --version</code> in your terminal of choice. Your Rust version should be at least 1.68. At the least, the code in the guide won't function without support for the 2021 or newer version of Rust.  </p> <ul> <li>git clone the guides repository <code>git clone https://github.com/absorensen/the-guide.git</code></li> <li>In your terminal navigate into the folder that just appeared</li> <li>Navigate to <code>m0_introduction/code/is_my_system_ready/</code></li> <li>In the command line write <code>cargo run</code>. This might take a while as cargo will now download and build some dependencies, and then compile and run the code in debug mode.</li> <li>For IDE, the guide has been developed with VS Code with the extensions rust-analyzer, CodeLLDB, Rust Syntax, WGSL, wgsl-analyzer and optionally Dracula For Rust Theme.</li> </ul>"},{"location":"m0_introduction/s1_intro_to_rust/#projects","title":"Projects","text":"<p>Projects are how a Rust codebase is organized. A project can contain subprojects, but the guide won't use this. Navigating to your commandline and writing <code>cargo new my_project</code> will create a project with a <code>Cargo.toml</code> file and directory named <code>src</code> in the root. Inside the <code>src</code> directory there will be a file named <code>main.rs</code>. The <code>Cargo.toml</code> is a file describing the name of your project and its dependencies, which will be empty to begin with. But if you write some dependencies, next time you build the code with <code>cargo build</code> or <code>cargo run</code>, with or without the <code>--release</code> flag, cargo will download and build all of the dependencies. By default cargo will look in the <code>src</code> directory. All of the files with the <code>.rs</code> suffix are Rust source files.</p> <p>Entering <code>cargo run</code> will compile and run your code in debug mode, which means it will be easier to step through the code and getting better error messages. It will also result in significantly less compilation time, but slower run time. If you add <code>cargo run --release</code> it will compile in release mode. Compilation will take longer, the code will run faster, but debugging will be harder.</p> <p>To save on space, especially for some of the smaller projects where you just need to run a command or two, write <code>cargo clean</code> once you are done to remove all of the relevant dependencies.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#testing","title":"Testing","text":"<p>Cargo can also handle unit testing of code. It requires that the project is split into an application and a library part, allowing the test to just test the library. In practice what is usually recommended is to just have a very small function in your application which tells your library to start running your code.</p> <p>Navigate to the project at <code>m0_introduction/code/how_to_test/</code> to see how a typical project is setup. Try running the commands <code>cargo run</code>, followed by <code>cargo test</code>. See how the code ran with <code>cargo run</code>, but <code>cargo test</code> actually told you which functions weren't living up to expectations? Setting up these unit tests is also how the guide tells you that the code you have been asked to write is correct. When doing exercises, keep going until all of the tests pass!</p> <p></p>  Fix the values in test_function_c() and run cargo test again!  <p>Now be sure to take a minute to look at the files involved in a project and read all of the comments! Sometimes you need more info as to what went wrong with the test. There's a fix for that on Windows.</p> <p>On some computers the GPU tests will currently fail unless being run with <code>cargo test -- --test-threads=1</code> as the tests are running concurrently. All of the tests requiring GPU access will try to grab the GPU without sharing resources. Even then it might fail. You can just try a few more times or try to run tests individually.  </p>"},{"location":"m0_introduction/s1_intro_to_rust/#3-clippy","title":"3\ufe0f\u20e3 Clippy","text":"<p>Clippy is cargo's tool for giving suggestions for improving your code and making it more akin to idiomatic Rust. The guide has most code conformant to Clippy's suggestions, however I choose to diverge where making the code simpler and easier to understand for people who have never programmed Rust before is a priority. Clippy's messages are very informative and a good learning experience. It is recommended that you use Clippy in your own code. It is as simple as calling <code>cargo clippy</code>.</p> <p>Running it on the <code>how_to_test</code> project, Clippy returns the following message -</p> <p></p>  The guide elects not to fix this, because the return statement was put there to make a point."},{"location":"m0_introduction/s1_intro_to_rust/#3-rustfmt","title":"3\ufe0f\u20e3 rustfmt","text":"<p>rustfmt is a formatter for Rust. Surprise! You can install it by running <code>rustup component add rustfmt</code> in a terminal. From then on you can run commands like <code>cargo fmt</code>, which automatically changes the code in your current crate (subproject, or the entire project if you are standing in the root).</p>"},{"location":"m0_introduction/s1_intro_to_rust/#3-fix","title":"3\ufe0f\u20e3 fix","text":"<p>fix is a tool for taking as many of those pesky compiler warnings as possible, and fixing your code for you. You just enter <code>cargo fix</code>.</p>"},{"location":"m0_introduction/s2_basic_concepts_in_rust/","title":"2\ufe0f\u20e3 Basic Concepts in Rust","text":"<p>The following chapter is an absolutely barebones introduction to concepts in Rust which you will need to understand to read the guide's code. If you would like a more thorough introduction to Rust, there is a number of nice tutorials available.</p> <p>The real contents of this section is the project in <code>m0_introduction/code/basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/","title":"3\ufe0f\u20e3 Less Basic Concepts in Rust","text":"<p>The real contents of this section is the project in <code>m0_introduction/code/less_basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#supplementary-comments","title":"Supplementary Comments","text":"<p>In this section, I'll take you through a few addendums, which aren't as much about a specific language construct, but some concepts it might help to know.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#aliasing","title":"Aliasing","text":"<p>Aliasing is a term for when two pointers or references refer to the same place in memory. That might not sound like much of a problem at first, but it allows the compiler to make optimizations. Take a look at this code -</p> <pre><code>fn compute(input: &amp;u32, output: &amp;mut u32) {\nif 10 &lt; *input {\n*output = 1;\n}\nif 5 &lt; *input {\n*output *= 2;\n}\n// remember that `output` will be `2` if `input &gt; 10`\n}\nfn compute(input: &amp;u32, output: &amp;mut u32) {\nlet cached_input: u32 = *input; // keep `*input` in a register\nif 10 &lt; cached_input {\n// If the input is greater than 10, the previous code would set the output to 1 and then double it,\n// resulting in an output of 2 (because `10&lt;&gt;` implies `5&lt;&gt;`).\n// Here, we avoid the double assignment and just set it directly to 2.\n*output = 2;\n} else if 5 &lt; cached_input {\n*output *= 2;\n}\n}\n</code></pre> <p>You can also check the Rustonomicon for a better explanation of aliasing. It is where the code snippets above are from. The code has been reformatted to preference. It may be on the more advanced side however.</p> <p>Basically, whenever you write to a value and there are multiple references to that value hidden away in different places of the memory hieararchy, such as some threads registers, or even within the same function, everything becomes invalidated. This is one of the reasons for the borrow checker adamantly enforcing that there can be multiple shared (read-only) references to a value, but only one mutable reference (read/write), and if there is a mutable reference, there cannot be any shared references to that value. If there were multiple shared references and a mutable reference it would be impossible to guarantee correctness as just when a value is retrieved from RAM by a shared reference a write to that value may have ocurred, which in order to make the sequence of operations correct might necessitate another read, but what if it happens again? Another read! This is not what happens, you just get a program behaving \"weird\". In another case, it would also mean you could not read from a value and save that value in a local variable to do a bunch of operations before writing it somewhere. It already sounds very headscratching and like you should only ever do single threaded programs. But thankfully, the borrow checker is there to keep things in check for you. One recommendation, you should try to minimize the time that a mutable reference to a value will exist.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#multiple-function-definitions-not-allowed","title":"Multiple Function Definitions Not Allowed","text":"<p>As opposed to languages like C++, you cannot have multiple functions with the same name in Rust. In C++ this is perfectly legal, and the compiler will attempt to deduce which one you mean based on the way you are calling function().</p> <pre><code>void function(int argument_a, int argument_b) {}\nvoid function(int argument_a, int argument_b, int argument_c) {}\nvoid function(int argument_a, int argument_b, float argument_c) {}\n</code></pre> <p>Rust seems to be designed in a way as to minimize the amount of ambiguity faced by the compiler (and you too). Sometimes in Rust code you will see several different constructor functions, such as <code>build</code>, <code>build_from_ints</code>, <code>new</code> and <code>default</code>. In one way, that is a pain in the ass. In another way, it's quite nice. It forces the programmer to be explicit about how the functions behaviours are different, instead of being unwritten, implicit, or 'well, you can just read the code, it's not that complicated'. If you ever think or say that. Remember this... ahem RED FLAG! Fix your stuff so people don't have to guess, it will probably make the next person to read your code hate you slightly less. Which is a good thing!</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#index-checking","title":"Index Checking","text":"<p>Whenever you access an element in an indexed collection such as a Vec:</p> <pre><code>for index in 0..data.len() {\ndo_something(data[index]);\n}\n</code></pre> <p>Whenever you do this in Rust, there is a runtime check to make sure this index is not outside of the memory of the vector. This does have some performance cost, but unless you are absolutely sure this processing is happening in a hot region (a region of your code where a lot of time is spent), it is not recommended to try and circumvent this.</p> <pre><code>for index in 0..data.len() {\nunsafe {\ndo_something(data.get_unchecked(index));\n}\n}\n</code></pre> <p>It requires an unsafe region, which is a region in your code where you tell the compiler to allow you to do some things it would otherwise not allow you to, and call the function <code>.get_unchecked(index)</code>. An unsafe region does not turn off all checking, but in general, if you are at the level of reading the guide, you don't need it and we won't be talking about it more. If you really want to read more about unsafe, the Rustonomicon is the defacto standard introduction to unsafe in Rust.</p> <p>The two above functions are equivalent to</p> <pre><code>for(int index{0}; index &lt; data.size(); ++index) {\ndo_something(data.at(index));\n}\n</code></pre> <pre><code>for(int index{0}; index &lt; data.size(); ++index)  {\ndo_something(data[index]);\n}\n</code></pre> <p>Note however, that square bracket indexing is the defacto standard way of accessing an array element in both languages. This showcases a core difference between the two languages. One being safety opt-out, and another being safety opt-in.</p>"},{"location":"m0_introduction/s4_exercises/","title":"4\ufe0f\u20e3 Exercises","text":"<p>Basic Rust exercises to come.</p>"},{"location":"m1_memory_hierarchies/","title":"1\ufe0f\u20e3 Memory Hierarchies, Computational Graphs and Compilers","text":"<p>Not all efficiency comes from optimizing the various computational details like multiplications, divisions and such of a function. Quite a large part of it, in fact, comes from optimizing how much you write to and read from memory. 'Which memory?' you might ask, rightfully. The answering the where, when and what of memory will be the focus of this module. We can almost always get more cores to throw at a problem, we can also, at least on the CPU, quite easily get more memory, but that does not change the amount of time it takes to get a piece of memory, only how much data we can have in memory before we have to go to a lower level, e.g. go from RAM to disk. This is even more important given the relatively slow improvement of memory over time.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#perspective","title":"Perspective","text":"<p>The further you move from simple, albeit heavy, problems such as a matrix-matrix problem to more heterogenous problems, such as training a neural network, the harder it can be to get good performance. How do you know or reason about what is where and when in complex systems like PyTorch, Tensorflow, JAX, Numba and Taichi? All of these frameworks, compilers and domain specific languages have to nudge you in different directions to give them the restrictions and hints needed to let them run your code as efficiently as possible. Nudges like defining your neural network as a computational graph. If you're unsure about what a computational graph is, the basic version is that you define a bunch of operations and how they relate to each other. Like input layer, followed by linear layer, followed by ReLU. But more on that later! Other advances include PyTorch, after several attempts with various degrees of success, finally introducing a compiler for optimizing the neural network you just defined. Or the functional programming style used by JAX in conjunction with the XLA compiler.</p>"},{"location":"m1_memory_hierarchies/#memory-hierarchies","title":"Memory Hierarchies","text":"<p>So what is memory anyways? Memory in a compute context is represented in several stages, all having their own capacity and speed. In order from smallest capacity and highest speed to largest capacity and lowest speed we have the registers, the L1-L3 caches, the main memory (RAM) and the disks. The registers are the fastest and smallest of the bunch. They reside right next to the parts of the CPU that does the computations. As a rule of thumb, most of the variables you declare in the scope of your function, unless there is A LOT of variables, will be kept in registers. The caches and the main memory all work in conjunction with each other as an invisible way of speeding up the accesses to the main memory (RAM).</p> <p></p>  A simplified view of the CPU memory hierarchy.  <p>Say you load in a file from the disk. If small enough, that entire file can be kept in memory. Which is great! We could keep all of the values in a great big array which we could access, like <code>current_value = data[index]</code>. But if you just wanted to read the first 5 values in the file in a loop, it would be incredibly slow to load those 5 values over and over again all the way from memory. What happens instead is that those 5 values might have separate copies in the L3, L2 and L1 caches, perhaps even in the registers. That would speed up things greatly. Whenever we asked for the first value we would first ask the L1 cache, do you have this value? If yes, that would be a cache hit, and we would pay a small amount of time to retrieve the value. If the L1 cache did not have a valid copy of the value, we would ask the L2 cache, and so on and so on, until we reach memory. If our file was too big to fit in memory, the operating system might even virtualize (don't worry about it) the memory and go all the way to the disk or to the internet to retrieve our value. Which is even slower than it sounds.</p> <p></p>  An example view of what the CPU memory hierarchy can look like with 2 cores.  <p>To further complicate things, multicore CPU's have each CPU sharing the disk, memory and L3 cache, sometimes they also share the L2 cache with a few other CPUs. We are also at risk of each core not just reading from the same values, but some of them could even modify one or more of the 5 values. At any point in time a value loaded from memory, to L3 cache, to L2 cache, to L1 cache, to the registers of thread A, might be invalid because thread B wrote to that value. This may have updated the value in memory and in thread B's registers, L1 and L2 caches, hopefully, it also updated it in an L2 and/or L3 cache it shared with thread A, but even then we would still need to move the value from L2/L3, to thread A's L1 cache and registers for it to be valid. Which is probably not happening. Multiple threads reading from a piece of data, which one or more threads are writing to is also known as a data race. Most likely thread A will end up with a stale version of the data and will continue as if the value had never been modified. Thread A will then write its own new version of the value, or just be working off an old version, resulting in incorrect results.</p> <p></p>  An example view of what CPU memory hierarchy can look like with 8 cores.  <p>Nudging the programmer (that's you!), to better define your program, not just line-by-line, but as a whole, to constrain these sorts of contentions, is one of the myriad reasons why frameworks like PyTorch can greatly speed up your code, if you help it along.</p>"},{"location":"m1_memory_hierarchies/#expanding-the-memory-hierarchy","title":"Expanding the Memory Hierarchy","text":"<p>To top it off we can expand this memory hierarchy with additional components, such as accelerators, networking and the internet! Let's start off with the GPU. It is an accelerator originally made for just computing graphics as fast as possible. It has a whole bunch of threads in it, meaning it can do very parallel work, like making every pixel of an image slightly darker. At the end of the 2000's, Nvidia saw a bunch of academics hacking the GPU to do stuff like fast fourier transforms using the fragment shader. Don't worry about what a fragment shader is, but shader basically means GPU program. So Nvidia releases CUDA as a pure compute (no graphics) API for using your GPU. It only runs on Nvidia GPU's though. Transfering memory from the CPU to the GPU and back, can be a quite explicit process. Not only does the CPU need to reserve some memory for copying to the GPU, the CPU and GPU have to be synchronized which can take a while, and then the data is usually transferred across the slower (compared to memory and cache) PCIe bus. This transfer is something you should always be thinking about if you are using a GPU for your program. Neglecting transfers is one of the fastest ways to slow code. The GPU also has its own memory hierarchy.</p> <p></p>  Image credit  <p>As you can see, this being representative of the Nvidia H100, there are 2 L2 caches and a whole bunch of smaller sections. Each of these smaller sections are a streaming multiprocessor (SM).</p> <p></p>  Image credit  <p>Here we have an L1 data cache and shared memory (more on shared memory later), shared between 128 threads. Each of these warps, Nvidia terminology for one of these four sections, have 32 threads with an L0 instruction cache, which is not matched for data. Additional accelerators exist, such as the neural engine featured in quite a lot of Apple products, and dedicated image and video processing hardware.</p> <p>You can even go outside of your current system. Two CPU's and eight GPU's could be tightly interconnected in a node, such as in the Nvidia DGX system. In the case of a DGX system everything is tightly interconnected with specialized hardware to minimize the time it takes to transfer data from one component to the other.</p> <p>Taking things even further, we could be sending data between more than one node, requiring yet another layer of communication, which is going to be slower than communicating internally in your CPU or in your node. When running on clusters with multiple nodes, the data you work from might have to be fetched from one or more storage nodes, which keeps your data between batch jobs. Taking neural network training as an example, if your data set is small enough to keep fully on the compute node you only need to load the dataset to the compute node before you begin training. Even better, the data set can be small enough that it fits, along with your model, completely on the GPU, meaning less transfers, less communication and better performance.</p> <p>If you wanted to make things worse however, and have your local systems administrator put you on speed dial, you would fetch your entire data set from the internet every time you launched a job. I am absolutely certain no one has ever just downloaded a Hugging Face data set whenever they launched a job... The internet can in this way be thought of as yet another, even slower, component of the memory hierarchy. Not much good comes from the internet. Try to avoid it. Except for this guide of course, which is very helpful.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#wrapping-things-up","title":"Wrapping Things Up","text":"<p>Hopefully, this teaser hasn't scared you away from charging ahead and learning more about memory hierarchies and computational graphs. Memory hierarchies are at the center of getting good performance in pretty much all programs and it is worth spending some time on having at least a tenuous grasp of how to use them.</p>"},{"location":"m1_memory_hierarchies/#5-further-reading","title":"5\ufe0f\u20e3 Further Reading","text":"<p>For a more in-depth explanation on the memory hierarchy see this chapter on Memory Hierarchy Design.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/","title":"2\ufe0f\u20e3 Soft Memory Hierarchies","text":"Memory hierarchy of the AMD Athlon.  Image credit  <p>As mentioned in the module intro, the CPU's memory hierarchy is represented by a series of hardware components with different sizes and speeds. But don't fret, memory hierarchies and their hardware design subtleties won't be the primary focus of this module. This section will focus on the various programming constructs to better use the memory hierarchy. First off we will start bridging hardware and software.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#getting-to-the-pointer","title":"Getting to the Point(er)","text":"<p>One of the core mechanisms in using memory is the pointer! All it does is point to pieces of memory. Why? Because a pointer is basically just an address. Anti-climactic, I know, but as one of the core building blocks of computing, we need to really examine these concepts from the bottom and up. If you have ever tried programming in C, you will invariably have been introduced to the pointer. The examples in this heading will be in C, but don't worry, we won't even define an entire function. Pointers are rife with opportunities for getting into trouble, to a degree where in Rust, which is made to be a reasonably safe language, you can't directly interact with a pointer unless you have an unsafe region around the pointer interaction. Yikes! On the other hand, you can get some of the most extreme performance by using raw pointers. So let's take a look!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#allocation","title":"Allocation","text":"<p>First of all, how do we get a pointer? Please note that checks for whether we have been given a valid pointer have been omitted. In the example below we get a pointer to a piece of memory which can hold up to 42 elements.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n</code></pre> <p>Let's break it down!</p> <pre><code>int element_count = 42;\n</code></pre> <p>We assign the number of elements to a variable in order to not have magic numbers.</p> <pre><code>int* integer_array;\n</code></pre> <p>This is actually bad practice. We have an uninitialized variable here. We could try and dereference the pointer, more on that in just a second, and try to access memory which we either don't have the right to access or which doesn't exist. The pointer at this point is likely to either be 0 or complete garbage. <code>int*</code> reads as \"a pointer to integers\" or \"address of one or more integers\".</p> <pre><code>integer_array = malloc(element_count * sizeof(int));\n</code></pre> <p>We ask for a memory allocation (malloc) from the operating system. What we get back is just a runtime dependent address. The address itself is what is known as a word. The size of the word dictates how much memory you can address in a system. If you have a 32-bit, 4 bytes, word and you use byte addressing, meaning one byte for every address, we can at most address 2GB of memory with a single word. If we have 64-bit words we can address more memory than we could possibly get. When you see something is a 32-bit or 64-bit operating system, this is why! It is also why we all of a sudden started using more than 2GB of RAM per computer in the 2000's. The address given by <code>malloc</code> will be different every time you run your code. Usually, any call to the operating system will be a very slow operation and should happen as little as possible. This can be stuff like writing to a terminal, accessing a file on disk, and so on. What we give malloc as an argument is the number of BYTES, as in 8-bits per element, we want. We want <code>element_count</code> elements which should each have a size of 32-bits (4 bytes). <code>sizeof(int)</code> returns 4. In total we ask for 168 bytes. <code>malloc</code> itself returns <code>void*</code>. Since C allows for implicit casting, what happens is that C, without us asking, changes the type to <code>int*</code>. Underlying it is the exact same thing. It is an address where 168 bytes allocated for us begins. What changes from <code>void*</code> to <code>int*</code> is how we dereference the pointer and what happens when we do.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#dereferencing","title":"Dereferencing","text":"<p>A pointer is a reference to another place in memory. Quite literally it is just a number. Dereferencing is a term for following the address to what it points to.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n*integer_array = 0;\n*(integer_array + 1) = 1;\ninteger_array[2] = 2;\ninteger_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>In this example there's three different ways of dereferencing shown.</p> <pre><code>*integer_array = 0;\n</code></pre> <p>In C, we use the <code>*</code> operator in front of the pointer to follow the address to the memory. The base pointer we got from <code>malloc</code> is the address of the first of the 42 elements in our memory. Another way of seeing it is that <code>integer_array</code> holds an address, let's say... 42. Our program now asks the CPU to write to the address 42, the number 0. So far so good. But then this happens.</p> <pre><code>*(integer_array + 1) = 1;\n</code></pre> <p>This is one of the myriad reasons why we needed to have an <code>int*</code>. If the address in <code>integer_array</code> is 42, to get the next integer element, we don't go to the address 43, which would just be the second byte of the first element. No, we want to go to the address 46, where the second element in the array begins. Since <code>integer_array</code> has the type <code>int*</code>, we have defined that each element is 4 bytes and we now have a stride of 4 bytes. We also need to keep track of the size of our allocation close to the pointer itself, as trying to access an element outside of our allocation will be catastrophic, and likely result in a segmentation fault. So, no <code>integer_array[42]</code>. Back to the line on hand. We put our <code>integer_array</code> in a parentheses to make sure the dereferencing doesn't happen until after we have changed the address. So we increment the base pointer (42) with a stride of 4 (46), and then dereference (*) to assign a value of 1 to the second element in our array.</p> <pre><code>integer_array[2] = 2;\n</code></pre> <p>A short hand for the previous line, is this line. <code>integer_array[2]</code> is shorthand for <code>*(integer_array + 2)</code>.</p> <pre><code>integer_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>With these lines we manipulate the base pointer itself, by reassigning a value of the base address (42), incremented by 3 (54), before doing a simple dereferencing and assigning a value of 3. This is not a recommended way of doing things. How do we ensure that we always have the pointer to the base address? The least you can do is to copy the base pointer and increment that. Why?</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n</code></pre> <p>Because we need the address to give the memory back to the operating system.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#deallocation","title":"Deallocation","text":"<p>Once we are done with the section of memory we have so graciously been granted by the operating system, we should remember to return it to the operating system. If we don't we might get a memory leak, which is when our program uses more and more memory until the program is stopped or crashes. The operating system might keep track of the memory though and clean up once our less than stellar code terminates.</p> <p>In C, we can return our memory like this, using the free function.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(integer_array);\n</code></pre> <p>Spot the error?</p> <p>We had two pointers and forgot to <code>free</code> using the base pointer, <code>base_integer_array</code>. This is undefined behavior, which means that there are literally no definitions of what will happen. It is really bad. What we should have done was this.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(base_integer_array);\n</code></pre> <p>Note that <code>free</code> takes a <code>void*</code>. Our <code>int*</code> is cast, without us asking explicitly, to a <code>void*</code>. The operating system just wants an address. This allows the operating system to mark the section, denoted by the start of the section, and probably by its own record of the length. Note also that the address (42) held by <code>base_integer_array</code> is still in play. It is what is known as a 'dangling pointer'. We could try to dereference it after giving it to <code>free</code>, which is the notorious <code>use after free</code>. This is also undefined behavior as we try to access memory that is no longer accessible by our program. What we could do is to set <code>base_integer_array</code> and <code>integer_array</code> to new values to denote that they were invalid.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\nfree(base_integer_array);\nbase_integer_array = NULL;\ninteger_array = NULL;\n</code></pre> <p>This does not however, stop us from trying to dereference those pointers, but it does allow for a more general check to see whether the pointers are still valid.</p> <pre><code>if (base_integer_array != NULL){\nfree(base_integer_array);\n}\n</code></pre> <p>If this all seems a bit scary, that's because it is. Anytime a system depends on humans just not making any errors and being rockstars at everything, it's a dangerous system and you should be on guard.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#access-patterns","title":"Access Patterns","text":"<p>While it is import that you increase your understanding of what it takes to get valid, predictable, boring code. Which is the best kind. What you are most likely interested in is for you to write more performant code. An absolutely essential part of getting performant code is how we access the underlying memory. Yes, we can address memory a single byte at a time with byte addressing. But, whenever we ask for a byte, the memory is transported as a cache line through the memory hierarchy. As in, the L3, L2 and L1 cache all receive an entire cache line. That cache line is usually 64 bytes.</p> <p>What is in the cache line is dictated by cache line alignment. If for example you had made a struct (it's like an object, but just the data) like the one below and you elected to turn off the auto-alignment with <code>__attribute__ ((packed))</code></p> <pre><code>struct __attribute__ ((packed)) my_struct\n{ short first; // 2 bytes \nint second; // 4 bytes\n}\n</code></pre> <p>and you made allocated an array of <code>my_struct</code> like so</p> <pre><code>int element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 6\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>If you had an alignment of say, 8 bytes, the last two lines would result in two cache lines being retrieved.</p> <p></p>  Bad cache alignment.  <p>Which is not good. What we could do instead would be to pad our struct a little bit, which is the default behavior in C.</p> <pre><code>struct my_struct\n{ short first; // 2 bytes \nshort _pad; // 2 bytes\n// Usually in C it will fix this automatically, padding\n// every element to a multiple of a value. This could for example\n// be 4 bytes.\nint second; // 4 bytes\n}\nint element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 8\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>Then our alignment becomes this.</p> <p></p>  Better cache alignment.  <p>And we now only involve a single cache line. Which to remind you, is quite a bit smaller than the more standard 64 byte cache line.</p> <p>Now that we have learned a bit about cache lines, we are equipped to actually talk about access patterns. I have made some Rust code for you which is located at <code>m1_memory_hierarchies/code/access_patterns/</code> or online.</p> <p>First off is sequential access. It is the one we usually strive for. We start at one end and go through every element until the end, from index 0 to the end. If everything is cache aligned, great! If not, the cost of not being aligned will probably be as low as it can be, when we aren't reusing any retrieved elements. If a value, say a 4-byte integer is spread across two cache lines, that specific value may have to be reconstructed which can be expensive.</p> <p>Next up is strided access. With strided access we only read every N elements. Based on the size of the stride and the size of the elements, it might result in each cache line only being used for a single element. In the implementations in the code there is both a non-wrapping and a wrapping stride implementation, meaning once we step over the end we wrap back around using a modulo operator. This is to ensure that it accesses the same amount of elements as the sequential access. With the non-wrapping stride we only access every N elements, but we also end up doing much less work.</p> <p>Finally, we have random access. This is basically the worst case scenario. We randomly select an element to access the same amount of times as the number of elements in the array.</p> <p></p>  Timing access patterns in Rust.  <p>Given that we just talked about cache lines, most of these numbers make good sense. Random access is catastrophic, wrapping strided access is bad, but most interestingly non-wrapping strided access, which actually accesses less elements than the others, is slower than sequential access for strides 2 and 3. With stride 4, where we are only accessing one fourth the elements of the sequential access pattern, we begin to get faster. But what do you know, sometimes the nice and predictable path, which might seem like we are doing more work actually runs faster. What a time to be alive!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#stacking-heaps-of-trouble","title":"Stacking Heaps of Trouble","text":"<p>If you aren't familiar with the stack and queue data structure types, this would be a good time to follow the link and familiarize yourself.</p> <p>The stack is not just a data structure, but also a core part of how all of the variables in your local scope are kept track of when the program enters into a function. The stack is a designated part of the memory allocated to your program. It starts at size 0. Once you enter a function, each local variable is pushed unto the stack. The stack generally requires that sizes are known at compile time. Once you call a function from within your function, the local variables are no longer accessible to the function you just entered, but once you return from that function, they are. When you enter that function, a pointer to where you called the function from is added to the stack and that function has its own local variables.</p> <p></p>  The call stack.  Image credit  <p>If you push enough of these frames unto the stack, you can get a stack overflow. This can for example happen if you write a recursive program that doesn't terminate. In general, using variables from the stack will be much faster than using variables from the heap. But we also can't return pointers to a stack variable as it might disappear or be overwritten at any moment.</p> <p>The heap, in this context, is not the actual data structure known as a heap. Instead it is a bunch of unstructured memory living in the same reserved space as the stack.</p> <p></p>  The stack and the heap sharing memory.  Image credit  <p>Thus if either one becomes too big they begin encroaching on the other. Everytime you ask for dynamically sized memory, it is allocated on the heap. This is a slow process and you have to remember to deallocate the memory to not get a memory leak. But the memory survives across functions. If you remember the pointer examples from earlier - the memory segment we asked for lived on the heap, whereas the pointer (address) itself lived on the stack. We are allowed to keep the pointer on the stack because a pointer is a known size at compile time. We can also have arrays on the stack, but they generally need to have a size known at compile time. Moving a pointer from place to place, is also a lot cheaper than copying every single element of a large array every time ownership changes hands.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-dynamic-array","title":"The Dynamic Array","text":"<p>The dynamic array is ubiquitous in C++ and Rust. It is quite often what we think about, when we think of arrays in those languages. C++ has <code>vector&lt;T&gt;</code> and Rust has <code>Vec&lt;T&gt;</code>. I highly recommend reading the first parts of the Rust Vec page. They are basically the same though and I will refer to them as vector from here on out. A dynamic array bundles up the behavior we saw earlier with the pointers, allocations and deallocations, but adds the ability to automatically create a new array that is larger (usually by a factor of 2) than the old array and move the old values over to the new array. The vector has three values. How much memory is in its allocation, <code>capacity</code>, how much of the memory is currently in use, <code>length</code>, and a pointer to the data which lives on the heap. The vector itself can live on the stack and make sure to free the memory it points to once the vector is dropped from the stack. The vector supports quite a few operations, but the core ones are <code>push</code>, <code>pop</code>, array access <code>[]</code>, <code>reserve</code> and <code>shrink_to_fit</code>.</p> <p>Let's start off though with how we allocate a vector (in Rust).</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\n</code></pre> <p>In this case we should get a completely empty vector. It will have a default <code>capacity</code>, because we didn't specify any capacity it should start with. Let's just say this <code>capacity</code> is 4. However, if we want to print the current size</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nprintln(\"{}\", data.len());\n</code></pre> <p>we would get an output of 0! We have a <code>capacity</code> of 4, but a <code>size</code> of 0. Meaning, we have 4 integers of 4 bytes each on the heap, but they are unitialized (containing garbage values), and we have not used any of them. If we however use <code>push</code> to add some actual data and then print</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\n</code></pre> <p>we would print the number 2. Now we have live, initialized values on the heap at indices 0 and 1. We can print them by accessing the values directly.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\nprintln(\"{}\", data[1]);\n</code></pre> <p>In this case we print 2, 0 and 1. Push finds the first unused index, which is conveniently indicated by the <code>size</code> value, increments <code>size</code> and puts the value into the designated index. If we pushed 5 values however, once we reached the 5th push, assuming the default capacity was 4, we would see the 5th push taking a lot of time compared to the other 4 pushes. In this case the vector would allocate a new memory segment on the heap with a size of 8, copy all of the values from elements 0-3 and then add the 5th value to the vector. Conversely, we can also use the <code>pop</code> function.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\ndata.pop();\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\n</code></pre> <p>Now we end up printing the values 1 and 0. In theory, a dynamic array should move to a smaller array at some point. Such as, when at a quarter of the reserved capacity. But in practice, Rust doesn't move to a smaller array unless explicitly asked to do so using the \u00b4\u00b4\u00b4shrink_to_fit\u00b4\u00b4\u00b4 function. In that case it will allocate and move to an array that is exactly the size of <code>size</code>, thus also making <code>capacity</code> the same. In practice, you should only do this for large arrays which are unlikely to see more elements added to it.</p> <p>But, in the case of knowing how many elements we actually we want to put in our vector, or at least an expcected minimum amount, we can just create the vector in a way where it has already reserved that amount of capcity. If you can at all do this, it is one of the easiest ways to get better performance as you remove a whole bunch of allocations, deallocations and copying. There's a variety of ways to control how allocation happens. The simplest one, if you know how many elements you want in your vector in advance, is to just create the vector with that capacity.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::with_capacity(5);\ndata.push(0);\ndata.push(1);\ndata.push(2);\ndata.push(3);\ndata.push(4);\n</code></pre> <p>In this case, we have been unambigously upfront about how many elements we will put in the vector. It was created with a <code>capacity</code> of 5 and a <code>size</code> of 0. We can also tell the vector to make sure we have a <code>capacity</code> of at least N. If it already has <code>capacity</code> to meet the minimum, nothing happens. If it doesn't it will allocate, copy and deallocate.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nlet element_count: usize = 42;\ndata.reserve(element_count);\nfor index in 0..element_count {\ndata.push(index as i32);\n}\n</code></pre> <p>There are more idiomatic ways to do this in Rust, which might also be faster, but you get the gist!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-vector","title":"The Vector","text":"<p>But, we aren't just interested in single lists of numbers, sometimes, we would even like a matrix. In Rust we can have fixed size arrays defined like so:</p> <pre><code>let data: [i32; 2] = [0, 1];\n</code></pre> <p>If the sizes given to the array definition are constants, known at compile time, the array will be stack allocated. From what we have learned previously, the elements will be stored in memory in the order of 0 and 1. But what if we create a two-dimensional array?</p> <pre><code>let data: [[i32; 2]; 2] = [\n[0, 1], [2, 3]\n];\n</code></pre> <p>In Rust the elements will be ordered in memory 0, 1, 2, 3. But that is not a universal truth. This is called row-major ordering and is the standard layout in C, C++, Rust, Python and most modern languages. The alternative is column-major which is seen in Fortran and Matlab. In column-major ordering the elements would be ordered in memory as 0, 2, 1, 3. With row-major ordering the memory will be most tightly packed in the last dimension from the left. To iterate through a 3 dimensional vector, this triple for-loop would access the memory in order.</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = [\n[[1, 2], [3, 4]],\n[[5, 6], [7, 8]]\n];\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\nfor x_index in 0..x_dimension {\nfor y_index in 0..y_dimension {\nfor z_index in 0..z_dimension {\nprintln(\"{}\", data[x_index][y_index][z_index]);\n}\n}\n}\n</code></pre> <p>Where as if Rust favored column-major ordering the in-memory-order traversal would be</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = [\n[[1, 2], [3, 4],\n[[5, 6], [7, 8]]\n];\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\nfor z_index in 0..z_dimension {\nfor y_index in 0..y_dimension {\nfor x_index in 0..x_dimension {\nprintln(\"{}\", data[x_index][y_index][z_index]);\n}\n}\n}\n</code></pre> <p>If you think back to stride and cache lines, traversing our 3-dimensional array like the above in the actual case, where Rust is row-major, would be like the stride access we looked at earlier. We could also do this with nested vectors.</p> <pre><code>let mut data: Vec&lt;Vec&lt;i32&gt;&gt; = Vec::&lt;Vec&lt;i32&gt;&gt;::new();\ndata.push(vec![0, 1]);\ndata.push(vec![2, 3]);\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nfor x_index in 0..x_dimension {\nfor y_index in 0..y_dimension {\nprintln!(\"{}\", data[x_index][y_index]);\n}\n}\n</code></pre> <p>This is even worse though. We now have a 2-dimensional array, which is highly flexible, but we have to dereference two pointers for every access.</p> <p>There is another way of doing this with a vector, which is the way I will be using multi-dimensional arrays in this module. It involves using a single dimensional vector as if it had more dimensions.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(vec![0, 1, 2, 3]);\nlet column_count: usize = 2;\nlet row_count: usize = 2;\nfor x_index in 0..row_count {\nfor y_index in 0..column_count {\nprintln!(\"{}\", data[x_index * column_count + y_index]);\n}\n}\n</code></pre> <p>We just create a vector with as much room as we need and then access it with a bit of calculation. We've flattened our matrix and can now both have it dynamic and with arbitrary dimensions. We can even dynamically decide to see the matrix in a different way, for example by deciding to swap the number of columns and rows. The formula to access each element is to multiply the index by the dimensions that come after it and add it to the next index. For example with three dimensions <code>x</code>, <code>y</code> and <code>z</code>, the index would be calculated by</p> <pre><code>x_index * y_size * z_size + y_index * z_size + z_index\n</code></pre> <p>and for the two dimensions <code>x</code> and <code>y</code>, we would access the 2-dimensional matrix with</p> <pre><code>x_index * y_size + y_index\n</code></pre> <p>I really hope this makes sense. Once it clicks it is a very simple formula, if a bit wordy. Some libraries will work like this under the hood but wrap it in an interface for you to simply access it like it was a multi-dimensional array.</p> <p>To wrap it up I have made a performance test of these approaches. The code doesn't match completely as we need bigger dimensions to get a good test. The code is at <code>m1_memory_hierarchies/code/the_vector/</code> or online.</p> <p>Implementing all of the methods described above in both row-major and column-major form, as well as an element-wise version, where we flatten the multidimensionality to save the administration of two of the for-loops, so we just get one for-loop running across a vector, we get the following numbers.</p> <p></p>  Access times for multidimensional arrays.  <p>The functions named Multi-Array are stack allocated instead of heap, which is why they are that fast. I was however unable to run them for 64x64x64 and 128x128x128. Rust refused citing a stack overflow. Interestingly as well, the element-wise function can be quite fast as it saves two of the for-loops. So, if you can, use element-wise. Otherwise, the row-major single vector function seemed to work the best. How much is saved by not having the two extra for-loops depends on how much work you are actually doing in each iteration. In this benchmark we do pretty much nothing.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#move-copy-clone-soldier-spy","title":"Move, Copy, Clone, Soldier, Spy","text":"<p>Now that we have examined how we can deal with a more expensive type, compared to the simpler integer or float, let's expand the scope a little bit. How do we actually move around these vectors as data? In each language there are some implicit rules, which can have wide reaching consequences, both in terms of correctness and performance.</p> <p>In Python, variables are all references to an underlying object, which is freed when there are no longer any references to said object. Don't worry about it too much, it is a 3\ufe0f\u20e3 concept I will introduce further down the page. But, it does have consequences when this happens</p> <pre><code>x = [5, 5, 3, 42]\ny = x\n</code></pre> <p>There aren't actually two lists, but two references to a list which has some data on the heap. This can be a bit problematic, as you now have two variables, which can both write to the same list without the other knowing. Once both <code>x</code> and <code>y</code> go out of scope, the list on the heap will be deallocated (eventually).</p> <p>In C and C++, the following actually results in two different lists on the heap, kept by two different variables.</p> <pre><code>vector&lt;int&gt; x{5, 5, 3, 42};\nvector&lt;int&gt; y = x;\n</code></pre> <p>C++ is copy by default, and this is a deep copy. Which is what Rust would call a clone. Rust however, is move by default.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x;\n</code></pre> <p>Once the values in <code>x</code>, the <code>capacity</code>, <code>size</code> and the pointer to the memory on the heap, have been moved from <code>x</code> into <code>y</code>, <code>x</code> is no longer accessible. The Rust compiler will complain. We can however, move it right back.</p> <pre><code>let mut x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x;\nx = y;\n</code></pre> <p>Now, <code>y</code> is inaccessible at the end. We could also create a scope, after which <code>y</code> is dropped, but the ownership is not moved back to <code>x</code>.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\n{\nlet y: Vec&lt;i32&gt; = x;\n}\n</code></pre> <p>Unless we move the values back ourselves.</p> <pre><code>let mut x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\n{\nlet y: Vec&lt;i32&gt; = x;\nx = y\n}\n</code></pre> <p>To actually create two lists, like we did in the C++ example, we have to explicitly ask for a deep copy - a clone in Rust terminology.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x.clone();\n</code></pre> <p>Usually, in Rust at least, adding lots of clones everywhere is the way to get around the borrow checker and have everything be correct. But once your first prototype is finished, one of the easiest improvements to your performance will be to search for all instances of .clone() and see whether there is some other solution that might work better. Rust isn't fighting you in this case, even if it can be strict, it is trying to protect you from having multiple write-enabled references to the same data, as in the Python example, which could make for incorrect code. C++ does have these move operations as well, it is even highly recommended a lot of the time. It is however, not the default behavior of the language.</p> <p>Rust has something called traits (don't worry about it). One of these traits is the <code>Copy</code> trait. If a type implements the <code>Copy</code> trait, it will be copied rather than moved when assigned to a new value or passed as an argument to a function. It is sort of like an implicit version of <code>.clone()</code>, except in the case of deeper structures, such as <code>Vec&lt;T&gt;</code>, in that case, it would copy all of the stack values, <code>capacity</code>, <code>size</code> and the pointer to the memory on the heap.</p> <p>But hold on a minute! That is illegal! We would have two pointers with full write rights. Which is illegal in Rust! Which is also why <code>Vec&lt;T&gt;</code> doesn't implement <code>Copy</code> and this has all been a ruse, for your edification.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#stacks","title":"Stacks","text":"<p>Now let's start looking at a couple of fundamental data structures. Next up is the stack. It isn't an array, but most implementations are just an array used in a restricted fashion. A stack is what is called a Last In, First Out (LIFO) data structure. The usual example is, imagine a stack of cantina trays. If you put a tray into the stack, in order to get a tray, you have to take the top tray, you can't remove a tray that is below the top tray.</p> <p></p>  Pushing a value on to the stack. The states are from before the push.  <p>If we implement this using a vector, we need at least the following 3 functions - <code>push</code>, <code>pop</code> and <code>peek</code>. <code>push</code> you might already know as the default mechanism for adding an individual element to a vector. The element to push is inserted at index <code>size</code> and <code>size</code> is incremented. With a <code>pop</code>, the element at index <code>size - 1</code> is returned and <code>size</code> is decremented. With a call to <code>peek</code>, either a copy or a reference to the element at <code>size - 1</code> is returned. Most, if not all functions are already implemented on the vector types, but if we want to maintain the invariant that all of the elements from indices 0 to <code>size - 1</code> are all valid, you need to make sure that only the stack related functions are called. In that way, if you need a stack, you should use not just a vector type, but a stack type, which might just be a wrapper around a vector, but also restricts anyone using that type to maintain the invariants needed for a valid stack. In that way sending a <code>Stack&lt;T&gt;</code> from function to function, instead of a <code>Vec&lt;T&gt;</code>, will communicate how the value is supposed to be used.</p> <p></p>  Popping a value from the top (end) of the stack. The states are from before the pop, and were the result of the previous push.  <p>Stacks scale well and all operations would be constant time, except when enough values have been pushed to necessitate a resize. However, the cost of this is low enough that across all of the operations it averages out and becomes amortized constant time.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#queues","title":"Queues","text":"<p>Queues, just like stacks, are a fundamental data type centered around constant time operations mostly implemented on top of dynamic arrays. Queues maintain a First In, First Out (FIFO) principle, just like queues of people. The first person to enter a queue, should be the first person to leave it. Now we no longer have <code>pop</code> and <code>push</code>, but <code>enqueue</code> and <code>dequeue</code>. Enqueueing is basically the same as <code>push</code> on a stack. An element is added to the index at <code>size</code>, except, the queue needs two new variables, <code>front</code> and <code>back</code>. Once the <code>back</code> index extends beyond the <code>size</code> or <code>capacity</code>, it can just wrap back around and starting again from 0, as long as it does not become equal to the <code>front</code> value. If it does so and <code>capacity &lt; back - front</code>, it can resize itself and adjust.</p> <p></p>  Enqueueing a value from to the back of the queue. The states are from before the enqueue.  <p>Resizing is just one way to handle the overlap. In quite a few real-time systems, we don't want the system to be overwhelmable. If data comes in too fast to process, and it keeps coming in faster than we can process, we might instead say that the <code>front</code> will move with the <code>back</code> if they become equal, thus letting the older data be overwritten. Other options could be to have whatever is trying to submit an element, wait until a spot opens up in the queue or the element could be \"added\", but not actually added to the queue. You'd of course like to be certain of how your queue type would handle being full. It's a central property and you should make sure if you are constructing systems with lots of data that you use a queue with the right behavior for your system.</p> <p></p>  Dequeueing a value from to the front of the queue. The states are from before the dequeue.  <p>Just like the stack, your local vector type probably has the functionality, but if you use it as a queue, you should probably just use a queue type, restricting any usage to maintain and communicate that it's a queue.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-smart-pointers","title":"3\ufe0f\u20e3 Smart pointers","text":"<p>Ok, so I promised previously, that I would explain how Python, and most other garbage collected languages, deal with assigning one variable to another. If you recall the previous example</p> <pre><code>x = [5, 5, 3, 42]\ny = x\n</code></pre> <p>We start by making a list and assigning a reference to <code>x</code>. In this case <code>x</code> is not the actual owner of the list. Instead, the system takes ownership of the list, and <code>x</code> is a live reference to that list. The system keeps track of how many live references there are to the list. Once <code>x</code> goes out of scope, the live reference count for the list decreases by one. Once the live reference count reaches 0, it is deallocated or marked available for future deallocation.</p> <p>Until we hit the end of the scope, and <code>x</code> and <code>y</code> disappear, there are two live references to the the list created at line 1. While a fine enough solution at first glance, sometimes, answering the question \"what is alive\" can be quite difficult. More on that in the garbage collectors section.</p> <p>When dealing with raw pointers, like we saw earlier, once a system grows beyond absolute simplicity, sharing multiple pointers to the same object becomes a bit complex. If you have 5 pointers to the same object floating about how do you ensure it isn't used after freeing? Who deallocates the pointer and who ensures that the pointers are no longer valid? This at the absolute crux of safety and your program not blowing up in C and C++.</p> <p>In C++11+ and Rust, we can elect to use something called smart pointers. Which can handle some of the intricacies for us. First off there is the unique_ptr, as in C++, or the Box in Rust. I will just refer to <code>Box</code> from here on out, their behaviors seem to be more or less the same. <code>Box&lt;T&gt;</code> is like a <code>T *</code> in C (pointer to object of type T). With two notable exceptions. It cannot be copied. As in, you cannot have multiple instances of <code>Box</code> pointing to the same underlying object. Thus <code>Box</code> in Rust, as well as in C++, requires that ownership is moved, and not copied. The other notable difference from a raw pointer is that once the \u00b4\u00b4\u00b4Box\u00b4\u00b4\u00b4 goes out of scope, the object on the heap that it is pointing to is deallocated automatically.   <pre><code>let box_variable: Box&lt;i32&gt; = Box::new(42);\nlet mut other_box: Box&lt;i32&gt; = box_variable; // box_variable no longer accesible due to move\nlet copied_variable: i32 = *other_box; // Dereference and copy the underlying value, this is not a move\n*other_box += 1;\nprintln!(\"{}\", copied_variable); // prints 42\nprintln!(\"{}\", *other_box); // prints 43\n</code></pre> <p>Next up are the shared pointers. They are essentially what Python is using in the example from earlier. In C++ it is called shared_ptr, in Rust it comes in two versions; Rc and Arc. <code>Rc</code> stands for reference counted. It is only made for single threaded usage as the reference count itself is susceptible to a data race, which you may recall, is several reads and/or writes to the same value. This could result in the count of live references being incorrect and the underlying value never being deallocated. <pre><code>use std::rc::Rc;\nfn main() {\nlet shared_reference_a: Rc&lt;i32&gt; = Rc::new(42); // Live references = 1\nprintln!(\"{}\", Rc::strong_count(&amp;shared_reference_a)); // prints 1\nlet shared_reference_b: Rc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 2\nprintln!(\"{}\", Rc::strong_count(&amp;shared_reference_b)); // prints 2\n{\nlet shared_reference_c: Rc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 3\nlet shared_reference_d: Rc&lt;i32&gt; = shared_reference_b.clone(); // Live references = 4\nprintln!(\"{}\", *shared_reference_c); // prints 42\nprintln!(\"{}\", Rc::strong_count(&amp;shared_reference_a)); // prints 4\nprintln!(\"{}\", *shared_reference_d); // prints 42\nprintln!(\"{}\", Rc::strong_count(&amp;shared_reference_d)); // prints 4\n}\n// shared_reference_c and shared_reference_d are now dropped\nprintln!(\"{}\", Rc::strong_count(&amp;shared_reference_b)); // prints 2\n// Live references = 2\nprintln!(\"{}\", *shared_reference_a); // prints 42\nprintln!(\"{}\", *shared_reference_b); // prints 42\n}\n</code></pre> <p><code>Arc&lt;T&gt;</code> is here to solve exactly that issue. It uses atomic reference counting. Atomics will be introduced in the parallelism module. But in this context, it means that the reference counting is thread-safe, but a bit slower.</p> <pre><code>use std::sync::Arc;\nfn main() {\nlet shared_reference_a: Arc&lt;i32&gt; = Arc::new(42); // Live references = 1\nlet shared_reference_b: Arc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 2\n{\nlet shared_reference_c: Arc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 3\nlet shared_reference_d: Arc&lt;i32&gt; = shared_reference_b.clone(); // Live references = 4\nprintln!(\"{}\", *shared_reference_c); // prints 42\nprintln!(\"{}\", *shared_reference_d); // prints 42\n}\n// shared_reference_c and shared_reference_d are now dropped\n// Live references = 2\nprintln!(\"{}\", *shared_reference_a); // prints 42\nprintln!(\"{}\", *shared_reference_b); // prints 42\n}\n</code></pre> <p>While <code>shared_ptr</code> from C++ allows you to mutate the value it refers to <code>Rc</code> and <code>Arc</code> do not. They require a synchronization primitive wrapped around your underlying value, like <code>Arc&lt;RwLock&lt;i32&gt;&gt;</code>, but that is more advanced usage, and don't worry about it right now. Other than the atomicity, and being shareable between threads, <code>Rc</code> and <code>Arc</code> work more or less the same.</p> <p>Finally, we have the weak pointer. This basically exists to weaken cyclical references. If object A refers to another object, object B, with an <code>Rc</code>, while the object B refers to object A, we have a problem. When either, or both go out of scope, they will not be deallocated as there is live references to both.</p> <p>Try to take a second and imagine this and the things that can go wrong when there are multiple references interconnected.</p> <p>Go on.</p> <p>I'll wait.</p> <p>To solve this issue, the weak pointer comes to the rescue. It is along for the party, but doesn't actually keep things alive. In Rust it is called Weak. It can reference the same underlying object as the shared pointer it comes from, but does not contribute to the live reference count. As such, it can allow you to have cyclical references, without causing a memory leak. If object A points to object B with an <code>Rc</code> reference, but object B points to object A with a <code>Weak</code>, once object A goes out of scope, both object A and object B can safely be deallocated. <pre><code>use std::rc::Rc;\nuse std::rc::Weak;\nfn main() {\nlet shared_reference: Rc&lt;i32&gt; = Rc::new(42); // Live references = 1\nlet weak_reference: Weak&lt;i32&gt; = Weak::new(42); // Create a weak reference from nothing\nlet weak_shared_reference: Weak&lt;i32&gt; = Rc::downgrade(&amp;shared_reference);\nprintln!(\"{}\", Rc::weak_count(&amp;shared_reference)); // prints 1!\n}\n</code></pre> <p>For more information on smart pointers in Rust, there is a nice example here and another example about reference cycles, which is what we needed weak pointers for.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-the-vector-reloaded","title":"3\ufe0f\u20e3 The Vector Reloaded","text":"<p>This isn't meant to be a one-to-one representation of how tensors work in <code>numpy</code> or <code>PyTorch</code>, but combined with creating different views on the same underlying one dimensional memory as we learned about earlier, we can look at a few other fun concepts in different ways to arrange tensors.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#strided-access-and-transposition","title":"Strided Access and Transposition","text":"<p>One of the most used operations is the matrix-matrix multiplication. If we assume two 2D matrices as input and output into another 2D matrix, one of those input matrices will be accessed with a stride access in a column major form.</p> <p></p>  Matrix-matrix multiplication. The numbers indicate access order.  <p>There is a solution for this. We can just transpose the second input matrix. Transposition, as you may remember, is flipping a matrix around the diagonal. Another way to do this is to flip all coordinates. (0, 0) becomes (0, 0), but (3, 1) becomes (1, 3). Transposition is an expensive operation however, and we have to create additional code for whether the second input matrix is transposed and the other multiplication code for just that case. We also need to keep track of which matrices are transposed. In a more general, flexible system, or one in which the system does a lot of optimization without user input, we also need to evaluate when and where to tranpose matrices. But, if the matrix is fairly static and is read from often, it can definitely be worth the time and effort.</p> <p></p>  Matrix-matrix multiplication with the second matrix transposed.  <p>Now, lets try out a simple example! Checkout the code at <code>m1_memory_hierarchies/code/strided_access_and_transposition</code> or check it out online .</p> <p>Interestingly, when running the code there doesn't seem to be much of a difference until the matrix sizes become quite big. Why do you think that is?</p> <p></p>  Difference gets bigger as the matrices get bigger.  <p>One guess would be a combination of the compiler aggresively optimizing the code, the branch prediction of the pipeline (don't worry about it) being really good at guessing these very uniform workloads, but most importantly, the caches doing a lot of the heavy lifting for us. Once the caches run out of space we begin to see a gap between the two ways of doing it. This might be more pronounced on the GPU. In most cases you should probably start with making the simplest and easy comprehendable code and try out, and measure, potential optimizations before spending your time going down rabbit holes. This is will be a bit of a theme in the next few sections. There won't be much of a difference between techniques until the caches begin running out of space. At least if you aren't coding something really terrible, like randomized access.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#permuted-arrays","title":"Permuted Arrays","text":"<p>Sometimes we might want to change around elements in a matrix, without permanently executing the change. Not permanently executing these changes may also allow for several different views of the same data. So let's take a look at how permutations work.</p> <p>In the example below, the permutation is kept track of with the data in one vector and the index changes in another. The second of the two indices we need to map from one index to another is implicit. Thus for our permutation vector, index 0, means that at index 0 in our new permuted array resides at index 4 in our original data.</p> <p>This is likely to be quite a bit slower compared to normal sequential access as we now have to follow more than one pointer to get to our data.</p> <p></p>  Create permutations of an array by creating a list of indices and permuting that list.  <p>If we only view the data through the lens of a single permutation array anyway and we read from this vector alot, we might as well execute the permutation. If we wanted to be able to undo the permutation, we could just keep track of the permutation we executed and undo it later. But we should now be able to get back to sequential access performance.</p> <p></p>  If reading a lot from the same array with the same permutations, go ahead and execute the permutations.  <p>There is a middle ground however, which is if we are just permuting rows. As long as the rows are long, we should be able to get partially sequential access, at least if we are moving through the elements in order.</p> <p></p>  Offset some of the cost of permutations, by just permuting rows.  <p>Now, lets try out a simple example! Checkout the code at <code>m1_memory_hierarchies/code/permuted_arrays</code> or check it out online</p> <p></p>  Huh, that's weird. There doesn't seem to be much of a difference.  <p>It seems pretty much the same.</p> <p></p>  The differences appear as the cache runs out of space with bigger data sizes.  <p>Once we run out of cache however, the executed permutation is quite a bit faster. Permuting just the rows can also give quite a performance boost.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#jagged-arrays","title":"Jagged Arrays","text":"<p>A weird form of array is the jagged array. A 2D matrix can't simply be expressed as having dimensions NxM, but Nx? or ?xM dimensions. As in N rows, each with their own individual lengths, or M columns, each with individual lengths. It's a highly flexible scheme, but unless you are absolutely sure you need it, you should absolutely avoid it.</p> <p>In the example below, we attain this complete flexibility by using a vector of vectors, which as you may recall is really bad for performance.</p> <p></p>  Create a jagged array by using a vector of vectors.  <p>If the difference between the smallest row and the largest row isn't too big, we can sacrifice a bit of additional memory for allocating all rows as if they had the same length and keep track of the length of the active sections in each row in a separate vector.</p> <p></p>  Slightly better now with the data in a single vector.  <p>If we really wanted to compact the jagged array above, we could remove all of the non-active segments (denoted -1) and use the auxiliary array to indicate where each new row starts. Just like the first permutation scheme, we are dereferencing two pointers for access.</p> <p>Finally, we could do all of this, still under the constraint that we have a reasonable ceiling on the max length of each row, by interleaving the auxiliary array with the data array.</p> <p></p>  All of the data in a single vector, with the blue values being the amount of active data in the row.  <p>We can do this either compacted or non-compacted.</p> <p></p>  All of the data in a single vector, with the blue values being the amount of active data in the row. Compacted data.  <p>We've now removed what was a consistent implicit form. We no longer have random access to the row lengths. We also now have to translate from whatever type is in the data array to valid integers for indexing. If the data is integers, casting won't be much of a problem, but for floating point numbers we have to be sure to get it right. If a number is not a whole number we are likely to have the number floored to the nearest whole number. Instead we have to go from row length to row length and find out how many indices we have to move forward to get to the next indicator. As such, to get to the lower right corner element (42), we would first have to read index 0, jump 4 spots forward to index 4, read the 4, jump 5 spots forward to index 9, and then jump forward 2 elements to get to what in a dense array would be index [2, 1].</p> <p>This sort of makes me miss the auxiliary array. We can sum up the jumps to denote where each row starts. This would allow for compaction of the data while keeping us to just two jumps. Note that we now keep track of the length of each row by taking the difference between the starting index of the row we are looking to find and the beginning of the next row. Which is also why I have inserted an extra starting index, which points to the end of the array. Otherwise, we can't get the length of the last row.</p> <p></p>  As we compacted the data, we can keep track of the starting index of each row in an auxiliary array.  <p>Now for a simple performance benchmark. Checkout the code at <code>m1_memory_hierarchies/code/jagged_arrays</code> or check it out online</p> <p></p>  Huh, that's weird. There doesn't seem to be much of a difference.  <p></p>  There we go, we ran out of cache!.  <p>Note that the only version which is not extremely slow for inserting values is the naive one. But in most other cases our final optimized version JaggedArraySizeCompactedAux seems to be the winner. It doesn't take a lot of memory compared to the other solutions and it seems to be in some cases on-par with the fastest (with a reasonable variance) or the fastest. In most other cases the NaiveJaggedArray seems just fine. Again, don't overcomplicate things and measure the differences for your case. In any case, you should avoid a jagged array if you can. And especially the CompactedJaggedArray, which costs the least memory, but has a catastrophic access time due to needing to accumulate the indices needed to find the row index. Plus, having the indices be interleaved with the values is problematic as we mix control flow and data, as well as needing to accomodate casting a data value to an index value. Please don't do that!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#sparse-arrays","title":"Sparse Arrays","text":"<p>Finally, we have the sparse array. In the case of huge matrices with lots of values we don't care about, especially 0's, we can use the run-length encoding we just saw to encode values. This usually results in having to reconstruct where the indices are on the fly. The method below is ok for singular values, such as a very large matrix with just diagonal values. If we have a band around the diagonal we could modify the strategy from the last example in the jagged arrays section.  </p> <p></p>  A sparse array created with run-length encoding. We could of course also just linearize the indices to get a single number.  <p>For this to be more efficient than the dense version, you usually need at least 90% sparseness, or an array so big that you are having issues with memory. Sparse matrices also require their own separate implementations and can be hard to parallelize.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-hash-maps","title":"3\ufe0f\u20e3 Hash Maps","text":"<p>Another fundamental data structure is the hash map. The hash map takes a key type and a value type. The value type can pretty much be anything, don't worry about it! But where things get really interesting is the key value. What a hash map does is to take a key value and translate it into an array index using something called a hash function. A very simple hash function takes a number, adds a number, multiplies by a very big prime number and then modulos that number by a number representing how much space we have available. The base recommendation is that a hash map should have at least twice the space needed to densely represent the same number of elements.</p> <pre><code>// Not actually a good hash function\nfn example_hash_function(key: Vec&lt;char&gt;) -&gt; usize {\nconst PRIME: usize = 6457;\nlet mut hash: usize = 0;\nfor element in key {\nhash = ( (hash * 31) + key as usize ) ^ PRIME;\n}\nhash\n}\n</code></pre> <p>Generally, a hash map will have constant time lookup and insertion. The reason for the recommendation of at least a factor 2 in space is collisions! A collision is when two different keys hash to the same value in our storage. Remember that we can have both the initial key that we queried with, and the post-hash key used for indexing into storage. One way of resolving the collision is to keep searching our storage until we find an empty spot. But then if we query our hash map and the first index we look at in storage, we iterate the array until we find a key that matches the one we queried with. Much like vectors, the hash map could dynamically expand to accomodate inserted data. Once we are done with insertions, we might have a fragmented performance. If we know we are done and have a significant amount of elements which need to be queried a lot, we can usually ask the data structure to <code>.shrink_to_fit()</code> or <code>.rehash()</code>. Rehashing will reconstruct the structure to be made as if it had only been the elements currently stored, all along.</p> <p></p>  A number of keys have been inserted in random order. We try to find the entry corresponding to the key \"Ni\" at index 3. But its natural spot was already taken by a spillover from the index 2. We find the entry in the next index instead. This is also known as open addressing.  <p>I will reiterate a theme here - if it can be done with a basic array, it should probably be done with a basic array. Of course there are different, more optimized methods for implementing hash maps, you can usually find a few different ones based on the specific needs for your usage, i.e. if you need better insertion performance or better read performance, but this is basically what you need to know. In Rust it is <code>HashMap&lt;K, V&gt;</code>, in C++ it is <code>unordered_map&lt;K, V&gt;</code>, in Python and C# it is called a dictionary. You can use anything for the key in Rust, as long as the type implements the <code>Hashable</code> trait. You can even use strings. This can be very useful for keeping an assortment of random data which you need to distinguish between. For example, if you needed to keep track of different layers of a neural network with random access, you can just create a new string \"Linear0\" and use that as a key for the first linear layer and its contents, and then \"ReLU0\", \"Linear1\", \"ReLU1\", \"Softmax0\" and so on. If possible, it is more efficient to use small types as your key. Such as an integer.</p> <p>Now for a simple performance benchmark. Checkout the code at <code>m1_memory_hierarchies/code/hash_maps</code> or check it out online</p> <p>As you can see the hash map using integers clearly outperforms Strings. To be fair, every insertion in the string based map, requires a clone of the original string, the read and update only requires a reference. But we can expect just about a factor 2 performance difference by using the simpler type with the simpler hashing function. It should however, be noted that the string keys were all just the integer keys as strings, which might have an influence on the distribution in the hash table. What we could do in our previous neural network layer example would be to have an integer value representing each layer type and then the id. We could relegate them to different parts of an integer. This could for example be the first 20 bits reserved for the layer type and the last 44, or perhaps just 12, bits reserved for the layer id. This does however incur a significant amount of extra code and the code will become more complex and implicit, so it's probably only worth it if you are doing A LOT of accesses for each layer.</p> <p>Generally, hash maps have an alright performance. C#'s dictionary lookup performance will usually go down hill at around 30k entries though. This doesn't happen for arrays. You can read more about different hash table implementations here.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-graphs-and-trees","title":"3\ufe0f\u20e3 Graphs and Trees","text":"<p>Now that we have dicked around with variations on a theme (that theme was arrays if you were in doubt), let's look at a different fundamental data structure. Graphs! Not the kind with the lines... wait these have lines too, uuuh, not the kind that has an x and a y axis, but the kind that has some circles with some arrows between them. \"But wait!\" you say, \"The heading says 'Graphs and Trees'\" you say, well, trees can be seen as a subset of graphs, while all graphs are not necessarily trees.</p> <p>Graphs and trees are some of the absolutely fundamental data structures which you need to be acquainted with. Along with arrays, queues, stacks and hash tables, they are the fundamental building blocks with which you can make pretty much anything. Graphs and trees are a bit special, however, in them being potentially easy to implement, but also very easy to mess up. Languages like C and C++ let you implement them with relative ease, but implementing graphs and trees without cyclical references (which can cause memory leaks), without data races, without dangling pointers and other robustness issues, is actually quite hard. Sometimes even fundamentally unsafe.</p> <p>I have used Rust as one of the primary languages for demonstrating and benchmarking things for you. The examples under this header will be more along the lines of toy examples as Rust code for graphs and trees can get quite hairy if you don't want to just sprinkle <code>Arc</code> everywhere. And even then you might end up having to battle cyclical references. It's really nice that the compiler puts on guard rails for you and herds you towards safe behavior. Implementing graphs and trees in Rust is notoriously difficult for this exact reason. Which is not to say that it is easier in C/C++, the compiler just doesn't stop you from doing something problematic.</p> <p>Anyways... the rest of the module will be about how using data structures like computational graphs, which is essentially what is created when you define your entire neural network on a single object in PyTorch, can speed up your code immensely as the structure allows the library/framework/compiler to reason about your program. Essentially, computational graphs communicate the intention of your program ahead of time before you start running everything in a loop. It can help the library/framework/compiler to optimize your code, optimize where the data should be located, when the data should be moved to/from the GPU, when two operations can be fused and so on.</p> <p>Additionally, I will take a look at one of the simpler trees, the binary tree, and if you are interested in graphics or computer vision, the octree is recommended reading.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#graphs","title":"Graphs","text":"<p>Ok, so let's get this show on the road. Graphs are primarily made up of two things, nodes and edges. Edges are references from one node to another. In a diagram they are usually represented by a line, some times with one more arrows on the ends. Edges can be represented by indices, pointers, smart pointers or something else that I can't think of right now. The node on the other hand, can be \u2728 whatever you want \u2728. It can even be just a number or an index to the corresponding data payload if you have seperated the graph structure from the data payloads.</p> <p></p>  A bidirectional graph. Each edge points both ways.  <p>Graphs come in lots of different flavors, but the three most important, and fundamental, are bidirectional, unidirectional and DAGs. Bidirectional means that the edges go both ways. If node A points to node B, node B also points to node A. Unidirectional graphs, you guessed it, means that the edges only point one way. That does not dictate that node A and B can't point to each other, but that it's not the default and it requires inserting two edges into the graph. Note that edges can also have weights or other values themselves.</p> <p></p>  A unidirectional graph. Each edge points one way. Note that edges can also have weights.  <p>Finally, the DAG, which stands for directional acyclical graph, is a unidirectional graph which does not contain cycles. A cycle is not just node A pointing to node B, which points to node A, it can also be node A pointing to node B pointing to node C pointing to node A, and so on an so forth until we have an infinite number of nodes to traverse until we get back to node A again, like going all the way to Mordor just to go back to the friggin Shire. No eagles will save you. You will just have to walk home. As you can imagine this can be a costly property to assert, unless we devise mechanisms to prevent this from happening in the first place.</p> <p></p>  The unidirectional graph is verified as being a DAG through a topological sorting. No edges points backwards.  <p>In the diagram, I have sorted the previous graph topologically. As long as none of the edges go backwards, we have a DAG. In general, if you are reading this, you should try to avoid graphs with cycles. It's a headache and you'll end up down a headscratching rabbit hole. It's also a good source of memory leaks if you haven't implemented your graph or tree in a certain way.</p> <p></p>  A neural network formulated as a computational graph.  <p>Note that formulating a neural network in advance like this, also allows us to perform dimension checking between all layers before running the network.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#trees","title":"Trees","text":"<p>Trees can be seen as a subset of graphs. They can be both bi- and unidirectional. Typically, there is a root node which will point to one or more child nodes. If the tree is bidirectional, the children will be pointing back. Leaf nodes are nodes which are not pointing to any children. Nodes which are not the root, but also not a leaf node are usually called internal nodes.</p> <p></p>  A binary tree where parent nodes point to children nodes, but children nodes don't point back.  <p>Typically, a tree can be really good for sorting data, like getting the biggest value, it can be good for finding things spatially, like, give me all of the nodes in a 3D scene which can be seen by the camera, or give me the closest number to some query. The hierarchical nature of the tree lends itself well to getting approximately <code>log(N)</code> performance in a situation which would typically have <code>N</code> performance. This typically requires that the tree is fairly balanced. Meaning that the maximum length from root node to any leaf node is reasonably close.</p> <p></p>  A balanced and an unbalanced binary tree. Note the sparseness and the differences in minimum and maximum height (distance from root node).  <p>One key difference which makes trees very powerful, compared to the more open definition of graphs, is that we need rules to define what makes a tree. Once we know these explicit rules, we can sometimes take advantage to make implicit assumptions of the structure, which can save quite a lot of space, reduce the amount of indirections we need to follow in order to traverse the structure and make it easier to serialize (write it to a file on disk) the tree.</p> <p></p>  A bidirectional tree. Note if the pointers pointing from children nodes to parent nodes are strong pointers, the tree is rife with cyclical references."},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#binary-trees","title":"Binary Trees","text":"<p>Binary trees are some of the simplest trees. Any node has at most two children. These are usually called <code>left</code> and <code>right</code>. In C and C++, they could be raw pointers or smart pointers, and you would have to check whether they were <code>NULL</code> or <code>nullptr</code> whenever you were considering whether child nodes were available. In Rust, you might have something like <code>Option&lt;Arc&lt;Node&gt;&gt;</code> and you would have to check whether the child was <code>None</code> or <code>Some(child)</code>.</p> <pre><code>struct BinaryNode {\npayload: i32,\nleft: Option&lt;Arc&lt;BinaryNode&gt;&gt;,\nparent: Option&lt;Weak&lt;BinaryNode&gt;&gt;,\nright: Option&lt;Arc&lt;BinaryNode&gt;&gt;,\n}\n</code></pre> <p></p>  A unidirectional binary tree with weak pointers from child to parent. In this case, due to the regular structure of the binary tree, we could have made do with indices.  <p>The baseline definition doesn't go much further than that. But, some variations built on the binary tree, like the heap (not the same as the one we talked about earlier), enforces that the binary tree is sorted and allows you to insert variations. Allowing the min or max value to bubble up, requires a sorting of the tree, but it allows you to very quickly get the minimum or maximum value from a list of nodes. The very predictable structure of the binary tree also allows for easy, memory efficient, implementation using just an array and no pointers. Especially if it is sorted as we need less array elements marked as empty.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#implementing-graphs-and-trees","title":"Implementing Graphs (and Trees)","text":"<p>Implementing graphs is generally considered hard in Rust specifically, which makes sense, because of the many caveats and potential issues in graphs. Dynamic graphs especially are problematic and you should consider very carefully whether all the logic is correct. To make things more difficult, constructing a graph, even if it has to spend the vast majority of its time as a read-only artifact, has to have a construction phase where pointers can be used and you can end up creating cyclical references. Uni-directional DAGs are easier, as long as you don't have to verify their correctness, but if implementing trees where you would like a pointer from the child to the parent, you can use a strong pointer from parent to child, and a weak pointer from child to parent. With graphs, in general, you cannot easily make a constraint that enforces that each node in your graph is only ever pointed to by a single strong pointer. What you can do however, is to contain all of the nodes in a graph object which has a strong reference to every single node, and the connectivity between the nodes being dictated by weak pointers. This will tie the lifetime (when the object is alive and not deallocated) to the containing object. What is unresolved here is how you can then get writeable access to the nodes, which is significantly more complex and I won't go into the details here, as it could easily be its own page. Another thing is... we can do all of this without pointers. We still have to contain all of the graph's nodes in a containing graph object. This object can instead of holding a pointer to every single node and the connectivity being dictated by pointers, just use indices. If you have all of your nodes in a vector without being contained by a unique pointer, the connectivity can just be a list of indices. Node A points to node B and node C. Easy peasy. We do have to trawl which nodes point to which if we want to remove a node, or we can keep an additional connectivity list for node A, specifying all edges pointing to node A, but again, let's just keep to the case where we have a construction phase, and then a reading phase- where lots of actors can read from the graph. In that case, if lots of functions would otherwise pass around pointers to a node, they can just pass around the node index. They can then ask the graph object for access to node N.</p> <p>Finally with trees, if the structure and rules are well defined, we can use implicit rules and just skip connectivity. In the case of the binary search tree, we can simply use an array and the knowledge of its doubling nature. In that case we know index 0 will always be the root. Index 1 will always be the left child, index 2 will always be the right child. To access any node's (index N) children, we merely have to read from index <code>N*2+1</code> for the left child and <code>N*2+2</code> for the right. We can handle a node not being present in this otherwise dense structure, by having a means of representing an empty value, but the greater the sparseness, the more inefficient this linearized tree structure becomes. The implicit/predictable structure makes the linearized treeqv easily serializeable (writing it to a file on disk) or transferable to and useable on GPU's.</p> <p>A better explanation of graphs in Rust and graphs in Rust using indices </p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#octrees","title":"\ud83e\uddec Octrees","text":"<p>Octrees are elevant for all of the specializations that aren't deep learning, especially computer graphics. But it might be relevant for deep learning too if you do stuff related to geometry or spatial data.</p> <p>Octrees are mostly concerned with sorting space. For every node, there are 8 children. If it is sparse, there are up to 8 children. What cannot change however, is the regular structure. Every node covers a certain space. The space covered by the child nodes are strictly within this space and are halved on each axis based on the center point of the parent node. Child 0 would be the eighth of space with <code>x</code>, <code>y</code> and <code>z</code> starting from the minimum value of the parent space up to the center point of the parent space. Child 1 could be the eighth of space the same as child 0, except with the x range starting from the midpoint's <code>x</code> value, going to the maximum <code>x</code> value of the parent space. So on and so forth, all child nodes gets an eighth of space. But again, there doesn't need to be exactly 8 active children, they do all need to go into predictable slots. If the definition of child 0 is what I wrote earlier, that range ALWAYS needs to reside in child 0. It cannot be moved to other children or other slots. One nice property of the octree is that we can describe any path from root to leaf by a sequence numbers from 0 to 7.</p> <p>Now let's talk about payloads. A typical use case within graphics is to use an octree to reason about which scene geometry to render or to use for nearest neighbor queries. Let's start with the simpler payload, point clouds. We have a list of three dimensional points. We want to find the nearest one relative to our query point. This is quite useful for algorithms like ICP. We start with the whole array of points and then continually go through our points sending them to one of the 8 children until a child receives only a single point, at which point that child node becomes a leaf node. Once the octree is built we can traverse the tree keeping track of which points have been closest so far. There is one issues though, given a query point Q, we might have a current closest point A, found in cell 0. The euclidean distance between point Q and point A might be 350. That is great so far. But right on the other side of the spatial divide in cell 7, there is another point, point B, with a distance to point Q which is only, let's say, 42 units from point Q. We only find that point if we continually search all relevant cells to point Q within some cell distance, e.g. if we know point Q is contained by some cell, we always need to examine the neighboring cells. But just the neighboring cells. We still need to compare our point Q against a good number of points, but it is way less than the potentially hundreds of millions of points.</p> <p></p>  The blue star is our query point. The full arrow line is towards the closest point, if we do not search the neighbors.  <p>For nearest neighbor queries having a single point per leaf node is wildly inefficient though, and you should consider fattening up the leaf nodes to contain more points, and in some cases points in the interior nodes as well. These could be efficiently searched by sorting them into linearized octrees. More on those in a future module. Quite often a point is not much more than 4x32-bits, in which case it is wildly inefficent to have more than 1 pointer per node. You might also end up with a stack overflow if you try to build the octree recursively. Last time I tried that in C++ I got a stack overflow at a depth of 1000. If you absolutely need a pointer based tree, try to add nodes of interest to a queue instead and just process that queue. E.g. you arrive at node X, it has 8 children. You deem 4 of them to be of interest, add all 4 to a processing queue, then dequeue the next node for you to process. This might take you all over the tree though. Another option could be using a stack. For spatially larger payloads, like meshes, you might also need to keep a reference to that geometry across more than one node, ending up with some geometry being evaluated more than once. You win some, you lose some. But it's all the same to me. It's the eighth of space.</p> <p>Another use case where the octree is very useful is when deciding what to render and at what level-of-detail. It also makes for a useful abstraction over virtualized geometry. More on that in a later module.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-garbage-collectors","title":"3\ufe0f\u20e3 Garbage collectors","text":"<p>Garbage collection is a way of freeing the programmer of having to deal with which memory is and isn't relevant. It is usually implemented by most variables (especially the heap allocated ones) being reference counted or otherwise tracked, which we will see later in the tracing section. Once a variable is found to no longer be referenced it is either immediately cleaned up or cleaned up during a garbage collection pass. A full garbage collection pass can be quite expensive, and if the implementation is not particularly optimized, lock the whole system while it is being performed as to not have memory that has just been cleaned up referenced anew.</p> <p>Garbage collectors aren't really that relevant to the rest of the guide, but if you are coming from Python, C#, Go or Java this section will use some of the concepts previously introduced on this page to give you a quick perspective to how garbage collectors work. This post takes a look at how python handles garbage collection although, a bit light on the details for the generational garbage collection. In the following sections I will introduce three different types of garbage collectors, and finally set you up with a few tricks for working with the garbage collector.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#reference-counted-garbage-collectors","title":"Reference Counted Garbage Collectors","text":"<p>Reference counting garbage collection is one of the simplest forms of dealing with garbage collection. Imagine that there is an <code>Rc&lt;T&gt;</code>, like we saw earlier, wrapped around every heap-allocated variable. Once the amount of references reaches 0, the object is deallocated. Simple, can be handled locally, scales well, doesn't burden the entire system with a lockdown to clean up, which makes it good for real-time systems which need to be responsive at all times and not have noticeable freezes. What makes it not quite usable is, that it is up to the programmer to not create cyclical references. Node A and Node B cannot refer to each other without causing a memory leak, despite not being referenced by anything else. They cannot be cleaned, unless one of the references is a weak reference. Just like the <code>Weak&lt;T&gt;</code> type we saw in the smart pointer section. But it is up to the programmer to make sure that the weak references are used correctly throughout the system, which isn't necessarily non-trivial.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#tracing-garbage-collectors","title":"Tracing Garbage Collectors","text":"<p>Tracing garbage collection on the other hand follows every root, this could for example be the variable holding the pointer to the root node of your graph, if there even is such a thing, and then following every pointer making sure to mark all the objects it finds along the way as not being ready for clean-up. This does however require that all the memory is frozen. There can't all of a sudden be new references to some of the objects or some of them be removed. Once the marking process has completed, all of the objects are traversed and every object not marked is cleaned up.</p> <p>Another more sophisticated method, promises better performance by using a white, gray and black marking. All objects start marked as white, and are then moved to grey, and then finally to black. Objects marked in white are possibly accessible from roots and are candidates for collection. Gray objects are definitely accessible from roots and might have pointers to objects marked in white. Black marked objects are definitely accessible from roots and definitely do not have pointers to the white set.</p> <p>You can read more about tri-color marking here.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#generational-garbage-collection","title":"Generational Garbage Collection","text":"<p>Generational garbage collection is a different technique which sequesters allocated objects into different memory regions. These regions, usually 3, are based on the age of the object. If an object survives a garbage collection pass it is promoted from one region to the next, older region. The youngest region will usually be significantly larger than the two older regions and it is estimated that most garbage collection will happen in the youngest region. This strategy might not find all unreachable objects, however, and can be supplemented by an occasional expensive full mark-and-sweep to ensure that no memory leaks go undetected for too long. For more on generational garbage collection .</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-virtualized-memory-hierarchy","title":"3\ufe0f\u20e3 Virtualized Memory Hierarchy","text":"<p>A simplified definition of virtualized memory is a single address space that doesn't correspond 1-to-1 to physical memory. As we have seen earlier in jagged arrays and permuted arrays, if we have all of our data in memory the caches, compiler and branch prediction take care of hiding memory access latencies, quite a bit, however, what if we don't have all of our data in main memory?</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualized-memory-and-operating-systems","title":"Virtualized Memory and Operating Systems","text":"<p>The operating system itself can, and will, virtualize your memory. It may at some point decide to spare the main memory, probably because it doesn't have any more, and instead allocates temporary space on the disk to swap in and out of main memory. This is painfully slow, but happens seamlessly behind the scenes to be able to continue to allocate more memory for your program. The programmer does not have to do anything as the virtualization is hidden. Usually, there will be hardware support for the virtualization with components such as a dedicated memory management unit.</p> <p>Each process, your program would be its own process, is given its own virtual memory space. Meaning that your program might see its addresses start in very low numbers despite a number of other processes running concurrently on your computer. In face, while the address space given to your process might look continuous it is probably fragmented, scattered across diffent physical locations, but the virtualization makes it appear continuous. In general, it is a major security risk for programs to read memory outside of the memory allocated for it. This is also known as a segmentation fault. The operating system dislikes this concept so much that it is likely to just kill your program entirely. If you have ever programmed C or C++, you have probably tried this making this mistake and your error has been met with swift and uncompromising reprisals. The virtual memory space allocated for your process, for stuff like heap and stack will typically look as below.</p> <p></p>  The stack and the heap sharing memory in their own virtual address space.  Image credit."},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualizing-your-own-application","title":"Virtualizing Your Own Application","text":"<p>As I just described in the preceding virtualized memory section, the operating system will store temporary data on the disk if it runs out of space in main memory, keep track of what is not in memory and in disk instead and, when needed, invisibly load requested data into memory while swapping some other piece of data unto the disk. But we can make our own virtualized memory too! We could for example have a dataset for training a neural network that is astronomically big. Terabytes even! We have for some reason decided it is of the utmost importance that we always random sample the entire dataset. So we randomly pick 20 samples. 4 were already on the GPU, 2 were in main memory, 4 were on disk and the remaining samples are on the internet. It will be slow as all hell, but that is something we can optimize too. The easiest would of course be to limit how often we decide to sample the parts that are on the internet. We could for example choose to download a random block from the internet portion of our virtualized memory, random sample from that block for a while and then download a new block. We could hide this by defining data object structs for each sample which have an optional payload, along with a bit of additional bookkeeping. We could make a sort of address space by keeping the list of samples, which need to be A LOT smaller than the total data set for this to work, and using heuristics on this list of samples and associated metadata to optimize our virtualized data set. We could give a priority to each of the samples based on how long ago they were sampled last and in which block they were located, on which physical memory they were located (the cloud is just someone else's computer). Optimizing these types of systems can be quite a fun algorithms and systems optimization process.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualized-rendering","title":"\ud83e\uddec Virtualized Rendering","text":"<p>Another use of this is the rendering of data sets too large to fit in a users computer. You preprocess all of the data you need to visualize into a tree structure and then just keep the tree in memory at all times. You can then render render progressively, which is where as soon as the camera stands still you render across multiple frames into the same uncleared buffers, letting the user see progress while the system downloads, unpacks and renders the entire scene. This also allows for rendering of scenes which are too big to fit in GPU memory or even main memory.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#5-further-reading","title":"5\ufe0f\u20e3 Further Reading","text":"<p>An explanation of memory allocation, stack and heap in C.</p> <p>A more rigorous explanation of the register, cache, main memory and virtual memory parts of the memory hierarchy. For even more virtual memory.</p> <p>Check out the memory and cache specs for Apple's M1 series.</p> <p>For an example of coding a tri-color marking garbage collector.</p> <p>For more about garbage collection in Python, more basic garbage collection in Pyton or garbage collection in Java.</p> <p>For more on implementing a heap with an array, priority queues, binary trees, binary trees using arrays in Python. These pages have implementation details in C/C++/Python.</p> <p>If you are into spatial data structures and/or graphics, computer vision, etc here's some links for octrees, BVHs, Kd-Trees, a comparison between kD tree and octree, levels-of-detail for point clouds (chapter 3) and levels-of-detail for meshes.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/","title":"2\ufe0f\u20e3 Computational Graphs","text":"<p>Conveying the importance of computational graphs to people who were probably using Python to program neural networks was the motivation for doing this whole thing in the first place. So now let's get to business.</p> <p>Computational graphs are more or less a way to communicate the flow of your program. It can allow a library or a framework to keep data at various levels of the memory hierarchy. It can allow it to check that all of the dimensions fit for the data, it can make assumptions about fusing nodes (combining them), remove redundancies and removed unused elements.  </p> <p>Let's take a look at this defined network from PyTorch's own documentation.</p> <pre><code>class Net(nn.Module):\ndef __init__(self):\nsuper(Net, self).__init__()\nself.conv1 = nn.Conv2d(1, 32, 3, 1)\nself.conv2 = nn.Conv2d(32, 64, 3, 1)\nself.dropout1 = nn.Dropout2d(0.25)\nself.dropout2 = nn.Dropout2d(0.5)\nself.fc1 = nn.Linear(9216, 128)\nself.fc2 = nn.Linear(128, 10)\n# x represents our data\ndef forward(self, x):\n# Pass data through conv1\nx = self.conv1(x)\n# Use the rectified-linear activation function over x\nx = F.relu(x)\nx = self.conv2(x)\nx = F.relu(x)\n# Run max pooling over x\nx = F.max_pool2d(x, 2)\n# Pass data through dropout1\nx = self.dropout1(x)\n# Flatten x with start_dim=1\nx = torch.flatten(x, 1)\n# Pass data through ``fc1``\nx = self.fc1(x)\nx = F.relu(x)\nx = self.dropout2(x)\nx = self.fc2(x)\n# Apply softmax to x\noutput = F.log_softmax(x, dim=1)\nreturn output\n</code></pre> <p>In PyTorch the user does not fully create a graph, but if the user makes sure <code>x</code> is on the GPU by calling <code>x.to_device()</code> all of the functions will be executed on the GPU until the output is transferred back to the CPU. This reactive paradigm might be part of why the first complete iteration of a PyTorch training loop will be signifcantly slower than the subsequent loop. Not to mention all of the allocations behind the scenes for backpropagation.</p> <p>If you use <code>torch.compile()</code> it will do something called tracing behind the scenes. You don't need to worry about the specifics, but just know that it creates a computational graph from the Python code above and then optimizes that code to run faster and/or use less memory.</p> <p>So why does it need the graph? That is something the rest of this module will try to answer, along with a really basic introduction to fusion, where layers are combined to be more efficient.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#what-is-a-graph","title":"What is a Graph?","text":"<p>A graph is a type of structure, for our needs, a data structure. In s0 there is a more elaborate examination of the concept for 3\ufe0f\u20e3. So for right now, just give the link to graph's wiki page a quick look. You should get the gist just by looking at a few of the images.</p> <p>Once you have done this, just know, that the rest of this module won't actually use a real graph. The graph we will concern ourselves with will be defined as being one way, unidirectional, and each node can point to at most one other node. This reduces the entire graph to just being a list of operations which will be executed sequentially.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#the-network-we-want-to-support","title":"The Network We Want to Support","text":"<p>For illustrating how computational graphs can benefit your code we don't really need to support a lot of operators. We need transfers to and from the GPU (eventually), a linear operator (matrix-matrix multiplication followed by an addition), a ReLU operator (single call to a max function with 0) and a softmax operator. The softmax operator is the most complex part, don't worry I will show you some CPU code that is fairly easy to understand. The GPU version gets a bit complicated and is only constructed in a fairly simplistic version.</p> <p></p>  Our minimal computational graph example will only contain these 5 operations.  <p>For the computation graphs we will use, note that due to the constraints of transfer first and last, and softmax next to last, the only difference is the amount of linear and ReLU layers. Eventually, when we look at fusion, new operators will be produced, linear-ReLU and linear-ReLU-softmax. We will only use 32-bit floating point, <code>f32</code> in Rust, as the data type.</p> <p></p>  An example computational graph.  <p>The code for the rest of the module can be found at <code>m1_memory_hierarchies/code/computational_graphs/</code> or online.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#whats-in-a-tensor2d","title":"What's in a Tensor2D?","text":"<p>First of all we are going to start on the CPU. We are going to create a data type which will hold the data our operators consume on the CPU. Let's call it <code>Tensor2D</code>. Our 2D tensor will actually be a simple piece of one dimensional memory under the hood and we will keep track of the number of rows and columns to find out how to access each piece of data. If you are in the root directory for <code>computational_graphs</code> go to <code>src/shared/tensor_2d.rs</code> or online.</p> <p>Start by taking a look at the definition of the <code>Tensor2D</code> struct at the very top. The <code>derive</code> stuff at the top is asking some macros to automatically implement (derive) some traits (interfaces and behavior) automatically. <code>Clone</code> means we can call a Tensor2D element as below -</p> <pre><code>let some_tensor: Tensor2D = Tensor2D::new(0.1, 8, 8);\nlet copy_of_some_tensor: Tensor2D = some_tensor.clone();\n</code></pre> <p>This creates a complete and total copy of <code>some_tensor</code>. If we manipulate or move <code>some_tensor</code>, <code>copy_of_some_tensor</code> will not be affected as they no longer have anything to do with each other.</p> <p>Next, take a look at the <code>new</code> function. In it we create a new <code>Tensor2D</code> by creating a new <code>Vec&lt;f32&gt;</code> with size <code>row_count*column_count</code>. Each element is given a value of <code>scale*index</code>. This is just for testing purposes so I found it useful for this to not be all zeros and not all random numbers. This allows us to verify that the GPU implementations are functionally equivalent to the CPU implementations.</p> <p>We don't need to otherwise relate to the values of <code>row_count</code> and <code>column_count</code>. Even if we implement a two dimensional structure on top of a piece of one dimensional memory, when we are iterating through all elements, such as we do when setting all of the elements to some value or accumulating the sum of all elements we can do away with the two dimensional stuff. Keeping up that illusion unneccesarily induces extra cost in the form of more time and code spent on control flow statements like <code>for-loops</code> and <code>if-statements</code>.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#implementing-operators","title":"Implementing Operators","text":"<p>In this section I will be going through various implementations of the three operators and their fused variants and show benchmarks to show you how big of a performance impact these sort of first guess optimizations can have even without profiling or microoptimizations.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#linear","title":"Linear","text":"<p>There's some dimension checking functions, you can just ignore those. They use <code>debug_assert</code> statements to raise an error if the dimensions of the tensors given to a linear layer function don't match. <code>debug_assert</code> is the same as an <code>assert</code> statement, except it is only run in debug mode. I did it this way to incur only a small hit to performance. You probably passed the <code>linear_layer</code> function on the way down, it just acts as a wrapper around the <code>linear_layer_preallocated</code> function. If you haven't already allocated a tensor to use as output, it will make one for you. If you do this a lot however, such as in a loop, you should be using the preallocated version to not have memory allocations in your loops.</p> <p>Finally, let's go down to the <code>linear_layer_preallocated</code> function. There are three main sections. One is the call to the <code>debug_assert</code> function from earlier, to check for valid input and output dimensions, the second is the matrix-matrix multiplication which needs three whole for-loops and finally the bias section. Note the use of linearized accesses, if you need a reminder what that is all about, go back to <code>m1::s0::The Vector</code>.</p> <p>It's not too bad, but we could do better, although we won't do more efficient implementations of matrix-matrix multiplication, note that the read accesses of the weights tensor is strided. We could have implemented that some tensors could be transposed, but you get the point. So we have a triple for-loop and a double for-loop in our linear operator. Try to think, based on the contents of the last couple of sections, what would be good first optimizations for this function?</p> <p>While you think about that in the back of your mind, I'll take a quick detour to introduce a very simple but finnicky concept - inlining!</p> <p>Inlining in Rust, and most other languages, is done via an annotation to the compiler. It is usually more of a hint or request than an actual instruction. In Rust it looks like the derivation of traits we saw earlier - <code>#[inline(always)]</code>. In that case it actually is more of a command. There's other variants you can put inside the parantheses like <code>#[inline]</code>, which is more of a suggestion, or <code>#[inline(never)]</code>. Inlining is basically taking all calls to that function and substituting it with the code from the function. This is largely good for very small functions, such as if we made a function for making our linearized array accesses prettier to look at, but for large functions it either does nothing or makes the performance worse. So, in general, unless you were trying to examine the concept of inlining like we are now, you should stick with <code>#[inline]</code> and suggest inlining to the compiler, without demanding it. The compiler is pretty smart and will usually figure out what is best. As you will see in the function <code>linear_layer_preallocated_inline</code>, the function itself is not different in any way.</p> <p>Next up is the function <code>linear_layer_local_accumulation</code>. Now it's memory hierarchy time! While I didn't get rid of the strided memory access of the weights tensor, I removed some of that control flow overhead, by linearizing the bias part. Now it is just a single for-loop, instead of two, because the dimensions of the output and bias tensors match, and there are no transformations to be made. So we get to just iterate through all elements. I also elected to not accumulate the result of each output element directly in the output tensor. Instead I accumulate in a local variable, which will hopefully be kept in a register.</p> <p>Think back! Where is the register located? And why can it be problematic to accumulate the sum in the output tensor?</p> <p>The bias is the same. But that does mean we are writing to the same output element twice. Let's move the bias calculation into the matrix-matrix multiplication loop. <code>linear_layer_optimized</code> moves the bias to just before the writing of the accumulated result to the output tensor. If the bias calculation was a lot larger, this might not be faster. In some cases for highly complex loops, it can instead make sense to separate them into more loops, which is a process called <code>loop fission</code>. I have only used a small subset of loop optimizations, but you can read about more ways of optimzing a loop here.</p> <p>Ok, so try and run the code locally! To begin with go to the file <code>src/lib.rs</code>. Comment out all of the lines within and including the <code>if configuration.compatible_gpu_found {</code> line. Then in your terminal navigate to the root folder, the one containing the <code>src</code> folder, and write <code>cargo run --release</code>. Your computer will now run a bunch of benchmarks relevant to the rest of this section. You can find the output in <code>computational_graphs/outputs/benchmarks/stack</code>. The one that should have been generated on your computer that we want to look at now is called <code>linear_layer_cpu_benchmark_stack.png</code>. If you weren't able to run it locally, don't worry, I got you covered!</p> <p></p>  Benchmarking linear operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Huh, it seems our loop fission didn't really matter in the grand scheme of things! The graph is quite high resolution to allow you to zoom in. The x-axis is the size of the tensors. Only NxN matrices are used. The y-axis is time in nanoseconds averaged over 1000 runs. Note how the lines are piecewise linear. There are two points where all of the lines get quite a bit slower and scale worse with the size of the tensors. Why do you think that is?</p> <p>You guessed it! You are seeing the size of the tensor becoming too big for the different caches! It looks like the last bend happens at 4096 elements. This corresponds to a 64x64 matrix. 4096 elements of 32-bits, or 4 bytes, each corresponds to 16384 bytes, or 16 KB. We have 4 of these tensor structs, so we should have total allocations of 64 KB just for that data and then add in all of the other memory used by the application and everything else running on the laptop at the time. But then again, the sampling is quite sparse. You can try and add more data points and see if you can narrow down the sizes of your caches.</p> <p>This might be a good time to experiment with changing the values in <code>lib.rs</code> for</p> <pre><code>let loop_count: usize = 10;\nlet loop_range: Vec&lt;usize&gt; = (2u32..8u32).map(|x| 2usize.pow(x)).collect();\n</code></pre> <p><code>loop_count</code> is how many measurements are made per data point. <code>loop_range</code> is a vector of matrix sizes. In this case, the first element is 4, so the first measurement will be done with matrices sized 4x4. Currently, it takes a range between 2 and 8 (not including 8) and yields a vector of sizes of 2^N. So 4, 8, 16, 32, 64, 128. If you wanted all values in the range of 0 to 100 you could write</p> <pre><code>let loop_range: Vec&lt;usize&gt; = (0..101).collect();\n</code></pre> <p>You can zoom in on these parts of the graph by modifying <code>lib.rs</code> to just test values in these interesting ranges. Like right around the size of the last bend. Another thing to note is that only the versions of the linear operator that uses local accumulation significantly outperform the naive version. One surprise is that keeping the bias outside of the matrix-matrix loop, is better performing than moving the bias in. Sometimes it really is better to keep things simple. So from now on, the <code>linear_layer_local_accumulation</code> version will be the preferred one.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#relu","title":"ReLU","text":"<p>Next up, we have the ReLU function, which is embarrasingly simple. It returns the maximum of two numbers. One of the numbers is zero. Done.</p> <p>So, move your way down to the <code>relu</code> and <code>relu_preallocated</code> functions. Here I also completely flatten out the two dimensional tensor to save on the amount of time spent on control flow vs. actual computation. But then we get a new variant. The <code>relu_inplace</code> function. Because it is possible to do the ReLU operation directly on the input array, let's try that variant as well and see what the effect is. Finally, we have <code>relu_inplace_inline</code>. This function is definitely small enough that it might benefit from inlining.</p> <p>So now go back into the same folder where you found the linear benchmark output and look at what your computer could crunch out. Alternatively, if you couldn't run it, I got you covered here.</p> <p></p>  Benchmarking ReLU operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Note the huge difference between the naive version and the other ones. Why do you think there is this huge difference?</p> <p>You guessed it! All of the other functions have either preallocated the output matrix, or do the operations inplace. Since the ReLU operation is so simple, it becomes easily dominated by allocation and memory costs. The difference between the preallocated version and the inplace version is not as big, but still substantial enough to warrant the optimization. Inlining on the other hand didn't make a big difference in this case. It is still doing one read and one write after all. Go back and look at the how much was gained by inlining the much more complex linear operator in the previous benchmark! Go on!</p> <p>Inplace operations are also available in PyTorch. The ReLU actually has a flag for the inplace version.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#softmax","title":"Softmax","text":"<p>Now let's look at the softmax operator. It isn't that complicated, except as we'll see when we talk about the GPU version, the max and sum values are actually non-trivial to implement on a GPU using more than a single work group (a group of threads).</p> <p>Anyways, head down to <code>softmax</code> and <code>softmax_preallocated</code>. In <code>softmax_preallocated</code> we have 3 distinct sections. The first section is finding the global maximum value. Once that is found a modified sum reduction is performed using the max value. Finally, the sum and max are used to calculate an offset value which is used to modify all of the values in place. The sum of all the values should now be 1. Note that we only use one for-loop for all of the elements as softmax doesn't care about dimensions. This allows us once again to cut down on control flow overhead. Once again, we create an inplace and an inline-inplace version. Try and look at the code for a second and go through why we can an unproblematic inplace version of softmax.</p> <p>Got it?</p> <p>Finally, checkout the results in your output folder! Or if you couldn't run them locally I have some here -</p> <p></p>  Benchmarking Softmax operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As can be seen, the inplace and inline versions beat the naive and preallocated versions by an extremely wide margin. The different between the inplace and inline seems quite small, but the inplace version seems to consistently beat the inline version.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#fused","title":"Fused","text":"<p>Finally, let's see what we can do with combining the operators with fusion! First take a look at the functions <code>linear_layer_local_accumulation_relu</code> and <code>linear_layer_optimized_relu</code>. In these two I combine the linear operator with the ReLU operator. In the local accumulation variant, the bias and ReLU are handled in their own distinct loop (loop fission). In the optimized variant, they are in the same loop as the matrix-matrix multiplication.</p> <p>Now let's add in the softmax operator. Take a look at <code>linear_relu_softmax_fused_fission</code> and <code>linear_relu_softmax_fused</code>. In the fission version, the max value is found in the same loop as the bias and ReLU computation. In fused version, bias, ReLU and max are all moved into the ending of the matrix-matrix multiplication loop.</p> <p>Finally, checkout the results in your output folder! Or if you couldn't run them locally I have some here -</p> <p></p>  Benchmarking fused operators functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As can be seen the naive version, which is just successive function calls to linear, ReLU and softmax operators is massively outperformed by the fused linear-relu-softmax operators, with the fissioned version with bias outside of the matrix-matrix loop winning out. Of course the linear-relu versions are the fastest, as they don't do softmax, but between the two of them it is again the version without the bias calculation at the end of the matrix-matrix loop which wins ever so slightly.</p> <p>It's hard to make a general conclusion based on that, without going a lot deeper, but in any case, you should always test and measure! Now, you can either continue on reading the 3\ufe0f\u20e3 material or go to the next section to get a general introduction to GPUs. We will be using the GPU on your laptop, with no CUDA involved, to see if we can make this even faster.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#3-testing-operators","title":"3\ufe0f\u20e3 Testing Operators","text":"<p>All of the operators have been tested to be numerically equivalent. Rust has some nice test facilities to help with this. If you look at the file next to <code>tensor2d.rs</code>, called <code>tensor2d_test.rs</code>, you can see a bunch of tests. This used to be in <code>tensor2d.rs</code>, but I moved it to this file during a clean up, because the file was getting massive.</p> <p>The generic way to run these tests is writing <code>cargo test</code> from the same spot you would otherwise write <code>cargo run --release</code>. This works just fine for all of the CPU based tests. The testing system launches lots of tests in parallel to speed up the amount of time it takes to run the tests. As we will see later, this parallel test launch can create some issues when testing our GPU functions.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#3-graphs-in-graphicsgpu-programming","title":"\ud83e\uddec3\ufe0f\u20e3 Graphs in Graphics/GPU Programming","text":"<p>Computational graphs are even making their way into the way you can program the GPU! Ways to define computational graphs have been added to DirectX12 and Vulkan. This development seems to be lead by game and graphics workloads being increasingly compute shader driven.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/","title":"2\ufe0f\u20e3 Intro to GPU's","text":"<p>Now, I'll just give you a quick introduction to GPU's as the next section is about immediate mode GPU computation.</p> <p>GPU's are fairly ubiquitous at this point. They started off as purely for graphics, but around 2008, enough researchers had tinkered with workarounds to use them for general computing, that Nvidia put out CUDA, opening up GPU's for more general usage. GPU's still do lots of graphics, but the opaque black box parts are increasingly opened and even graphics API's such as OpenGL, Vulkan, Metal and DirectX have opened up. With modern graphics API's you don't even necessarily need a graphics output to use them. You can just use the pure compute capabilities. This guide won't get into graphics, except for the graphics specialization.</p> <p>Ok, so anyways, GPU's are pretty hot stuff right now as the software support becomes deeper and deeper, the hardware increasingly has hardware support for neural network specific operations and ChatGPT has increased the hype and demand for AI to exasperating levels.</p> <p>You can think of the GPU as an expansion of the memory hierarchies we have been examining earlier. It is not running in lock step, and you have to program more things explicitly, while also changing your mindset about how programming works. Memory transfers to and from the CPU and GPU will be relatively explicit, you have explicit control of a part of the L1 cache, you have start programming in a warp oriented fashion and if-statements become quite dangerous.</p> <p>If the CPU, with its numerous cores is like a team of highly skilled specialists building a car, sure, they can build an amazing car, they can adapt to changing circumstances quite well, they can act independently, then the GPU is like a factory. Each path and process has to be carefully optimized, they might each only deal with a very small piece each and people have to work in lockstep. But. Their throughput is unmatched.</p> <p>At 3\ufe0f\u20e3 I will go into more detail as to how to actually write GPU code, but the guide is set up using Rust and a GPU API abstraction layer called wgpu. You don't need to understand how it works right now, but it means that you should be able to run all code, including GPU code, on your platform, even if it's made by Apple or AMD.</p> <p>In general, I will be using terminology specific to the compute part of the graphics API's and I will keep to <code>wgpu</code> and <code>wgsl</code> terminology. You might see significant differences in terminology if you follow up this guide with some <code>CUDA</code> programming.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-hardware","title":"GPU Hardware","text":"<p>First off, when dealing with the GPU, you will have to manipulate the GPU from the CPU with commands like \"allocate this much memory\", \"transfer this memory from the CPU to GPU\", \"execute this shader/kernel\" and \"synchronize\". These are all done in whatever language you are writing in on the CPU side, except for the actual program the GPU has to run. This is distinct from the GPU API, some GPU API's even accept shaders written in multiple shading languages, as they can either be transpiled (translated) from one language to another, or they can be compiled to an intermediate representation, such as SPIR-V, which they can then ingest.</p> <p>But once we have built up all of these commands, at least if they are non-blocking, as in the CPU program won't advance until the command has completed, we have to actually submit them to the GPU. We do this with a synchronization. The commands may/may not have already been submitted, but if you call a synchronization function, the CPU-side code will block and wait until any and all submitted commands have executed on the GPU and the GPU sends the all-clear signal in return. Imagine you are at a horse track. You have to give instructions to a jockey on a race horse. You stand on the periphery of the big oval race track. You tell the jockey to make some adjustment and do a lap. The horse first has to accelerate and then once it nears you, it slows down and you can talk again. What would be more efficient was, if you could leave notes for the jockey to pick up whenever he was coming around and the horse could just continue at speed. In some API's the GPU can just be set in motion and then whenever you have a change to the loop it is running, adjust or change. Or you can set work in motion and come back at a later time, checking whether the work might be done.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#transfer","title":"Transfer","text":"<p>When transferring memory, you should have the following model in mind, nothing gets transferred without a staging area. When transferring from CPU to GPU, at least in the CUDA programming model, it will pin an area in memory. That memory won't be movable until it is unpinned. You basically transfer some memory from say, a vector you want transferred to the GPU, to this pinned memory staging area. That pinned memory area means the GPU can work in peace without interruptions. In CUDA, if you don't explicitly do it, CUDA will create a pinned memory area and do it for you. If you do it yourself and optimize this process you are likely to see around 2x improvement in transfer speed. The same thing happens on the GPU, a staging area visible from the CPU is where the transferred memory is stored, and then moved from the controlled area to the rest of GPU memory, where the GPU is free to do what it wants with it, without interruptions and guarantees.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#threads-warps-and-blocks","title":"Threads, Warps and Blocks","text":"<p>Threads are sort of like a CPU core, except a CPU core is a physical entity, whereas a thread is more like a set of variables (think back to the stack and function calls) which is following its own set of instructions. Thread 1 could be running program A with various states in registers and local variables X. It makes a call to something expensive, like a cache-missing memory access. While waiting, thread 1 is swapped for thread 2. Its state is of course saved, but thread 2's program B and state Y are swapped in for it to do some work. This keeps the CPU core itself occupied with work.</p> <p>Threads on a GPU, will usually be executing the SAME program, unless several calls are overlapped, but let's just focus on you having called a single operation. In that case all of your threads will launch, running the same program. They might however, go down different branches (think if-statements!), but this more expensive on the GPU and CPU, and should in general be avoided as much as possible. Each thread will have its own local variables. Threads on a GPU are launched in groups. Depending on the platform and the API they will be called something different. In wgpu, which is what we will be using, it is called a workgroup, while in CUDA terminology it is called a warp. On Nvidia GPU's it will be at most 32 threads per workgroup and on AMD it will be at most 64 threads. The \"at most\" might seem a bit weird, but there is something called register pressure. All of the execution units that can run those 32 or 64 threads at the same time, share a lot of the same physical memory, so if your program uses lots of memory, you might have to decrease the amount of threads to have enough memory to run your program.</p> <p>Anyways. Once you decided to write a matrix-matrix multiplication shader, you need to figure out which threads are gonna go where. In that case, I would begin by launching 1 thread for every output element.</p> <p>When programming for a GPU you have some maximum amount of threads you can launch. This is usually defined in three dimensions. Yes! You can define these threads in three dimensions. It doesn't actually have much of an effect, but it makes sense to tailor how you launch threads to your problem area. If you are performing image processing or matrix multiplication, by all means, launch a 2D grid. If you are summing an abitrary list of numbers, a single dimension will probably suffice.</p> <p>So, we should launch a 2D grid, matching the output elements of our problem. Next up, how do know which thread does what work? Each thread will usually begin its program by asking built-in variables, which thread it is. This can be which thread it is within its own workgroup, or it could be globally. Once it knows that, it should usually check whether it is within legal bounds of the problem. We almost always want n^2 threads in our workgroup, and it wouldn't be very flexible if the problem size always had to match exactly. So usually, you should launch too many threads and then have an if-statement following the thread ID calculation. If within acceptable range, do work, otherwise, don't do work.</p> <p>It cannot be assumed that all work groups are running concurrently. The GPU might need to launch waves of work groups because there aren't enough physical execution units. As such, we can only synchronize between threads inside the warp.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-memory-hierarchy","title":"GPU Memory Hierarchy","text":"<p>The memory hierarchy on a GPU looks a lot like the memory hierarchy on the CPU. Here it is exemplified by the Nvidia H100, which is a very expensive data center GPU and most definitely not the one residing in your laptop. But the bandwidth (how much data per second can be transferred) internally on the card is a lot higher than on the CPU. All of the streaming multiprocessors share the L2 cache and each streaming multiprocessor shares an L1 cache. On Nvidia GPU's the streaming multiprocessor is a number of, in this case 4, units which can each execute a work group, or in Nvidia terminology, a warp.</p> <p></p>  The layout of a H100 GPU. Note that connectivity to the memory (HBM3) is on the left and right sides.  Image credit  <p>Take some time to study these two diagrams and think about how data moves first from the CPU, to the GPU's main memory, then to the L2 cache, then to the streaming multiprocessor which needs its L1 cache until it finally is loaded up into the registers of the 32x4 threads executing on different, but adjacent, segments of the same data.</p> <p></p>  The layout of a single Streaming Multiprocessor. It can execute 4 work groups or warps at a time.  Image credit  <p>The threads accumulate their data into their own registers until they are done and write the result to main memory. The CPU waits for the GPU to be finished, until the GPU is, transfers to the CPU and signals that it is finished.</p> <p>It's not always as clear cut, though. If you are using a laptop, you probably have an integrated graphics card. The CPU and GPU coexist and share the same memory. There may be sections where there is higher bandwidth than just normal CPU-based memory, but overall the integrated GPU has access to the same memory the CPU has. This makes for faster transfers, but probably slower overall computation. This has become quite useful recently with most consumer grade GPU's having around 8 GB of memory and locally run neural networks like diffusion models easily being able to use more than that. A desktop GPU with more than 16GB of RAM would probably still outperform an integrated graphics card with 16GB of RAM available, but it would be very expensive.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-introducing-wgpu-and-wgsl","title":"3\ufe0f\u20e3 Introducing wgpu and wgsl","text":"<p>The guide will for all GPU purposes make use of the graphics library wgpu, but only the compute parts. wgpu is based on the WebGPU spec, which is supposed to be the new web GPU API, as well as not being particularly creative with their naming, the actual support in browsers for WebGPU is nascent. Chrome supports if you fiddle with some settings, but for most systems, especially if you aren't actually running in a browser, wgpu will default to using different, more powerful backends. For example, at the time of writing this, I am using an HP laptop, with an Intel integrated graphics card running Windows 10. Whenver I run a program with wgpu, wgpu tells me it has chosen Vulkan as my current backend. We could of course just write Vulkan, but it would be a bit more complicated, as Vulkan is slightly more low-level than wgpu, but it would also be more powerful. But attaining ultimate performance isn't the purpose of the guide. It's to get as many people as possible started as soon as possible. It has to run on an Apple computer and it has to be easy to install. So, wgpu it is. While any API which has to cover as many platforms as wgpu does will usually be hampered by the lowest common denominator, it is possible to query wgpu for hardware support for various features, such as fp16. While wgpu is still quite new, it has some exciting features on the way, such as a hardware accelerated ray tracing extension.</p> <p>The default shading language (the language you use to write the code the GPU will run) is wgsl, which was defined along with the WebGPU specification. It is possible to use other shading languages, such as glsl and hlsl, which also have more info and general documentation, but because of the increased code complexity in building the files to SPIR-V and then ingesting them, I elected to just use what was simplest.</p> <p>We can add wgpu to a project by going into the <code>Cargo.toml</code> file in the root directory, and under <code>[dependencies]</code> write the line <code>wgpu = \"*\"</code>. It will pull down the latest version of wgpu. You can of course also get a specific version of it, such as <code>wgpu = \"0.16.3\"</code>.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-basic-gpu-programming","title":"3\ufe0f\u20e3 Basic GPU Programming","text":"<p>GPU programming, as has previously been mentioned, has two major elements. Host (CPU) code and device (GPU) code. We'll start off with the basics of the host code and then move on the GPU code. Just enough for you to be able to read the following sections and understand what is going on in this entire module, as it doesn't go into the finer details of GPU programming, but is centered around a GPU-oriented paradigm.</p> <p>The rest of this section will be make use of the code location at <code>m1_memory_hierarchies/code/gpu_add/</code> or online. Make sure to go and actually read the code. It is full of comments! And they're made just for you! If you want to learn more about wgpu you can visit Learn Wgpu.</p> <p>Be sure to read through the code! Do this before you read the rest of this section, which will go into greater detail.</p> <p>Starting in the <code>main</code> function, first we initialize the environment logger with <code>env_logger::init()</code>. This will get us more helpful feedback from wgpu. This should only happen once in your code, so by putting it as the very first line, we should be sure that it shouldn't need to happen anywhere else.</p> <p>Next up, we call <code>pollster::block_on(self_test())</code>. The <code>self_test</code> function, is a function I made, and use elsewhere to make sure your system is compatible and to print the system info so you can see what GPU is being found and what backend is being used. <code>pollster::block_on</code> allows us to call asynchronous code from a normal function. If you don't remember what asynchronous means, just think of it as being non-blocking. Meaning, we can launch an asynchronous function and just continue on to the next line of code. But the way we do this is different depending on whether we are inside a normal function or an <code>async</code> function. An <code>async</code> function definition example - <code>pub async fn self_test() -&gt; bool {</code>.</p> <p>If we are in a normal function and we call an <code>async</code> function, we have to wait for it to complete. As in, block on the function call, which is of course <code>pollster::block_on()</code>. Inside the <code>async</code> function itself it can either block on async function calls by using <code>await</code> - such as <code>let result = async_function().await;</code> or you can store what is known as a future. We could set in motion the loading of a number of files, and then once we were done and actually genuinely NEEDED to use the files for something, <code>await</code> on the future. The <code>async</code> function, when called from a normal function also returns a future, but we can't use <code>.await</code> on it.</p> <pre><code>pub async fn load_four_files(path_a: &amp;str, path_b: &amp;str, path_c: &amp;str, path_d: &amp;str) -&gt; (File, File, File, File) {\nlet file_future_a = load_file_async(path_a);\nlet file_future_b = load_file_async(path_b);\nlet file_future_c = load_file_async(path_c);\nlet file_future_d = load_file_async(path_d);\nlet file_a = file_future_a.await; // Block on the future\nlet file_b = file_future_b.await;\nlet file_c = file_future_c.await;\nlet file_d = file_future_d.await;\n(file_a, file_b, file_c, file_d)\n}\n</code></pre> <p>Ok, so why do we need <code>async</code> when dealing with the GPU? In some cases, we don't care about synchronization. We just want to keep transferring data to the GPU as fast as we can get it, the GPU might output to the display or we might get some data transferred back, but if we are doing this in a real-time setting, we might not care to synchronize, as in block, and we just need things when they are ready. Anything to do with gpu - <code>async</code> will be involved. At least in Rust.</p> <p>Let's move on. We set up our CPU-side data. This is a simple vector addition, and I elected to make the data in a way that was easily verifiable as correct for humans. Input A and B are just vectors of 32-bit floats with values equal to their index. The correct result in the output vector should of course be double the index value then.</p> <p>Finally, we call <code>initialize_gpu()</code> and block on it. Let's go into that function!</p> <p>First we get an <code>Instance</code>. The <code>Instance</code> is a wgpu context which we will use to get <code>Adapter</code> and <code>Surface</code>. The <code>Adapter</code> corresponds to your GPU. We specifically request the adapter with high performance. If you are on a system with more than one GPU, such as a laptop with an integrated GPU, which shares memory with the CPU and a more powerful dedicated GPU, it should try to get access to the dedicated GPU. We also request <code>None</code> for <code>compatible_surface</code>. Surfaces are what you would render to if you were doing graphics. Think of an image with extra steps, which you could show on your display. If we don't need to do graphics, not having one is less work. It also means we can run on data center GPU's, which might not even have a display port. So we just get the <code>Adapter</code>. We use the <code>Adapter</code> to get <code>Device</code>, which will be our handle to the GPU from now on. Whereas the <code>Adapter</code> is more of a raw connection, which we can't do much with. The <code>Device</code> is a handle that has some guaranteed features. The <code>Adapter</code> tells us what features we can get. Once those features are guaranteed, it is much easier for wgpu to open up for more functionality with the <code>Device</code>. We actually don't need the <code>Adapter</code> after we get the device, but I keep it around in the GPUHandles for you to tinker around with auto-complete to see what it can do. We do need the <code>Device</code> though. We also need the <code>Queue</code>. The <code>Queue</code> is where we can submit the work we want the GPU to do.</p> <p>Note that when defining our <code>DeviceDescriptor</code> for getting a <code>Device</code> that lives up to our needs our current requested <code>features</code> is <code>wgpu::Features::empty()</code>. We just want the absolute basics. But we could request, or at least see whether we could get them, features like 16-bit floating point support.</p> <p>Now back to the <code>main</code> function!</p> <p>We now have our bundled GPU-related handles. Now we calculate how many threads we need to launch for our problem size. <code>let element_count: usize = 100;</code>, so we need to launch AT LEAST 100 threads if each thread only processes one element of our problem. Which it does, in our simplified case. Given that we would like to fill up our work groups, I have elected to use 32 threads per work group. <code>let block_size: usize = 32;</code>. Given that the register pressure is likely very low for our shader, this should be no problem. Finally, we calculate how many blocks to launch. This simple calculation is found all of the place when doing GPGPU programming. <code>let launch_blocks: u32 = ((element_count + block_size - 1) / block_size) as u32;</code>. The basic premise is that we add one element less than the full work group size and then use integer division to make sure we always have at least as many threads as we need. In the worst case of a work group size of 32, we will have a work group at the very end of the vectors with 31 idle threads.</p> <p>Next up, we compile our shader code <code>add_vectors.wgsl</code> with <code>.create_shader_module()</code>. Compiling shaders is quite expensive, so if you are programming a bigger system than this, you might want to save the compiled code or do it as part of your build step. When running you can load it from disk as needed. Once we have compiled our shader code we can create a compute pipeline with a specific entry point. The entry point is just the function that will be called when dispatching the shader call later on. Once we have a <code>ComputePipeline</code> we can begin doing our bind group layouts. In CUDA you can pass device side pointers to your CUDA functions when dispatching. Or phrased differently, when using CUDA you can pass along memory addresses for buffers you have explicitly allocated on the GPU. When using the graphics APIs the most basic thing to do, if you are not going bindless, which is... well, don't worry about it, is to use bindings. There is a certain amount of bind slots available in a shader depending on the API and perhaps the system. What can be a bit tricky is the binding slot you declare on the CPU for buffer X, has to match the exact binding slot in the shader. E.g. if you bound your input buffer to binding slot 0 on the CPU, it has to be bound to binding slot 0 in your shader code. Additionally, the compiler will complain if you don't use that buffer in the shader. Finally, you can have multiple sets of bindings in the same shader. These are called bind groups and each has N binding slots.</p> <p>When I created the <code>GPUVector</code>s earlier, the <code>new</code> function allocated a storage buffer, which is visible to the shader and transferred the contents of the given vector to the GPU. This can be done more effectively, but it's a nice and easy way to start things off. We don't have to keep track of whether we remembered to transfer our data to the GPU or not, which makes sure we don't use initialized data. In the case of the output vector, we have also allocated a <code>staging_buffer</code> to more explicitly transfer data back to the CPU. This <code>Buffer</code> has also been flagged as readable from the CPU.</p> <p>The <code>storage_buffer</code>s we have created, when creating the <code>GPUVector</code>s from before, can be bound. I add these binding references to a vector and send them to a convenience function <code>create_bind_group()</code>, which binds the array of bindings in order. Do note that we don't specify what can and cannot be done at this step. It was specified at the creation of the <code>storage_buffer</code>s and it will be specificed locally in the binding of the buffers in the shader.</p> <p>Once we have our bindings set up, we create a <code>CommandEncoder</code>, which we get from <code>Device</code>. The command encoder is a buffer of commands. We can add stuff like render and compute operations, they are sort of like a collection of operations and state, and transfer operations. The command encoder needs to be finished, before it is submitted to the queue. Remember the <code>Queue</code> we got earlier? This is what it was for. We submit finished <code>CommandEncoder</code>s to our <code>Queue</code>, which submits the jobs to the GPU. For this specific program we add two commands to the <code>CommandEncoder</code>. We dispatch our compute shader, enclosed in a <code>ComputePass</code> and launch the appropriate number of threads. Note also the <code>label</code> field. This field permeates wgpu usage. It is mostly for debugging. It helps us identify what is causing an issue. Once we have finished our <code>ComputePass</code>, due to it going out of scope, we add a transfer operation. We use the <code>staging_buffer</code> on our <code>output</code> vector, to read the output back to the CPU. Then we finish our <code>CommandEncoder</code> and submit it to the <code>Queue</code>.</p> <p>We then setup a <code>oneshot_channel</code>. Don't worry too much about this. It is a connection which can only be used for sending data once. We map the <code>staging_buffer</code> and send its data using the sender/receiver pair. Once we have done this <code>map_async</code> call, we wait for the GPU to be finish all operations currently in its queue. Once it has finished we block on the receiver. Until the receiver sends the <code>Ok</code> signal, we wait. Once we get it we retrieve the data. This is raw data in bytes, <code>u8</code>, which we recast to the type we know it is, which in this case is <code>f32</code>. We do a bit of clean up, and don't you know it, that's the program!</p> <p></p>  Adding our two vectors, it should be easily verifiable that it is correct.  <p>Maybe now might be a good time to go back to the code and try to run through it again.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-remove-the-loop-where-you-say","title":"3\ufe0f\u20e3 Remove the loop where, you say?","text":"<p>When writing GPU programs, you should usually start writing a CPU-based version. Once that works, you have something to verify your GPU program against. Often the part of your program that you want to offload to the GPU, will have loops. For example, in a vector addition snippet you might have -</p> <pre><code>for index in 0..ouput.len() {\noutput[index] = input_a[index] + input_b[index];\n}\n</code></pre> <p>When transferring your program to a GPU shader, as a way to get comfortable with thinking about this sort of parallelism, you should start with writing a single threaded version on the GPU. You can do this by dispatching a single thread <code>cpass.dispatch_workgroups(1, 1, 1);</code>. It WILL be slower than the CPU version, but it allows you to get all of the transfers and synchronizations out of the way first. Once you have done that, and you have verified that it works, mind you, you can start adding, or rather removing dimensions. You do that by removing one of the for-loops in your code and replacing it with added dimensionality in your shader dispatch. So in your first version of your vector addition shader, it might look like this sketch (don't know if it compiles) -</p> <pre><code>@compute @workgroup_size(32, 1, 1) fn main(\n@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n) {\nlet thread_id: u32 = global_id.x;\nif (thread_id &lt; 1) {\nfor (var index: u32 = 0u; index &lt; dimensions.element_count; index += 1u) { output[index] = input_a[index] + input_b[index];\n}\n}\n}\n</code></pre> <p>When that works, you can begin thinking about how to remove that pesky loop. You do that by removing a dimension in your shader, but adding one in your dispatch and then making accomodations in your shader. We can take that and transform it by instead dispatching more 1D threads: <code>cpass.dispatch_workgroups(launch_blocks, 1, 1);</code>. Then we change the shader to have each thread work on a single element -</p> <pre><code>@compute @workgroup_size(32, 1, 1) fn main(\n@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n) {\nlet thread_id: u32 = global_id.x;\nif (thread_id &lt; dimensions.element_count) {\noutput[thread_id] = input_a[thread_id] + input_b[thread_id];        }\n}\n</code></pre> <p>If there had been more dimensions we could have continued expanding and removing dimensionality. We can continue until the third dimension, usually you can launch less threads in the third dimension than in the first two. You also have to remember to check whether the thread is outside of the valid range for each dimension. You should always look up your graphics cards and GPU API to see how many threads you can launch. You might have to break it into several passes. It's not actually quite this simple, as, well you remember how we learned stride had a negative impact on performance earlier? Well, that is not quite the same on GPU's.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-coalesced-accessing-and-strides","title":"3\ufe0f\u20e3 Coalesced Accessing and Strides","text":"<p>Because of the way threads and work groups share memory on a GPU, and each thread executing the same line of code at the same time, if thread A calls for memory at indices 0, 1, 2, 3 and thread B, which is right next to it in the same work group, calls for indices 4, 5, 6, 7, they will be asking for two different cache lines at the same time. Imagine the whole work group doing this at the same time. They will all be waiting, while requesting different cache lines. What is normally faster, is if, given a work group size of 32, thread A calls for indices 0, 32, 64 and 96, with thread B calling for indices 1, 33, 65 and 97. This allows for the work group to call for a minimum of cache lines in lock step and each getting a piece of the cache line. This is called coalesced accessing and if you ever say that to a GPGPU programmer, you will see a faint smile on their face. Think of a jigsaw puzzle, where the pieces are slowly being adjusted. Eventually, they all snap into place. All of the pieces fit exactly right.</p> <p>Here's a small example, if we for some reason were intent on turning our vector addition shader into 2D matrix addition, but we were deadset on keeping the thread grid for our dispatch one dimensional we could do something like this -</p> <pre><code>const BLOCK_SIZE: u32 = 32u;\n@compute @workgroup_size(32, 1, 1) fn main(\n@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n) {\nlet thread_id: u32 = global_id.x;\nif (thread_id &lt; dimensions.first_dimension_count) {\nfor (\nvar index: u32 = thread_id; index &lt; dimensions.second_dimension_count; index += BLOCK_SIZE\n) { output[index] = input_a[index] + input_b[index];\n}\n}\n}\n</code></pre> <p>Again, not verified/compiled code. But hold on for a second! We have to remember that there are other work groups too. We can't necessarily just stride through the single dimension in the same way. We would be reprocessing elements that had already been processed by a different work group. What we could do instead would be to step along the rows instead.</p> <pre><code>@compute @workgroup_size(32, 1, 1) fn main(\n@builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n) {\nlet thread_id: u32 = global_id.x;\nif (thread_id &lt; dimensions.first_dimension_count) {\nfor (\nvar index: u32 = thread_id; index &lt; dimensions.second_dimension_count; index += dimensions.first_dimension_count\n) { output[index] = input_a[index] + input_b[index];\n}\n}\n}\n</code></pre> <p>In other cases, using a stride of the work group size can work as well. In this case, stepping along the rows made better sense, but keep thinking in these terms, implement different versions and test them! It's the only way to be sure! Once you have made a couple of different versions and done simple timing you can always add in a profiler, m4 has got you covered!</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-divergence-overlap-and-occupancy","title":"3\ufe0f\u20e3 Divergence, Overlap and Occupancy","text":"<p>One statement I tried to sweep under the rug in the last section was - \"each thread in a work group executes in lock step\". It is highly desirable for a work group to have each thread executing in lock step. That is each thread is executing the same line in your program. If you have branches, like if-statements, some threads might execute path A and some threads might execute path B. This will lead to divergence. Divergence will result in the group of threads A executing while group of threads B will wait until A is finished, and then executed. Finally, they might join again.</p> <p></p>  An if-statement causes a work group to diverge into two.  Image credit  <p>As you can imagine, this expands the timeline of executing the code compared to a non-diverging execution. But if you were within a workgroup where all threads take the same branch there wouldn't be an issue. Thankfully, recent hardware takes less of a performance hit when work groups diverge.</p> <p>Once you have most of your work groups not diverging, are you sure your threads aren't just sitting around waiting? Whenever a thread wants to load a piece of data all the way from memory, it can take quite a long while to retrieve. If however, you have dispatched enough work, the threads waiting around for memory can be swapped out for another work group which might be able to do some work, once this work group has time for a nap, like when it is also requesting data from memory, the first work group can be swapped back in, when the requested data has, hopefully, arrived. Without this overlap, GPU programs are likely to seem a lot slower than they need to be. If however, you launch a lot more threads than there are physical execution units, you are likely to see this overlap resulting in higher occupancy. The higher the occupancy the more time a physical execution unit spends on doing actual work and not just stalling until everything is ready. So you can either launch a lot of independent work, or use a lot of elements in your data. Like really big matrices!</p> <p>In machine learning terms, if you have pipelined and made your computational graph relatively independent, you might see a big increase in occupancy by using less layers and make the ones left very wide.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-shared-memory-and-synchronization","title":"3\ufe0f\u20e3 Shared Memory and Synchronization","text":"<p>Just two final pieces are missing before we go back to memory hierarchies. Shared memory and synchronization. GPU's have more programmable pieces of the memory hiearchy, such as sharing directly between threads, sharing between work groups and more, but WGSL just has the primitives for shared memory, which is the only one I will present for you. Shared memory is a programmable section of the L1 cache. If a cache miss, resulting in retrieving data all the way from memory costs 100's of cycles, quite often somewhere around 250-300, accessing data from shared memory costs around 10 cycles. This is very useful if each piece of data is accessed more than once. It could for example be overlaps in convolutions or storing preliminary results in shared memory for the work group to finally reduce the results internally in the workgroup, before one of the threads writes the final result to global memory.</p> <p>Typically using shared memory, you will first see a section where each thread loads one or more pieces of data into shared memory, followed by a synchronization primitive. This synchronization primitive is available in wgsl and is called <code>workgroupBarrier();</code>. It is available in most shader languages, although it will likely be named something else. It is a barrier ensuring that all threads in the workgroup will stall and wait until each thread has signalled that it is ready to proceed. This is very handy when you are loading data into shared memory for reuse between the threads. A small example snippet -</p> <pre><code>var&lt;workgroup&gt; shared_data: array&lt;f32, BLOCK_SIZE&gt;;\n@compute @workgroup_size(32, 1, 1) fn workgroup_phase(\n@builtin(workgroup_id) group_id: vec3&lt;u32&gt;, @builtin(local_invocation_id) local_id: vec3&lt;u32&gt;,\n) {\nvar tid: u32 = local_id.x;\nif (group_id.x == 0u) {\n// In this first section we can use all 32 threads\nvar elements_left: u32 = sum_uniform.block_count;\nvar i: u32 = tid;\nvar sum_value: f32 = 0.0;\n// How do we handle the odd case?\nwhile (BLOCK_SIZE &lt; elements_left) {\nsum_value += data[i];\nelements_left -= BLOCK_SIZE;\ni += BLOCK_SIZE;\n}\nif (0u &lt; elements_left) {\nif(tid &lt; elements_left) {\nsum_value += data[i];\n}\n}\nshared_data[tid] = sum_value;\nworkgroupBarrier();\n}\n</code></pre> <p>As usual, this code isn't very well tested and there might be some cases where it isn't fully functional, but you can see the primitives for declaring shared memory, accessing it and synchronizing. Now back to the memory hierarchies!</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#5-further-reading","title":"5\ufe0f\u20e3 Further Reading","text":"<p>The GPU Memory Hierarchy, GPU Memory Hierarchy, GPU Programming, Hopper Architecture In-Depth and GPU architecture and CUDA Programming. The last entry is highly recommended.</p> <p>A slightly more detailed explanation of asynchronous memory transfers for GPUs.</p> <p>If you want to learn more about wgpu, this is the most used tutorial - Learn Wgpu.</p> <p>To learn more about optimzing shaders with shared memory.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/","title":"2\ufe0f\u20e3 Immediate GPU computation","text":"<p>Now let's look at speeding up our operators with GPU, after this we will take a look at building an actual computational graph, both on the CPU and the GPU.</p> <p>The first version of our GPU implementation, will be for immediate mode computation. In this version the behavior will be treated a bit like executing a Python script. Every command will be processed one at a time, no optimizations can be executed outside of a single operation, and all data has to be ready for a new and completely different operation to be executed afterwards. The result of this being that if the system is told to execute a linear operator on some data, it will compile the code needed to run on the GPU, allocate all necessary buffers on the GPU, transfer the needed data, execute the operation, synchronize with the CPU and then transfer back all of the necessary data to the CPU. If that linear operator is then followed by a ReLU operator, it will have to do the whole thing over again. Compile the ReLU code for the GPU, allocate buffers, transfer, execute, transfer back. Then possibly deallocate.</p> <p>This is not a good way to accomplish this. But it is highly flexible and we're gonna do it anyways! Don't worry about the 'how' too much, 3\ufe0f\u20e3 goes into greater detail.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-linear-node","title":"Building the Linear Node","text":"<p>Okay, so let's try building the linear operator again, but this time on the GPU! Don't worry too much about the particulars. The setup is quite a bit like what is described in the 3\ufe0f\u20e3 section of m1::s2.</p> <p>There are three central files for this. <code>src::shared::tensor2d_gpu.rs</code>, <code>src::shared::shaders::linear_layer.wgsl</code> and <code>src::immediate::nodes.rs</code>. If you don't have them locally you can them here, here and here respectively.</p> <p>First of all, let's go directly to the shader (GPU) code in <code>linear_layer.wgsl</code>. The version of the two functions we are interested in is <code>main</code>. At the top there is a struct called <code>TensorDimensions</code>, it is bound as something called a <code>uniform</code>. The <code>uniform</code> is a struct, it can also just be a single value, which is read only for the duration of our shader. As such all threads can safely keep it entirely in their registers or in whichever cache they have room, without fear of the data getting stale. It also means that each thread will be accessing the same data, and not just overlapping data. But let's stay with the definition. We now have the <code>group</code>, which is 0. A binding group is a set of bindings. There are a limited amount of binding slots available so you could have one set of buffers bound for group 0 and another set of buffers bound for group 1. In the <code>var</code> declaration, you declare how the bound data is used. Uniform is what I just wrote, you don't need to specify whether it is <code>read</code> or <code>read_write</code>, it is always just <code>read</code>. For the more general <code>storage</code>, which is your normal buffer, you can specify whether it is <code>read</code> or <code>read_write</code>. Whether there is a performance impact for one or the other might depend on your system, but the <code>wgsl</code> compiler is likely to complain if you declare <code>var&lt;storage, read_write&gt;</code> without actually writing to it. That's not just a performance thing, that is also good software engineering. Don't give stuff more rights than it needs to. That allows the compiler to give you helpful reminders that, at the very least writing to this buffer wasn't your original declared intention. We have several instances of <code>array&lt;f32&gt;</code> which is the raw data from our <code>Tensor2D</code>s. You can reconstruct the entire <code>Tensor2D</code> by combining this data with the dimension sizes from the <code>TensorDimension</code> struct.</p> <p>So now we get to the code itself. For the matrix-matrix multiplication, the dimensionality of the problem lends itself to a two dimensional solution. So above the <code>main</code> function, which can be named anything by the way, we define that our work group will have a size of 8 on the x-axis and 8 on the y-axis. This changes the way our threads individual ID's are computed. Don't worry about it!</p> <p>Then we define a number of built-in variables we would like to have access too. There are more available than we declare here. These are various ID's such as, what is this threads number inside this work group, which work group is this thread in, out of all threads which number thread is this thread, that sort of thing. These are <code>vec3&lt;u32&gt;</code> because we actually dispatch our GPU programs with three dimensional grids of threads. In the case where we only use a single dimension, such as a sum reduction shader where we don't care about dimensionality, we still launch it three dimensionally, we just set the two last dimensions to one and then ignore them from then on.</p> <p>Next, the thread will calculate the correspondence between its global ID and the element of the output matrix this specific thread will calculate. Then we check whether these specific indices are outside the valid range of indices, remember that we often launch more threads than we need. Then we calculate the linearized index (multiple dimensions collapsed into one) in our one dimensional output array. We declare and initialize a local variable to hold our running result, which will hopefully ensure that we keep it in a register. Then we just loop through the input row and weight column, multiplying and adding, until we have accumulated our result, add our bias and then store the result in the output tensor.</p> <p>Given the immediate mode usage, we will always be paying for a full transfer to and from the GPU. This implementation of the linear operator is quite suboptimal, it has been left as an optional 4\ufe0f\u20e3 exercise to implement a more optimal version using tiling and shared memory.</p> <p>If I get the time, at some point I might put up a performance benchmark comparing it to the CPU, but I have one later on for the more complex case of handling arbitrary graphs, which is a bit more representative of the real use case. Just know that immediate mode is highly suboptimal.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-relu","title":"Building ReLU","text":"<p>We then implement ReLU, Softmax and the fused operators in the same way. ReLU you can just check out yourself in <code>shaders::relu.wgsl</code> or online along with an inline implementation in <code>shaders::relu_inline.wgsl</code> or here .</p> <p></p>  Benchmarking ReLU operators on the GPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Interestingly, when done this way, the inline version is significantly slower than the normal, more functional, version.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-softmax","title":"Building Softmax","text":"<p>Next up, we have the softmax operator. You will find the three shaders needed for the softmax operator in <code>shaders::softmax.wgsl</code> or online . In this case, finding and communicating the maximum value and the sum is a lot more complicated on a GPU. The implementation provided is not even using all the possible threads, but just a single work group to make the code more readable. Implementing a tree reduction with iterative calls to max and sum shaders is left as a 4\ufe0f\u20e3 exercise. So you don't need to know what that is right now, just know that it is not just implemented suboptimally, but even without more than 32 threads. I did however cheat a little bit and use shared memory, to make it a bit faster. Don't worry about shared memory, I will introduce it in 3\ufe0f\u20e3.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-fused-operators","title":"Building Fused Operators","text":"<p>Finally, the fused operators are basically implemented through doing a single transfer to and from, and calling the kernels and bindings in succession. This is done CPU-side and there are no unique shaders for them. Again, don't worry about the stuff happening CPU side. Just know that it is implemented slighty suboptimally, and these shaders aren't implemented optimally.</p> <p>The only really interesting of the performance benchmarks, as we don't have many different implementations of each operator is the fused ones.</p> <p></p>  Benchmarking fused operators on the GPU. If there is no underscore between the operators they were fused. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, using fused operators, especially when we are in immediate mode, save us a whole bunch of performance, having 2 transfers instead of 6 makes a big difference. The fully fused operator wins by a large margin. Now, let's start building some graphs!</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#3-setting-up-to-dispatch-multiple-shaders","title":"3\ufe0f\u20e3 Setting up to Dispatch Multiple Shaders","text":"<p>If you actually delve into how these immediate mode operators are implemented on the CPU side, go to <code>src::immediate::nodes.rs</code>. The code is almost the exact same as in the <code>add_gpu</code> example from <code>m1::s2</code>, except now we will dispatch several shaders in a row, sharing data. Especially the softmax and fused operators will be significantly different on the CPU (host) side.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#3-caching-shaders","title":"3\ufe0f\u20e3 Caching Shaders","text":"<p>One other thing that is fundamentally suboptimal about this, is that we compile the shaders for every operator, every time we use the operator. If you do lots of small matrix calls, this will incur a significant overhead. Shader compilation is expensive. What you could do instead is to cache your compiled shaders for reuse later on. This is done with the graph implementations, but it has been left as an optional 4\ufe0f\u20e3 exercise for you to implement this for immediate mode operations. This works just fine when you have 4-10 shaders to compile and keep track of, but what if you had in the 1000's of combinations? In that case you might need some form of cache eviction mechanism, such as LRU.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/","title":"2\ufe0f\u20e3 Building a Computational Graph","text":"<p>Ok, so now we have the basic building blocks ready. We also have a very simplifying set of constraints. This allows us to just represent this graph as a straight line. So we can just represent our graph as a series of nodes in a list. This list needs to be runnable on both the CPU and the GPU, so we'll look at how we can make a CPU graph runner and a GPU graph runner which can interpret the same list of commands and still work just fine. First we are going to do this on the CPU, then the GPU. Then we are going to run the GPU graph in a loop instead of reconstructing it with every iteration. This also the part where we really start to let loose, creating graphs of different sizes, creating arbitrary permutations of linear and ReLU nodes and seeing what the benchmarks can show us.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-the-cpu-graph","title":"Building the CPU Graph","text":"<p>The CPU version of the computational graph is found in <code>src::graph::nodes.rs</code>, <code>src::graph::graph_runner.rs</code>, <code>src::graph::graph_validation.rs</code> and <code>src::shared::graph_operators.rs</code>. First of all, a graph is given as a <code>Vec&lt;GraphOperator&gt;</code> whether it is headed for the CPU or the GPU. It can be found in <code>src::shared::graph_operators.rs</code>. <code>GraphOperator</code> is an enum and has all of the operators we are used to, <code>Linear</code>, <code>ReLU</code>, <code>Softmax</code>, <code>LinearReLUFused</code> and <code>LinearReLUSoftmaxFused</code>. Additionally, it has <code>HostToDevice</code> and <code>DeviceToHost</code>. But hold up, you might think, the CPU part of the code shouldn't do any transfers to device (GPU) or back, well, I'll get to that in a second. In benchmarking we will define this <code>Vec&lt;GraphOperator&gt;</code> the same for both CPU and GPU. For this to work, the list has to live up to the highest minimum requirements, which is the GPU in this case. Each graph runner (that's just a term I came up with, I don't know what the standard term is) has to interpret that list and translate it to its own set of instructions.</p> <p>In general, it is a way to sequester state by changing the type from layer to layer in your pipeline. Imagine a physics engine. Sure, you could just represent everything as a float of a high enough precision and \"just name our variables correctly\", which is once again a red flag as our systems should never rely on the absence of human errors, or we could create new types which makes sure that we don't mix up Newtons per second and meters per second. We can even ensure that only certain operations are available for a <code>NewtonsPerSecond</code> type. One of those operations could be a <code>GarbblediGook(&amp;self, meters_per,_second: &amp;MetersPerSecond) -&gt; NewtonMetersPerSecondPerSecond</code> function (sorry, I'm not a physicist), which would nudge us towards correct type usage. Getting back to our graph system, in much the same way, our <code>GraphRunner</code> found in <code>src::graph::graph_runner.rs</code> can take in a <code>Vec&lt;GraphOperator&gt;</code> and output a verified <code>Vec&lt;Node&gt;</code> which contains not only verified data and dimensions, but additional information. So what happens for the <code>HostToDevice</code> and <code>DeviceToHost</code> on the CPU is basically just dimension verification. Just keeping track of which buffer goes where. You can almost think of it as a passthrough operation. It is used when verifying the dimensions and when creating the buffers needed to run the graph, but at run time, encountering an <code>Input</code> or <code>Output</code> operator does nothing.</p> <pre><code>#[derive(Clone, Debug, Eq, Hash, PartialEq)]\npub enum NodeOperator {\nInput,\nOutput,\nTransfer,\nLinearLayer,\nReLU,\nSoftmax,\nLinearReLU,\nLinearReLUSoftmax,\n}\n#[derive(Debug)]\npub struct Node {\npub name: String,\npub operator: NodeOperator,\npub buffer_indices: Vec&lt;usize&gt;,\n}\n</code></pre> <p>In this case I decided, when first verifying and then processing the input vector to generate the requisite buffers, put them in a vector of buffers and for each <code>Node</code> to just carry around the indices of the buffers in our buffer list. This puts some restrictions on how dynamically we can treat our list of buffers. We can't just remove a buffer and move all of the succeeding buffers one slot to the left. <code>buffer_indices</code> is <code>Vec&lt;usize&gt;</code> because each <code>NodeOperator</code> type, has a different amount of buffers it needs to reference. Using indices like this also means we don't need any smart pointers and the transfer of data from one operator to the next is handled by having indices to the same buffer, as well as our graph being executed sequentially, so we don't need to worry about only one access happening to a buffer at a time as only one operator will be running at a time.</p> <p>Ok, so now we've rummaged around a little bit, try to go back to the files relevant to the CPU graph and see what they're doing. Don't worry about understanding the <code>sorted_mutable_references</code> function.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-the-gpu-graph","title":"Building the GPU Graph","text":"<p>Ok, so now let's take a look at how the GPU graph does almost the same. Verification, translation and allocation. Start by perusing the relevant files - <code>src::graph::nodes_gpu.rs</code> and <code>graph_runner_gpu.rs</code>. The validation is the exact same.</p> <p>Once again, the GPU graph runner, takes a vector of nodes, it validates the correctness, it translates the nodes to its own intermediate representation (see below), and allocates any and all buffers. It uses the indices to share data between operators.</p> <pre><code>#[derive(Clone, Debug, Eq, Hash, PartialEq)]\npub enum NodeOperatorGPU {\nHostToDevice,\nDeviceToHost,\nDeviceToDevice,\nLinearLayer,\nReLU,\nSoftmax,\nLinearReLU,\nLinearReLUSoftmax,\n}\n#[derive(Debug)]\npub struct NodeGPU {\npub name: String,\npub operator: NodeOperatorGPU,\npub buffer_indices: Vec&lt;usize&gt;,\n}\n</code></pre> <p>Note the <code>DeviceToDevice</code> operator is in there now. It is just a transfer of data from one operator to the next. It actually does nothing in itself, but it is used by operators to get the previous operators output and it outputs a <code>DeviceToDevice</code> itself to send its output to the next operator. This simplifies our handling of the inputs and outputs and makes the communication between operators explicit instead of depending on implicit communication. This isn't actually that much of an issue with this setup as we know there is only ever one preceding operator and none of the operators writes to more than a single buffer, but the second either of those cases were needed we would have to rewrite it or accept that we had to handle a lot of cases. The <code>DeviceToDevice</code> operator doesn't cost anything when running the graph and you can verify that the runner does nothing when it sees a <code>DeviceToDevice</code> operator.</p> <p>Finally, you can see that for every operator, the necessary bindings are made and commands are generated and enqueued in a command buffer. Once all operators have been added to the queue note that there is not a synchronization, but the commands are submitted to the queue. The synchronization does not happen until the result are retrieved.</p> <p>In the next section, s5, we will look at how a system could take the computational graph and aside from verifying and translating, actually optimize, given the knowledge of the graph. I will also run a benchmark so you can see the differences in our assortment of implementations.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#3-borrow-checking-a-graph","title":"3\ufe0f\u20e3 Borrow Checking a Graph?","text":"<p>Ok, so what if we actually wanted a more complex, and applicaple in real-world circumstances? We ironically need to deal with the issues that Rust enforces through the borrow checker. If we have one node writing to multiple other nodes, that is fine.</p> <p></p>  One node writing to multiple buffers is fine.  <p></p>  One node reading from multiple buffers is fine. The dashed line is a synchronization barrier. Nodes A and B can write to buffers, node C just has to be sure that nodes A and B are done before reading.  <p></p>  Multiple nodes writing to the same buffer is not fine.  <p>Multiple nodes writing to the same buffer is not correct, unless they either write only to specific sections, such as if concatenation takes place and node A might exclusively write to indices 0-16, with node B exclusively writing to indices 17-32. Either that or we have to use synchronization through mechanisms like atomics (more about that in m2) to ensure that the calculations are correct. This is essentially what the borrow checker enforces in Rust. You either hand off one part of memory to one writer and another part of memory to another writer (slice references) for exclusive access or you use the more expensive synchronization primitives to make the reads and writes to those elements sequential.</p> <p>Unfortunately, <code>wgpu</code> and <code>wgsl</code> don't have a borrow checker, and we have to do that analysis ourselves. Sometimes it can make sense to actually do this contentious writing to a shared buffer anyway, as the synchronized version can be expensive enough that absolute correctness might not be worth the cost. But if you feel you need to introduce this sort of incorrectness to your system, my recommendation is that you make absolutely sure that the incorrectness is confined to as small a pocket of your system as possible.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/","title":"2\ufe0f\u20e3 Building a Computational Graph Compiler","text":"<p>Ok, so we are almost done with this module. Let's put it all together and execute the graphs, except I have three more variations to explain before I present the benchmark graph.</p> <p>Compiling the shaders for each operator is actually quite expensive, so I put a flag on the GPU graph runner which if <code>true</code> will cache all of the compiled shaders for reuse later.</p> <p>Next up, we have the fused variant. This is our own little mini-compiler. This one is for both the CPU and the GPU graph runners. If <code>true</code> the graph runners will replace any instance of <code>Linear/ReLU/Softmax</code> or <code>Linear/ReLU</code> with a fused version. But as softmax can only ever be the final operation and a ReLU the one before the softmax, and thus there will always be at most a single instance of a fused <code>Linear/ReLU/Softmax</code> operator, we will mostly see the effect of <code>Linear/ReLU</code> fused operators. Remember, that for a network of depth N we have N - 2 operators that are randomly either a linear or ReLU operator.</p> <p>Finally, there is the graph loop variant. This variant is not just creating a computational graph, but moves the loop from the measuring function closer into the graph runner itself. One thing to note though is that while we do cut down on some transfers, the implementation is still suboptimal. I will elaborate about why in 3\ufe0f\u20e3. So just take this as an indicator of why you should use computational graphs and computational graph compilers when you can.</p> <p></p>  Benchmarking randomly generated graphs of depth 64 at various tensor sizes across 10 samples. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p></p>  Benchmarking randomly generated graphs of various depths and a size of 256 across 10 samples. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As we can see from the benchmarks, if you have small enough matrices, it can at some point be more efficient to just use the CPU. But the graph running in a loop seems to work to be better over all. If you were a machine learning researcher, these graphs are sort of an overview of why you should use a system that formulates a computational graph, and why you should use optimization on that graph if it is available. The difference in performance you pay for. Either with your time or your budget. So if you use something like PyTorch, do be sure to optimize, and be sure to check whether the <code>torch.compile()</code> function works for your setup. Next up is the parallelism module where you will be introduced, very superficially, to a couple of different concepts in parallelism. Hopefully, this will help you to understand stuff like model distributed parallelism and data distributed parallelism.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-seeing-the-memory-hierarchy","title":"3\ufe0f\u20e3 Seeing the Memory Hierarchy","text":"<p>Ok, so what we just did, with the graphs and the operator fusions and the what not, what did it do in terms of the memory hierarchy? For the CPU, it didn't do a whole lot other than adding another layer of complexity, but let's focus on the GPU. When we went from the immediate mode, to the graph version, it saved us from transferring to and from the GPU for every operation. It also meant that instead of adding each operation to the queue and then waiting for it to complete, we could just add all of the operations of our graph and then wait. So far so good. But what did the fused versions do?</p> <p>When calculating the linear operation, it kept the output in the threads register and applied the ReLU function once the data in register before storing it in the GPU's RAM and dispatching a new compute shader. One of the suggested exercises is to optimize the linear shader. In any case that should involve tiling and shared memory. That would mean that the matrix-matrix multiplication would need the work group to act in coordination and load in tiles of the matrices into shared memory (L1 cache) and synchronize, in order for them to get more out of memory they might otherwise have overlapping accesses of. Finally, when we use the graph loop, we don't actually tell the GPU to run this same queue for N iterations, but add to the queue, submit and wait N times. If we found the correct tools to tell the GPU to run some program for 100 times, it might completely set the GPU free for a while.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-the-results-and-the-caveats","title":"3\ufe0f\u20e3 The Results and the Caveats","text":"<p>Ok, so we just saw some results previously. I am mostly concerned with showing you the relative timings, otherwise I probably wouldn't be benchmarking on a laptop, but there a number of caveats that might make the benchmarks look slightly different. Some of these I might fix if/when I get the time, some are left as potential exercises, as again, this is more of an introduction to a bunch of concepts, and you get the point at this point hopefully. Point.</p> <p>Anyways, the <code>Tensor2DGPU</code> currently allocates a staging buffer, even for the ones that don't need it. If this was made optional, immediate mode computation would do an allocation less, so would all of the other GPU paradigms, but immediate mode is probably the one hurting the most here.</p> <p>Another pain point is the use of the one shot receiver, instead of being a reusable receiver. I tried to change it but I wasn't quite able to debug its behavior. If I get the time this is definitely something I would like to change. This results in the <code>graph_loop</code> versions only actually transferring back to the CPU once. So, fixing this is likely to make the <code>graph_loop</code> versions a bit slower. Not remaking the receiver each time and mapping buffers, but keeping them around might make that part of the process a bit faster.</p> <p>Finally, for the umpteenth time, the max, sum and linear shaders are completely unoptimized. The impact of max and sum are neglible due to only being called once per graph, but optimizing the linear operator would probably see all of the GPU based implementations run significantly faster.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-metaprogramming","title":"3\ufe0f\u20e3 Metaprogramming","text":"<p>Finally, I am going to introduce you to another way of representing these operators. Instead of having an operator with a full implementation of each operator and a lot of hardwired rules like, if a linear operator is followed by a ReLU operator, fuse them, you can attain a bit more flexibility by realizing one thing...</p> <p>Programs are just strings!</p> <p>We can decimate all of our neatly written shaders into something called op codes. You start by defining all of the data that goes in, you have a few lines of the thread figuring out its own ID and so on. Peruse the directory <code>src::op_code_compiler::runner.rs</code> or online . This is just a toy example, it didn't make sense to make the whole thing and I won't be benchmarking it since the results will be the exact same as the operator version. Each op code is just a string. Each operator is just a list of op codes. In this op code example we do operator fusion by adding our ReLU op-code to the list.</p> <p>This is sort of like ordering a standard cheese burger at a restaurant that ONLY SERVES BURGERS. You realize that you want pickles. So you can either order an entirely new cheese burger, the kitchen has to make a new one from scratch for this one, or you can order pickles between two buns, which technically qualifies as a burger. This will be delivered quite fast. But its frowned upon in a restaurant to play with your food so you have to eat the pickles as they come. But you do technically get your pickles. Another option is to realize that a burger is just a stack of ingredients, or a list of op codes, and it would be much easier to send the burger back to the kitchen (compilation) for them to just slide in a few pickles. Using op codes makes our code so much more complex, but it allows us a great amount of flexibility. If we found out we were running on a GPU with a much bigger L1 cache, we might change how we handled shared memory programming. If we were doing a render graph with a number of image-based single pixel operations such as tone mapping, changing hues or saturation, we might use op codes to merge these several different calls, keeping the data as close to the registers as possible.</p> <p>Another thing, often done in graphics is to have various defines in your shader code like</p> <pre><code>#ifdef USE_SHARED_MEMORY\n// something with shared memory\n#endif\n</code></pre> <p>Then if at runtime you find out it would be optimal to use shared memory you can merely append a</p> <pre><code>#define USE_SHARED_MEMORY\n</code></pre> <p>at the top of the shader file and then compile. This makes your code less readable, but not as unreadable as fully using op codes.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#5-further-reading","title":"5\ufe0f\u20e3 Further reading","text":"<p>Fun and hackable tensors in Rust Massively Parallel Fun with GPUs Compute Shader Glossary Torch.fx torch.compile Getting started with PyTorch's Dynamo compiler </p>"},{"location":"m1_memory_hierarchies/s6_exercises/","title":"4\ufe0f\u20e3 Exercises","text":""},{"location":"m1_memory_hierarchies/s6_exercises/#pen-paper-exercises","title":"Pen &amp; Paper Exercises","text":"<p>Speak to a classmate about your solutions. Which pointer kills the reference counting garbage collector? If the garbage collector implements cycle detection to depth X adding which pointer would break it? Weak pointers. Write out the memory of THIS sequence of vector operations. N dimensional indexing in 1D array</p>"},{"location":"m1_memory_hierarchies/s6_exercises/#programming","title":"Programming","text":"<p>Extend the computational graph with an inplace operation for the ReLU operator (only for the non-fused ReLU)</p> <p>The following list is sorted by expected complexity - do at least 1</p> <ul> <li>Add reusable buffers to the computational graph system</li> <li>Implement a shader cached version of the immediate mode GPU operators and add it to the benchmark</li> <li>Implement a version of the linear layer functions which uses shared memory and tiling</li> <li>Change the <code>Tensor2DGPU</code> to have switchable access details on its buffers. It should be able to accomodate some tensors being exclusively read-only. Do you see any performance differences for whether they are read-only or not?</li> <li>Implement the tree reduction version of the sum function and add it to the softmax function. Also compare the single pass and the tree reduction performance graphs. Reference</li> <li>Implement a max pooling operator, as well as fusing with ReLU, in all levels and implement tests</li> <li>Implement a convolution operator, as well as fusing with ReLU, in all levels and implement tests</li> </ul>"},{"location":"m2_concurrency/","title":"1\ufe0f\u20e3 Concurrency","text":"<p>Ok, so in the past module we looked at parallelism in the form of GPU-parallelism. In many ways, I find it to be an easier introduction to the concept of parallelism. Parallelism and concurrency are often used interchangably, but they aren't necessarily the same. Concurrency is when we run several calls at once, but they aren't necessarily running on two different processors. This could for example be the downloading of several files at once. Things are happening in the background, the process doesn't necessarily need to sit and wait for the first file to download and then request the second file for download and so on. Instead it can ask to download all of the files and then wait for all of them to be done, or for the first one to be done so it can begin processing the files.</p> <p>Parallelism on the other hand implies that we are actually running different cores and threads. So far I have introduced parallelism in small pockets inside a function which cannot do anything too complicated. The programs aren't long running and we choose a specific subset of problems to use the GPU for. In this module, I'll mainly introduce you to CPU based parallelism with different mechanisms. In creating longer running CPU-based parallel programs you will likely need to combine a bunch of these mechanisms along with your accrued knowledge of data races, as enforced by the borrow checker in Rust. Additionally, I will introduce a few more concepts in GPU programming in 3\ufe0f\u20e3.</p> <p>Anyways, why do we need parallelism in CPU's? Eventually, the clock frequencies, as in how many times per second a processor can do something, more or less flattened out. We get increased performance by either doing things in a smarter way or by increasing the amount of processors, either through a massive amount of parallelism in an accelerator, such as a GPU or through adding more processors.</p> <p>But parallel programming and parallel-friendly algorithms put a much greater cognitive strain on you, the programmer. The more you learn about parallel programming, the more you will see that the basic components are actually quite simple. The strain lies in thinking about parallelism and who owns what memory at which time. This is critical in not just getting faster programs, but retaining the correctness of your program from before you started parallelizing it.</p>"},{"location":"m2_concurrency/#algorithms-and-systems-design","title":"Algorithms and Systems Design","text":"Amdahl's Law  Image credit.  <p>Amdahl's Law is a fundamental concept in parallelism. Skim the link, but the concept is very simple. If 90% of your program is infinitely parallelizable, you will still be left with a runtime of 10% of the original runtime - if you take parallelization to the absolute limit. But how do you actually gauge which parts of your system are parallelizable? The answer is quite frustrating.</p> <p>It depends.</p> <p>It depends on what type of algorithms are in play in your system, what sort of hardware platform you are running on, it depends on what amount of development time and skill you have available. Sometimes when you think about optimizing your code you might visualize it as explosions and speed, flamethrowers and excess!</p> <p></p>  Witness Parallelism!  Image credit.  <p>But in actuality, working with parallelism takes restraint and consideration. Like a watchmaker placing tiny gears with a pincette. If we look back at the way we constructed computational graphs in <code>m1</code>, we were able to parallelize internally in each node/operator, but if we had very small matrices with a big depth, we would more or less be unable to do any parallelization, as the launching of threads to parallelize the matrices themselves, might cost more than simply having a single thread just handle the whole thing.</p> <p>Some elements in your system you might be able to parallelize lock free, wherein you find a solution without needing synchronization primitives like scopes, barriers, atomics, locks or mutexes. Some parts of your system might be amenable to fine-grained parallelism, such as a matrix-matrix multiplication, whereas other parts might only be amenable to coarse grained parallelism, such as a SLAM system pipelined into 4 stages, thus only being able to utilize 4 threads.</p> <p>All of these put one thing into the center of everything. Can you guess it?</p> <p>Memory!</p> <p>Some ways of accessing memory can seem completely fine when single threaded, but break down under the scrutiny of parallization. Trees can be hard, especially if you also have to modify them. As one researcher found it, a hierarchical hash map performed siginifcantly better for some types of algorithms on the GPU.</p> <p>Once you have the correct CPU based implementation, you should start asking yourself, where is this going to run and how is the memory accessed in order to accomplish what I want to do?</p>"},{"location":"m2_concurrency/#here-be-dragons","title":"Here Be Dragons","text":"<p>Some of the things you have to get used to when parallel programming is the sudden lack of serialized interactions. Thread 1 won't necessarily execute before thread 8, and the way you debug and verify your code will have to take that into account.</p> <p>Along the way, you will encounter a number of hazards. Especially race hazards are prevalent. The race condition happens when at least one thread is writing while one or more are writing or reading. Typically, these types of bugs can be very hard to find due to some part of your code being serialized once you try to find the bug or due to the multithreading, the execution might be non-deterministic.</p> <p>Take a few minutes to familiarize yourself with race conditions in software and data races here.</p>"},{"location":"m2_concurrency/#platforms","title":"Platforms","text":"<p>When you decide you want to parallelize your application, you almost always have to consider the platform you will be running on. Do you expect to run it on users' laptops, will you have GPUs available, will it be running in a data center with powerful, very expensive GPUs, will you be using an integrated GPU on a laptop, will it run on the edge or in the cloud? I will assume you are running on your own laptop or desktop for following along, but running on multiple data center GPUs seems to be all the rage these days, so I will keep that in mind.</p>"},{"location":"m2_concurrency/s0_data_parallelism/","title":"2\ufe0f\u20e3 Data Parallelism","text":"<p>The next sections will be heavily inspired by the book \"Programming Rust\"'s multiple implementations of Mandelbrot image generation. If you don't know about the Mandelbrot image, you can see what that's all about here! Ok, so I will start off talking about the parallel part of things. First off, lets look at  Rayon, which is the easiest way of doing parallelism in Rust.</p> <p>To use Rayon, we just have to formulate our computations as iterators. Under the hood Rayon divvies up the work into chunks and distributes it to a fitting amount of threads. It also does something called work stealing where if one thread is done with its work sooner it gets work from one of the other threads. This is really good for very uneven workloads like generating Mandelbrot images or path tracing. Again, this is the easiest way of doing parallelism in Rust, both because it is such a small change from idiomatic Rust (using iterators) and the cognitive load, if there is no communication between threads, is so very small.</p>"},{"location":"m2_concurrency/s0_data_parallelism/#a-parallel-iterator-benchmark","title":"A Parallel Iterator Benchmark","text":"<p>You can find the code for this section in <code>src/m2_concurrency/code/data_parallelism</code> or online.</p> <p>First we define our data, how many elements we want, and how many iterations we will iterate through the data to do our benchmarks.</p> <pre><code>    let element_count: usize = 10_000_000;\nlet iteration_count: usize = 100;\nlet mut data: Vec&lt;f32&gt; = (0..element_count).into_iter().map(|x| x as f32).collect();\n</code></pre> <p>Now let's see what a non-iterator mapping of all the elements would look like.</p> <pre><code>    for _ in 0..iteration_count {\nfor element in &amp;mut data {\n*element = *element * 3.14;\n}\n}\n</code></pre> <p>A key difference with this approach compared to the other two is that we are ensured this mapping happens in-place. The iterator version is here -</p> <pre><code>    for _ in 0..iteration_count {\ndata = data.iter().map(|x| *x * 3.14).collect();\n}\n</code></pre> <p>Once we have done this reformulation, Rayon is made to be a drop in replacement. We just import the needed Rayon prelude and replace <code>.iter()</code> with <code>.par_iter()</code> or <code>.into_iter()</code> with <code>.into_par_iter()</code>.</p> <pre><code>    for _ in 0..iteration_count {\ndata = data.par_iter().map(|x| *x * 3.14).collect();\n}\n</code></pre> <p>Finally, for reasons I will explain in a little bit, there is a variant of all three where they instead of a multiplication call this function -</p> <pre><code>#[inline(always)]\nfn map_function(x: f32) -&gt; f32 {\nlet mut x: f32 = x * x * x * x + x * x + x * x / x + x;\nfor _ in 0..62 {\nx = x * 2.0 + 4.0 + 12.0 / 59.0;\n}\nx\n}\n</code></pre> <p>Now let's run the benchmark.</p> <p></p>  First a simple mapping operation, and then a slightly more complex map function. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Ok, so what actually happened here? For the first three lines, we have an extremely small workload per element. We are very likely just limited by memory bandwidth. The simplest implementation seems to win out. Rayon needs to make sure it has the amount of threads ready and to distribute the work. Think of it like adding a for-loop. It's extra administration. Much like real life, simple processes rarely need complex administration. But once we add a more complex workload the simple for-loop and the iterator seem to converge to the same performance, whereas the <code>.par_iter()</code> from Rayon begins to win out. If we gave each element an even heavier workload, it is likely that the performance gain from Rayon would increase. Personally, I have used Rayon to parallelize a path tracer, starting with a range of all the pixels and then having Rayon distribute the workload of path tracing every pixel. In that case we have a VERY complex workload and I saw an almost linear scaling compared to the amount of threads available. I wouldn't recommend it, but if you want to see a larger system you can check it out here. The parallelization can be found in <code>render_pixel()</code> here and the <code>render()</code> here.</p> <p>So, now that we can conclude that Rayon can be really good and easy to use for some things, let's move on to more explicitly define our own parallel system with, perhaps, longer running threads.</p>"},{"location":"m2_concurrency/s0_data_parallelism/#3-examples-with-rayon","title":"3\ufe0f\u20e3 Examples with Rayon","text":"<p>You can find the code for this section in <code>src/m2_concurrency/code/data_parallelism</code> or online.</p> <p>Ok, so I made two additional examples. There's lots of different adaptors for iterators and I'll just show two. <code>.filter()</code> and <code>.window()</code>. If you go back to the file from earlier and change the bool in the main function to <code>true</code>, it should now run these two 3\ufe0f\u20e3 benchmarks. First off let's look at the filter benchmark. Filter is like map, except it will only emit elements which result in a <code>true</code> evaluation inside the closure. First we generate a vector of random floats with values between 0.0 and 0.1. Then we filter on them using the following lines -</p> <pre><code>    sums += data.iter().filter(|x| if 0.5 &lt; **x { true } else { false }).count();\n</code></pre> <p>If the random number generator used to generate the floats in <code>data</code> is completely uniform, every time we use this filter, the filter should emit half the elements. Then we just count the amount of elements emitted. The parallel version is very similar -</p> <pre><code>    sums += data.par_iter().filter(|x| if 0.5 &lt; **x { true } else { false }).count();\n</code></pre> <p>If we run this benchmark we get the following -</p> <p></p>  Counting all floats greater than 0.5. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, once again, with a limited amount of work, parallelism isn't necessarily the answer to everything.</p> <p>Next up, I will run a small program to perform convolution. We generate a data vector of random floats. Then we have a number of filters of different sizes, to show the effect of a greater sized filter. To convolve, with a filter of size N, for this example let's say N = 3, for the first output element, we take data element 0, multiply it by filter element 0 and add it to a sum. Then data element 1 times filter element 1, add it to the sum. Data element 2 multiplied by filter element 2. Add it to the sum and then emit/store the output element. In Rust, it can look like this -</p> <pre><code>    //\n// Convolution\n//\nlet element_count: usize = 1920*1080;\nlet iteration_count: usize = 1000;\nlet filter_sizes: Vec&lt;usize&gt; = vec![3, 5, 7, 9, 11, 13, 15];\nprintln!(\"Running convolution benchmark for {} elements with {} iterations!\", element_count, iteration_count);\nprintln!(\"Filter sizes are: {:?}\", filter_sizes);\nlet mut rng: ThreadRng = rand::thread_rng();\nlet data: Vec&lt;f32&gt; = (0..element_count).map(|_| rng.gen_range(0.0..1.0)).collect();\nlet mut filters: Vec&lt;Vec&lt;f32&gt;&gt; = Vec::&lt;Vec&lt;f32&gt;&gt;::new();\nfor size in &amp;filter_sizes {\nlet filter: Vec&lt;f32&gt; = (0..*size).map(|_| rng.gen_range(-1.0..1.0)).collect();\nfilters.push(filter);\n}\n// Remove mutability to be sure.\nlet filters: Vec&lt;Vec&lt;f32&gt;&gt; = filters;\n</code></pre> <p>Note that you are completely free to just change around the first three values. You can also try running different filter sizes. Now that we have created our 7 filters we will apply them -</p> <pre><code>    for _ in 0..iteration_count {\nlet filtered: Vec&lt;f32&gt; = data.windows(*size).map(|x| {\nx.iter().zip(filter).map(|(element, filter)| *element * *filter).sum()\n} ).collect();\nfiltered.iter().sum::&lt;f32&gt;();\n}\n</code></pre> <p>or in the parallel version -</p> <pre><code>    for _ in 0..iteration_count {\nlet filtered: Vec&lt;f32&gt; = data.par_windows(*size).map(|x| {\nx.iter().zip(filter).map(|(element, filter)| *element * *filter).sum()\n} ).collect();\nfiltered.iter().sum::&lt;f32&gt;();\n}\n</code></pre> <p>Note that instead of the <code>.iter()</code> adaptor that we would normally use to get an iterator to a single element at a time, we now use <code>.windows()</code>, which takes an argument. This argument is the size of the window. If we for example give it the size 3 it will return elements 0, 1 and 2 for the first iteration, then elements 1, 2 and 3 for the second iteration. This isn't available in a <code>.windows_mut()</code> like <code>.iter_mut()</code>. Why do you think that is?</p> <p></p>  Random convolution on random data. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>In this case, with the greater computational complexity, Rayon easily takes the lead.</p>"},{"location":"m2_concurrency/s10_sorting_and_random_access/","title":"3\ufe0f\u20e3 Random Access","text":"<p>Gyro Dropout Monte Carlo</p>"},{"location":"m2_concurrency/s11_parallel_graphs/","title":"3\ufe0f\u20e3 Parallel Graphs","text":""},{"location":"m2_concurrency/s12_exercises/","title":"4\ufe0f\u20e3 Exercises","text":"<p>Describe the base architecture of the egui-winit-wgpu template. Expand on the template and program some things (needs suggestions) using some of the primitives introduced in the module</p>"},{"location":"m2_concurrency/s12_exercises/#4-exercises_1","title":"\ud83e\uddec4\ufe0f\u20e3 Exercises","text":"<p>Pick items worth a total of 3 points or more, write am interpretation of each item of at least 10 times the number of points lines.</p> <ul> <li>2 - A Mandelbrot program served 5 different ways</li> <li>1 - Data-oriented design - Entity component systems</li> <li>1 - Array of Structs, Structs of Arrays, Auto-Vectorization</li> <li>1 - Linearized octrees</li> <li>2 - Sorting kernels in divergent workloads - Wavefront path tracing</li> <li>4 - ORB-SLAM - design and a warning about trying to code it</li> <li>4 - Nanite</li> <li>1 - PyTorch - Data-Distributed-Parallelism</li> <li>1 - PyTorch - Model-Distributed-Parallelism</li> <li>1 - PyTorch - Optimizing inference</li> <li>2 - Shadertoy</li> <li>2 - Gyro Dropout - MLSys 2022</li> <li>1 - Hierarchical Frustum Culling</li> <li>1 - SIMD optimization</li> <li>2 - Flash Attention</li> <li>2 - Custom memory allocators</li> <li>2 - JAX</li> <li>1 - Branch Prediction</li> <li>2 - Work Graphs in DX12</li> </ul>"},{"location":"m2_concurrency/s1_threads/","title":"2\ufe0f\u20e3 Threads","text":"<p>Largely based on the book.</p>"},{"location":"m2_concurrency/s1_threads/#what-is-a-thread","title":"What is a thread","text":""},{"location":"m2_concurrency/s1_threads/#launching-a-thread","title":"Launching a thread","text":""},{"location":"m2_concurrency/s1_threads/#joining-and-waiting","title":"Joining and Waiting","text":""},{"location":"m2_concurrency/s1_threads/#3-traits-send-and-sync","title":"3\ufe0f\u20e3 Traits - Send and Sync","text":""},{"location":"m2_concurrency/s1_threads/#3-crossbeam-instead-of-rayon","title":"3\ufe0f\u20e3 Crossbeam Instead of Rayon","text":""},{"location":"m2_concurrency/s2_mutex/","title":"2\ufe0f\u20e3 Mutex","text":"<p>Mutex, RwLock</p>"},{"location":"m2_concurrency/s2_mutex/#3-work-queue","title":"3 Work Queue","text":"<p>work queue</p>"},{"location":"m2_concurrency/s3_atomic/","title":"2\ufe0f\u20e3 Atomic","text":""},{"location":"m2_concurrency/s4_message_passing/","title":"2\ufe0f\u20e3 Message Passing","text":"<p>Channels</p>"},{"location":"m2_concurrency/s5_async/","title":"2\ufe0f\u20e3 Async","text":""},{"location":"m2_concurrency/s6_events/","title":"2\ufe0f\u20e3 Events","text":""},{"location":"m2_concurrency/s7_more_gpu/","title":"3\ufe0f\u20e3 More GPU","text":""},{"location":"m2_concurrency/s7_more_gpu/#asynchronous-memory-transfers","title":"Asynchronous Memory Transfers","text":""},{"location":"m2_concurrency/s7_more_gpu/#spir-v-glslhlsl","title":"SPIR-V &amp; GLSL/HLSL","text":""},{"location":"m2_concurrency/s7_more_gpu/#warp-shuffling","title":"Warp Shuffling","text":""},{"location":"m2_concurrency/s7_more_gpu/#distributed-shared-memory","title":"Distributed Shared Memory","text":""},{"location":"m2_concurrency/s8_branchless_programming/","title":"3\ufe0f\u20e3 Branchless Programming","text":""},{"location":"m2_concurrency/s8_branchless_programming/#simd","title":"SIMD","text":""},{"location":"m2_concurrency/s9_sparsity/","title":"3\ufe0f\u20e3 Sparsity","text":""},{"location":"m3_types/","title":"Types","text":""},{"location":"m3_types/#types-energy-usage-and-inference-quantization-sparsity-and-pruning-of-neural-networks","title":"Types, energy usage and inference, quantization, sparsity and pruning of neural networks","text":"<ul> <li>Floats</li> <li>Float precision</li> <li>Integers</li> <li>Energy usage (Vivienne Sze and the controversial paper)</li> <li>Inference</li> <li>Quantization</li> <li>Sparsity</li> <li>Pruning</li> <li>3\ufe0f\u20e3 Fast inverse square root</li> <li>3\ufe0f\u20e3 Bit tricks</li> <li>3\ufe0f\u20e3 Basic compression</li> <li>3\ufe0f\u20e3 Batch based data processing</li> <li>3\ufe0f\u20e3 Tensor Cores</li> <li>3\ufe0f\u20e3 Using integers instead of strings in hash tables</li> </ul>"},{"location":"m3_types/#4-exercise","title":"4\ufe0f\u20e3 Exercise","text":"<p>Find a suitable model and inference library. Perform inference. Optimize the model and inference process. Can you do inferencing on one thread, training on another and swap in the new model? ADD SUGGESTED MODELS  </p>"},{"location":"m3_types/#additional-reading","title":"Additional Reading","text":"<p>Full-Stack, GPU-based Acceleration of Deep Learning</p>"},{"location":"m4_profilers/","title":"Profilers and Case Studies","text":""},{"location":"m4_profilers/#introduction-to-profiling-optimization-case-studies","title":"Introduction to profiling, Optimization case studies","text":"<ul> <li>Profilers (PyTorch, web, GPU, general)</li> <li>Memory bound</li> <li>Compute bound</li> <li>Timing</li> <li>Variance</li> <li>Multiple samples</li> <li>Where to measure</li> </ul>"},{"location":"m4_profilers/#specializations","title":"Specializations","text":"<ul> <li>Training a neural network</li> <li>Optimizing a neural network for inference</li> <li>Running Yolo</li> <li>Optimizing a point cloud renderer</li> <li>Optimizing a path tracer</li> </ul>"},{"location":"m4_profilers/#4-exercise","title":"4\ufe0f\u20e3 Exercise","text":"<p>Try out the profilers relevant to your own system with some sample programs. Now try it with some of your own code from before you started on the guide!</p>"},{"location":"m4_profilers/#4-group-discussion-and-presentation","title":"\ud83e\uddec4\ufe0f\u20e3 Group discussion and presentation","text":"<p>Pick one of the following topics. Read and understand it, then present and discuss the topic with one or more other people. Preferably classmates.</p> <ul> <li>Packing bits for atomic operators</li> <li>Inverse depth buffers</li> <li>Bittricks, packing normals and colors</li> <li>Morton codes / Z-order curves, tiling and GPU textures</li> <li>Calculating compression precision in a lossy point cloud compression scheme</li> <li>DLSS</li> <li>Real-Time Texture Decompression and Upsampling</li> <li>2:4 sparsity with Tensor Cores</li> </ul>"},{"location":"m5_projects/","title":"3\ufe0f\u20e3 Tips and Tricks in Real-Time Systems","text":"<ul> <li>Events</li> <li>Key and Mouse events</li> <li>Event Loops</li> <li>GUIs &amp; egui</li> <li>memcpy</li> <li>Check/validate everything before the hot loop</li> <li>Hot loops, event loops</li> <li>Allocations in a hot loop</li> <li>Object Pools</li> <li>System calls - hoist out of the hot loop</li> <li>Logging and printing</li> <li>Bindings - PyO3 and cxx</li> <li>Walk, don't run, testing for correctness before optimization</li> <li>Don't use abbreviations</li> <li>Don't talk moon man language to me, ya Blargon!</li> <li>Don't use postfix incrementation++</li> <li>When to care about software engineering and when to care about performance</li> <li>Don't use a string key/identifier or integer, when a type safe enum will do the job</li> <li>Hard coding types</li> <li>Cognitive load, and delaying errors to after the first draft - deliberate development vs. debugging</li> <li>Prefer stateless programming, minimize stateful programming (functional inspiration)</li> <li>Implicit casting</li> <li>Compression</li> <li>Know your system - mobile, laptop, desktop, integrated memory, which GPU</li> <li>Use version control even for solo development</li> <li>Am I copying/cloning things that don't need to be copied?</li> <li>Anything that can be immutable, should be immutable - aliasing!</li> <li>Testing and Seeding RNG's</li> <li>Faster RNG</li> <li>Timing real-time systems and how to escape or offload compute</li> <li>Multi-resolution computing for making your real-time target</li> <li>Pressure testing and worst cases</li> <li>Static/Dynamic Dispatch - dyn, enum, trait</li> <li>The Markov Chain</li> <li>If using recursion and risking stack overflow, use a loop and a queue</li> <li>If you can, always prefer an array over more complicated structures</li> </ul>"},{"location":"m5_projects/#4-projects","title":"\ud83e\uddec4\ufe0f\u20e3 Projects","text":""},{"location":"m5_projects/#how-to-create-real-time-systems-good-frameworks-for-the-different-fields-and-project-proposals","title":"How to create real time systems, good frameworks for the different fields and project proposals","text":"<ul> <li>Starting with a simple prototype</li> <li>Identify your components</li> <li>Single threaded correct implementation -&gt; Testing to avoid regression</li> <li>Optimize</li> </ul>"},{"location":"m5_projects/#what-makes-for-a-good-project","title":"What makes for a good project?","text":"<ul> <li>What is your concept/project?</li> <li>Which concepts from the previous material do you think are relevant to your project and why?</li> <li>Preprocessing your data?</li> <li>How do you adapt to your chosen/available platform?</li> <li>Which libraries did you choose for this problem?</li> <li>How fast did you get to your minimum viable product?</li> <li>Which steps did you take from there and why?</li> <li>How did you determine which parts of your system to optimize?</li> <li>What else would you like to do with your system?</li> </ul>"},{"location":"m5_projects/#project-proposals","title":"Project Proposals","text":"<ul> <li>Virtual 3D scanner for a point cloud dataset</li> <li>EEG system</li> <li>Change the latent variables in a network using GUI, optimize the network</li> <li>Point cloud renderer</li> <li>Real-time style transfer on a web cam feed</li> <li>Rendering fractals influenced by a web cam feed</li> <li>Eye tracking -&gt; Present to screen and read from web cam -&gt; feature extraction -&gt; classifier -&gt; intervention signal -&gt; reading app (Wolfgang Fuhl, PISTOL, fixation detection)</li> <li>Bird classification from sound / Real-time classification of sound (Xeno-canto database)</li> <li>Who is talking? Real-time classification of sound</li> <li>Are you dyslexic? Eye tracking classifier</li> <li>Cognitive load tracker - Eyes &amp; pupil dilation and online estimation of signal strength (pupils vs. sound for the hearing impaired)</li> </ul>"},{"location":"m5_projects/#components-librariesframeworks","title":"Components - libraries/frameworks","text":"<p>blessed rayon egui wonnx tch winit cv ultraviolet arewelearningyet burn time chrono hifitime </p>"}]}