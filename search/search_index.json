{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Don't Panic","text":"<p>Hello There!</p> <p>You've found a guide for learning about all the stuff needed to either program or reason about data oriented and real time systems. It will help you with things like what memory allocations are, why computational graphs used to program neural networks are a good idea, different concepts in parallelization of code, what types are, how to profile and optimize code and how to create real-time systems. All running on your laptop!</p> <p>The guide is meant to be followed several times, with every increase in level the curriculum expands and it is assumed that you have learned and understood the previously introduced concepts. It's done this way, because computer science is a bit hard. To learn more stuff, you have to know some stuff, about some stuff, but to learn more, you have to know a lot of stuff about a lot of stuff. Which section is part of which level is indicated by an emoji, like so 2\ufe0f\u20e3.</p> <p>To make things more complicated, because everyone loves complicated, there are some sections which are meant to tailor to you, the reader! These specialization sections could for example have one set of tasks for people interested in compute graphics and a different set of tasks for people interested in deep learning. These sections are indicated by this DNA emoji - \ud83e\uddec.</p> <p>A course based on this material can be found here.</p> <p>The material comprising the guide is divided into 7 modules.</p> <ul> <li>Intro to the course, the different ways to use the material, intro to Rust and wgpu.</li> <li>Memory hierarchies and computational graphs</li> <li>Concurrency</li> <li>Types, energy usage, bit tricks and compression</li> <li>Optimization with an introduction to profiling</li> <li>A random grab bag of tips and tricks, GUI and bindings</li> <li>How to create real time systems and project proposals</li> </ul> <p>So let's get started!</p> <p>\ud83c\udf0c Queue Eric Idle singing while wearing a white wig \ud83c\udf0c</p>"},{"location":"TODO/","title":"TODO","text":"<ul> <li>Try rust-nexttest to solve the testing issue</li> <li>Find the right benchmarking and performance tools (blessed.rs)</li> <li>Look into friendlier error handling? Perhaps logging instead of panicing to get students used to logging. Introduce anyhow for better error handling?</li> <li>Loom</li> <li>Napoleon Dynamite and Llamas for GPU synchronization</li> </ul>"},{"location":"TODO/#emojis-for-later","title":"Emojis for later","text":"<p>\ud83c\udf0c \ud83d\udd1c 1\ufe0f\u20e3 2\ufe0f\u20e3 3\ufe0f\u20e3 4\ufe0f\u20e3 5\ufe0f\u20e3 \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb \ud83e\uddec \ud83d\udc7d \ud83e\ude90 \ud83d\ude80 \ud83d\udef0\ufe0f \ud83e\udd80 \ud83d\udd25  </p>"},{"location":"TODO/#references-and-additional-reading","title":"References and additional reading","text":"<p>High Performance Machine Learning High Performance Machine Learning Flash Attention Branchless Programming The Rust Programming Language Learn wgpu Install Rust wgpu ShaderToy Inigo Quilez ORB-SLAM ORB-SLAM2 Z-order curves Linearised Trees on the GPU Vivienne Sze - Energy Efficient AI Rust Profiling RenderDoc Book of Shaders Scratchapixel Ray Tracing in One Weekend Physically Based Rendering Crafting Interpreters Programming Rust Godbolt Advent of Code </p>"},{"location":"TODO/#im-done-but-i-want-more","title":"I'm done, but I want more!","text":"<p>Visual Computing Systems - Stanford Parallel Computing - Stanford</p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<ul> <li>Nicki Skafte Detlefsen</li> <li>Lars Kai Hansen</li> <li>Pioneer Centre for AI</li> <li>Mark Bo Jensen</li> <li>Mathias Gammelmark</li> <li>Anders Jess Pedersen</li> <li>Jens Egholm Pedersen</li> <li>Hans Henrik Brandenborg S\u00f8rensen</li> <li>Bernd Damman</li> <li>Andreas B\u00e6rentzen</li> <li>Jeppe Frisvad</li> <li>Mikkel Gj\u00f8l</li> <li>Nebula icons created by Eucalyp - Flaticon</li> </ul>"},{"location":"m0_introduction/","title":"1\ufe0f\u20e3 Introduction","text":"<p>Hello there! If you are reading this you might have been enticed by promises of performance and other some such advanced black magic, but first, a digression...</p> <p>There are so many things to keep track of as a modern day programmer and most systems hide these things from the user. You call something called... <code>?numba?</code> and annotate a few functions and it magically makes your code faster. You use something called <code>?Py?...?Torch?...</code> and it seems really slow for some reason. You're sure you have a GPU in your machine, but it's still slow. <code>PyTorch</code> has something called a profiler, but you don't know how it works and you don't know what the hell <code>DtoHMemCpy</code> even is. It can be hard to reason about what is going on inside these black boxes. On top of that you might not be able to find anything to walk you through all of the stuff you don't know that you don't know. As scary as it can sometimes seem to get your hands dirty and take on what might seem an insurmountable obstacle, not doing so can have consequences.</p> <p>The speed of execution of a program is approximately correlated to the energy consumption of that program. Until we use 100% green, renewable, energy in all of computing we have a shared responsibility to at the very least practice some modicum of restraint and sensibility in our resource consumption. Taken to the limit by putting large scale machine learning, with its massive energy consumption for both training and inference, in everything without necessarily generating value comensurate to the expended resources is an irresponsible use of resources.</p> <p></p>  Image credit  <p>If someone trains a deep learning model for two weeks on eight huge data center GPUs in a cluster, it is their responsibility that that training process is fairly well optimized, and that all data is responsibly retrieved, such that that training does not have to run again because of sloppiness.</p> <p>And thus stops the finger pointing!</p> <p>Optimizing code, especially on systems you might share with others both means that you can get your results faster, but that others can have use of the system in a reasonable time as well. If you are creating large models, optimizing them to be smaller or more efficient also results in entities with profits less than the GDP of a small nation actually being able to train and run inference with your model, increasing the democratization of your work and its reach. If its able to run on a consumer grade desktop - even better!</p> <p>This guide was made to be an iterative process, taking you by the hand, speaking to you at the level at which you are following it, trying not to overwhelm you. Reading that back, it could sound a bit condescending, but it basically means that the types of concepts you are assumed to know about will gradually increase with each level. Due to the guide gradually introducing certain concepts, jumping around the material is not recommended. I also acknowledge that some people have different interests. As such, some portions of the guide will be tailored to people who are into deep learning, people who like computer vision, or computer graphics or other some such. You are more than welcome to read and do all of it, but no one says you have to do anything. If you just follow along the path that is most relevant to you, you will be just fine. The guide does contain code, sometimes just small snippets, but also frameworks in which most of a module will take place.</p> <p>Most importantly - Don't Panic! The guide is here for you! And now for something completely different... practicalities!</p>"},{"location":"m0_introduction/#levels-and-annotations","title":"Levels and Annotations","text":"<p>The guide's way of doing things is to iteratively expand the curriculum and the depth at which concepts are described. You can follow the guide iteratively by doing it multiple times, each time advancing in level or you can jump right in to the relevant level.</p>"},{"location":"m0_introduction/#1","title":"1\ufe0f\u20e3","text":"<p>This level is for people unsure about investing the time and effort to do level 2. People are busy, and inherently looking to maximize the value gained given the time invested. Just for those people, each module has been boiled down to approximately 2 pages of reading. Reading all of the material should take at most an afternoon and is comprised of main page of each module. Basically, you could stop reading once you are done with the 1\ufe0f\u20e3 header and just click each \"MX - XXXX\" on the left, read that page until the end, then click on the next \"MX - XXXX\" title and read that until the end and you would be done. That does not include \"M4 - Optimization\" and \"M5 - Projects\". Happy reading!</p>"},{"location":"m0_introduction/#2","title":"2\ufe0f\u20e3","text":"<p>At this level you might be a comfortable programmer in Python, you might be a researcher, or you might just be warming up for level 3. In most cases you might not be all that comfortable with lower level languages, such as C, C++ or Rust. It is expected that you checkout the repository and try out the code on your own laptop. It is expected that you might change a few variables here and there to explore the performance benchmarks generated, but not much more than that. Don't worry, it does not require an Nvidia GPU to run on your laptop. There will be Rust code, \ud83e\udd80, but it will be as simplified as possible to just focus on making your learning experience as smooth as possible. If you have a systems programming background or are experienced in C/C++/Rust, you should be able to move through this level easily. This level does not take into account any module, section, page or info box with a 3\ufe0f\u20e3 or above in front of the name. You are still welcome to read them of course, but the language and code are a bit more advanced and it might be a bit too much if you are still working on the basics.</p>"},{"location":"m0_introduction/#3","title":"3\ufe0f\u20e3","text":"<p>This level is made up of all the material from 1\ufe0f\u20e3 and 2\ufe0f\u20e3. At this level it is expected that you have experience with C/C++/Rust and that you have tried programming a GPU or that you have previously done level 2 and are up for a challenge. If you haven't done any of these things, you'll be ok, but it might take significant effort on your part.</p>"},{"location":"m0_introduction/#additional-reading","title":"Additional Reading","text":"<p>The secret bonus level! It is the one where you do levels 1-3, perhaps even do the exercises and then read the \"Additional Reading\" sections at the bottom of select pages. The references here will generally be more rigorous. Typically these references will be to book chapters, university courses, scientific papers and in-depth blog posts.</p>"},{"location":"m0_introduction/#specializations","title":"\ud83e\uddec Specializations","text":"<p>Throughout the guide there are sections and exercises prefixed by \ud83e\uddec. These exercises and topics are meant to be up to you to follow. If you are mostly interested in deep learning, by all means only read and do the sections and exercises which are relevant to deep learning. Which section and exercise is relevant to which specialization will be explained in each section. To begin with deep learning will be supported, but once a full first draft is ready, I will begin adding things relevant to computer graphics, computer vision and cognitive signal processing.</p>"},{"location":"m0_introduction/#_1","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb","text":"<p>Sections indicated by \ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb are recommended exercises if you want to really learn the topics in-depth or if you are following a course based on the material.</p> <p>If you are a teacher who wants to make use of this material, feel free to use the course site. The course focuses on teaching real-time systems for deep learners and visual systems programmers. It allocates half of 15 days to go through the material in the guide and the other half to making a project relevant to the students' specialization, which is the contents of m5. It is designed to give shallow introductions to a wide range of concepts with a handful of varying exercises. The student is to then reach back and explore the topics relevant to their specialized project in greater detail. The breadth of topics is quite wide, and each student shouldn't be expected to pass an exam in every topic. In most cases they might remember that a thing exists and that they can search for it. The thing they hopefully all learn is how to reason about performance, systems design and that getting your hands dirty can be both fun and invigorating.</p>"},{"location":"m0_introduction/s0_intro_to_computing/","title":"2\ufe0f\u20e3 Introduction to the Computing Landscape","text":"<p>If you are new to programming, or perhaps have been able to get by using scripting languages only, you might not have been introduced to the other options. Some of the concepts presented here lay the foundations for the choices dictating the rest of the guide. Though I have made some clearly defined choices about which tools to use, you should at all times use the right tool for the job. Not only in which language or framework you might choose, but in how you put together and design your systems using those tools. Part of the guide's strategy is to introduce you to quite a lot of tools and concepts, also known as the \"learn what to DuckDuckGo\"-strategy, and then going into greater detail about core concepts and concepts relevant to your specialization. The guide will introduce concepts that aid some programs in producing faster results than others. An important factor is limitations. Usually, the word limitations carries a negative connotation, very few people think less freedom sounds enticing, but in computing, limitations can be a wonderful thing to have and set. Especially, once you are past the first prototype. In some cases, even when prototyping.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#scripting-languages","title":"Scripting Languages","text":"<p>Chill out, take things one line of code at a time. Scripting languages aren't compiled, but run one line at a time. This leaves the system unable to look beyond the current line of code, unless you add a compiler to the mix, whic usually takes a look at all of your code.</p> <p>Python is likely the scripting language you are most familiar with. Python is very popular due to its apparent ease-of-use, but it is quite slow in its raw form. The main advantage of vanilla Python is the ability to glue together a number of libraries written in other languages. In time, improvements have been made, such as putting type hints into your code which helps catch errors and gives more informative function definitions.</p> <p>In general, if other people reading your code must read the high-quality comments, that you definitely remembered to write... right?, then you are laying the foundation of a codebase that will frustrate people, probably including yourself. Python is easy to write at first, but the lack of a compiler can leave your long running code to fail just at the moment it is about to save the results because your forgot you were trying to save the wrong type.</p> <p>Python does have additional tools you can use to compile it. This allows for additional verification and performance improvements, but without additional limitations and indications of your intention, it might not be possible to optimize your code as much as a language which leaves things less open to interpretation.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compilers","title":"Compilers","text":"<p>A compiler processes the given code in one or more steps. In some steps it might verify that all of your code is correct, it might transform and optimize your code, it might change it into different representations like byte code or machine code. Some compilers strictly function before running it in an operation like <code>some_compiler -compile my_file.code</code> and outputs a runnable executable, specifically for your type of machine. This is usually done once before running your code and then only when changes are made. This is called ahead-of-time (AOT) compilation. Most compilers require additional constraints to transform and improve your code. Usually, you can also give your compiler additional commands to tell it how to compile. It could be things like \"please optimize my code to have a smaller executable size\" or \"please show me all warnings as errors\".</p> <p>Imagine you ask someone to go get you some milk every Thursday at 12. An unreasonably pedantic person (engineer) might be ready at 12 every Thursday and ask you what type of milk you would like today. It seems annoying and strange. You know what type of milk you like, the pedantic person should know what type of milk you like. That bastard! If you instead asked for X brand skim milk delivered at 12 every Thursday, the pedantic person might even try to optimize the process before the delivery day. If it was milk with a long expiration day, they could buy it in bulk and just have it ready for you. That unreasonably pedantic person is the compiler of whatever programming language you are using. It will go far to help you, it just doesn't perform well in ambivalent circumstances. Compilers are genereally not allowed to guess in a way that might functionally alter your code, such as reducing the level of precision.</p> <p>Most compiled languages are all designed with at least one compiler, usually compiling to byte code or machine code. However, it is possible to write a compiler after the fact. Cython is one such compiler made for AOT compiling Python. It benefits quite a bit from having the user perform additional annotations of their Python code, allowing for a decent speedup.</p> <p>Other compilers act just-in-time (JIT). Just as you want to run your code it will compile it. While this seems a bit weird, why not just compile it once and for all, this can allow the compiler to optimize the program specifically for your machine. Afterall, while two machines might have similar setups, one might have a GPU and other one not. The Java HotSpot VM even tries to optimize your code as it runs. If allowed to become a long-running process it can swap byte code for compiled machine code. In general, JIT compilers increase the startup time of your code, afterall, it has to compile it, just like the AOT compiler. Some JIT compilers save the compilation artifacts (the outputs of the compilation process) for later to merely reload it, but that won't help you much while you are developing your code. Some libraries and frameworks such as numba perform JIT compilation of your annotated code to optimize the performance.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compiled-languages","title":"Compiled Languages","text":"<p>In some languages like C, C++ and Rust, machine code is the outcome. That machine code can be quite platform specific, both because of the operating system and the hardware, and is in binary. 1's and 0's! These three languages are not garbage collected (more on that later).</p> <p>Another quite popular language is Go, which also compiles to machine code, but is garbage collected. Julia has more of a scientific/numerical focus, but features garbage collection, JIT compilation and can use either a runtime or compile to a standalone binary.</p> <p>Other languages like Java and C# compile to something called bytecode, which can then be interpreted by a process virtual machine. Thus all Java programs compile to the same bytecode, regardless of whether it's supposed to run on a Mac, Linux or Windows platform. The bytecode is then interpreted, sometimes optimized as well, at runtime by a virtual machine written for that specific platform.</p> <p>Javascript is a just-in-time compiled language running on most web pages. It can occassionally have a reasonable speed due to the optimizing runtime. Heavy development has tuned the widely used V8 runtime to improve Javascripts performance. Writing Javascript can seem easy, but the amount of things you are allowed to do, but shouldn't, can make it an arduous experience once the errors start piling up. The advantage is highly portable code, because everything is essentially just a string... including numbers, which is a traumatic experience, I would prefer not to elaborate on.  </p>"},{"location":"m0_introduction/s0_intro_to_computing/#the-guide-and-languages","title":"The Guide and Languages","text":"<p>As you can probably see in the column on the left... the guide will be using Rust from here on out, with a few exceptions. C will occasionally be used for reasoning about low level stuff like pointers and memory allocations, while C++ will be used as a comparison to Rust and Python will be used for a bit of perspective. In any case it will be assumed you don't really know any of the languages except Python and that you have read the introductions to Rust in this module. If you read the section on GPU programming, you will see there are no easy, one-size-fits-all, solutions. Thankfully, the guide has clear goals and limitations. To help you get familiar with new topics, we only need reasonable performance and for all the code to be runnable on most laptops. Most importantly, the setup process should be easy and not make you want to stress-eat the contents of your entire fridge when going through the installation process. As such the guide will mainly use Rust and the GPU API wgpu. The guide will in all cases that do not require graphics output only concern itself with pure computation through wgpu, which makes setup quite a bit simpler. wgpu is an abstraction layer that runs whatever GPU API it finds best suitable on your system. Having exact control and the absolute best performance isn't as important as allowing as many people to participate and learn as possible. After all, if it doesn't work on your laptop/desktop, you can't really play around and have fun with it!</p>"},{"location":"m0_introduction/s0_intro_to_computing/#3-gpu-apis-and-languages","title":"3\ufe0f\u20e3 GPU APIs and Languages","text":"<p>GPUs are some of the most readily available accelerators. Originally made for graphics, since around 2008 using them for general computation has been fairly wide spread. All graphics API's now also support general computation. Usually, it will be called a compute shader. Shader is the common name for a GPU program. If running CUDA or OpenCL, it is called a kernel. The guide will mostly focus on the pure compute parts of GPU APIs, except for the graphics specialization. Thus it will be assumed that if you are interested in the graphics specialization you might already have done a graphics course or a tutorial such as LearnOpenGL or Learn Wgpu. It is worth noting that a compute shader using a graphics-based API, such as Vulkan, can perform just as well as an implementation in a compute-only API, such as CUDA. One example of this is VkFFT. A GPU API is all the stuff that you have to write in your code that is not the function (shaders) that you want to run. It could be calls like creating a connection to the GPU, allocating memory on the GPU, transferring the contents of a buffer to the memory you just allocated on the GPU or launching your shader/kernel and transferring the results back to the CPU. The GPU languages themselves vary with the APIs. Some APIs, such as Vulkan, can take an intermediate representation called SPIR-V, this allows the user to write in any shading language, or even Rust in one case, as long as it is compiled to SPIR-V. Usually a shading language will look a lot like C/C++, but have its own distinct rules. You can't always make the same assumptions.</p> <p>The rest of this section is an overview of the various available GPU APIs.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#web-apis","title":"Web APIs","text":"<p>An often used strategy for making your programs as widely available as possible, is to use web-based techonology. Whatever browser you, or the end user is using supports some GPU APIs. For a long time it has been WebGL, which is a subset of OpenGL. WebGL has a version 2.0, which was finally supported by all major browsers not too long ago. The 2.0 version brought support for compute shaders with it. The modern newcomer is WebGPU which has a way of doing things that more closely resembles modern APIs such as Vulkan, DirectX 12 and Metal. It is not widely supported in browsers, outside of developer modes. Until then, the wgpu abstraction can be used. It has an API which follows the WebGPU specification, with some optional extensions for more features, but under the hood it uses whatever API it deems best for the current platform. Once the support for WebGPU becomes widespread, it can merely choose to run using WebGPU instead. In general, you will find that most frameworks or APIs which have to support a lot of things will be centered around the lowest common denominator. However, tools such as Vulkan and wgpu do allow you to query the system you are on for support of an extension, which does allow access to specialized features. You may however, end up with several versions of some elements of your code, based on whether some feature is there or not.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#platform-specific-apis","title":"Platform-Specific APIs","text":"<p>Some GPU APIs are specific to specific operating systems. DirectX11 and DirectX12 targets Windows and XBox platforms, while Metal targets Apple devices. The guide won't concern itself too much with these. DirectX11 is somewhat similar to OpenGL, while DirectX12 and Metal are from the same, more low-level, generation as Vulkan. Metal however, seems to be a bit less low-level compared to DirectX12 and Vulkan.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#cross-platform-apis","title":"Cross-Platform APIs","text":"<p>OpenGL and Vulkan are cross platform. OpenGL hasn't seen any updates for a while. Vulkan on the other hand is a low level, but generally popular API. It puts a lot of responsibility on to the programmer, but works on Windows and Linux, as well as Intel, Nvidia and AMD GPUs. It even works fairly decently on Apple devices thanks to MoltenVK Another cross-platform tool is wgpu, mentioned earlier. It is also the one that will be used in the guide for GPU code.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#compute-apis","title":"Compute APIs","text":"<p>Some GPU APIs are strictly not for graphics, such as CUDA and OpenCL. OpenCL is cross-platform (works on all GPUs), as well as compiling to FPGAs, DSPs and parallelized CPU code. On the other hand CUDA is just for Nvidia GPUs. CUDA is widely used in scientific computing and mostly dominates academia. Both CUDA and OpenCL have their kernels written in a specialized version of C++.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#shader-languages","title":"Shader Languages","text":"<p>Shader languages are languages specifically tailored for the combined graphics/compute APIs. Graphics APIs have some specific functionality which the language has to support. Usually you will find support for small vectors and matrices (up to 4x4) and various types you might not find on the CPU such as fp16. They will also usually have something called textures, bindings, samplers and built-in variables. You don't need to worry about that very much in the guide. GLSL, HLSL, WGSL and MSL are all shading languages developed for graphics APIs. OpenGL, DirectX, WebGPU and Metal, respectively. GLSL is the main shader language of Vulkan, but HLSL is also seeing rising popularity in that community. Lately, the tooling for cross compiling and running the same shaders on different graphics APIs has become a lot better. Shaders can be compiled to SPIR-V, an intermediate representation, sort of like the byte code we discussed earlier. This allows the platform independent SPIR-V to be translated to the specific instructions the GPU the code is actually run on. One tool for compiling shaders is naga.</p>"},{"location":"m0_introduction/s0_intro_to_computing/#3-domain-specific-languages-and-frameworks","title":"3\ufe0f\u20e3 Domain Specific Languages and Frameworks","text":"<p>Shading languages all benefit from limitations and specializations from being specifically for graphics on a GPU. Another form of limitation is domain specific languages and frameworks. One such framework you might know of is PyTorch. You are generally supposed to formulate your neural network as a graph and not just a sequence of operations. This allows PyTorch to have a clearer picture of what it is you want to do. It can check that all dimensions fit before running the training loop and it can optimize the process. Taking things even further PyTorch even has its own compiler from version 2.0.  </p> <p>Another way of achieving speedy results in a flexible format is retrofitting an existing language, in this case Python, with a slightly different language. Taichi combines a domain specific language to JIT compile highly performant code, which can also run graphics, to whatever platform you are running on. It can do this because of increased requirements of the user. Namely, annotating their code and setting limitations. Halide on the other hand restricts itself to be a AOT- or JIT-compiled language embedded in C++ made specifically for working with images and tensors.</p> <p>Futhark is a language made specifically for replacing the parts of your code which need to be fast. As such it is not a general language and can make opinionated choices which allows it to generate more performant code.</p>"},{"location":"m0_introduction/s1_intro_to_rust/","title":"2\ufe0f\u20e3 Introducing Rust","text":"<p>Why use Rust for the guide? Say you've decided you want to eat 35 burgers. Python is the friend that helps you order them to be delivered at your door. C++ is the friend who says 'do whatever you want'. Rust on the other hand, is the friend who stops you and then recommends you a salad. That may be annoying at times, especially if you were just craving 35 entire burgers, but it is what is best for you. Since the guide is not a beginners introduction to programming, and I will be introducing at times fairly advanced concepts, having a language that keeps you on the straight and narrow, even if it seems pedantic and like it is getting in your way, is a genuine advantage. If the Rust compiler accepts your code without any unsafe sections, it is probably going to work. Another point in Rust's favor was the easy setup and use on Windows, Linux and macOS. The setup time needed to be less than 10 minutes and the chosen language needed an easy-to-use, preferably integrated, package manager which didn't cause too many versioning issues. The options considered were C, C++ and Rust. C and C++ contained too many footguns and required the use of an external package manager and the use of header files and build systems. Rust takes care of all that with cargo. cargo can help you run and test your code, as well as helping downloading and building all of the dependencies in the <code>Cargo.toml</code> file, which you will find in the root of each code project. This guide was not supposed to be a guide for learning either any these languages. Rust's very helpful compiler is likely to be a boon for guiding users towards sensible code. The process of having to program in such a memory focused, modern, compiled language will turn what is otherwise an implicit, unspoken process inside out, forcing you to think about what good code is, where is the data located in memory, which thread has access to which data, and so on.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#setup","title":"Setup","text":"<p>To use the code in the course, as well as doing the exercises, first of all, you should have git and Rust installed.</p> <ul> <li>Install git</li> <li>Install Rust</li> </ul> <p>Once you have installed both, ensure they are properly installed by calling <code>git --version</code> and <code>cargo --version</code> in your terminal of choice. Your Rust version should be at least 1.68. At the least, the code in the guide won't function without support for the 2021 or newer version of Rust.  </p> <ul> <li>git clone the guides repository <code>git clone https://github.com/absorensen/the-guide.git</code></li> <li>In your terminal navigate into the folder that just appeared</li> <li>Navigate to <code>m0_introduction/code/is_my_system_ready/</code></li> <li>In the command line write <code>cargo run</code>. This might take a while as cargo will now download and build some dependencies, and then compile and run the code in debug mode.</li> <li>For IDE, the guide has been developed with VS Code with the extensions rust-analyzer, CodeLLDB, Rust Syntax, WGSL, wgsl-analyzer and optionally Dracula For Rust Theme.</li> </ul>"},{"location":"m0_introduction/s1_intro_to_rust/#projects","title":"Projects","text":"<p>Projects are how a Rust codebase is organized. A project can contain subprojects, but the guide won't use this. Navigating to your commandline and writing <code>cargo new my_project</code> will create a project with a <code>Cargo.toml</code> file and directory named <code>src</code> in the root. Inside the <code>src</code> directory there will be a file named <code>main.rs</code>. The <code>Cargo.toml</code> is a file describing the name of your project and its dependencies, which will be empty to begin with. But if you write some dependencies, next time you build the code with <code>cargo build</code> or <code>cargo run</code>, with or without the <code>--release</code> flag, cargo will download and build all of the dependencies. By default cargo will look in the <code>src</code> directory. All of the files with the <code>.rs</code> suffix are Rust source files.</p> <p>Entering <code>cargo run</code> will compile and run your code in debug mode, which means it will be easier to step through the code and getting better error messages. It will also result in significantly less compilation time, but slower run time. If you add <code>cargo run --release</code> it will compile in release mode. Compilation will take longer, the code will run faster, but debugging will be harder.</p> <p>To save on space, especially for some of the smaller projects where you just need to run a command or two, write <code>cargo clean</code> once you are done to remove all of the relevant dependencies.</p>"},{"location":"m0_introduction/s1_intro_to_rust/#testing","title":"Testing","text":"<p>Cargo can also handle unit testing of code. It requires that the project is split into an application and a library part, allowing the test to just test the library. In practice what is usually recommended is to just have a very small function in your application which tells your library to start running your code.</p> <p>Navigate to the project at <code>m0_introduction/code/how_to_test/</code> to see how a typical project is setup. Try running the commands <code>cargo run</code>, followed by <code>cargo test</code>. See how the code ran with <code>cargo run</code>, but <code>cargo test</code> actually told you which functions weren't living up to expectations? Setting up these unit tests is also how the guide tells you that the code you have been asked to write is correct. When doing exercises, keep going until all of the tests pass!</p> <p></p>  Fix the values in test_function_c() and run cargo test again!  <p>Now be sure to take a minute to look at the files involved in a project and read all of the comments! Sometimes you need more info as to what went wrong with the test. There's a fix for that on Windows.</p> <p>On some computers the GPU tests will currently fail unless being run with <code>cargo test -- --test-threads=1</code> as the tests are running concurrently. All of the tests requiring GPU access will try to grab the GPU without sharing resources. Even then it might fail. You can just try a few more times or try to run tests individually.  </p>"},{"location":"m0_introduction/s1_intro_to_rust/#3-clippy","title":"3\ufe0f\u20e3 Clippy","text":"<p>Clippy is cargo's tool for giving suggestions for improving your code and making it more akin to idiomatic Rust. The guide has most code conformant to Clippy's suggestions, however I choose to diverge where making the code simpler and easier to understand for people who have never programmed Rust before is a priority. Clippy's messages are very informative and a good learning experience. It is recommended that you use Clippy in your own code. It is as simple as calling <code>cargo clippy</code>.</p> <p>Running it on the <code>how_to_test</code> project, Clippy returns the following message -</p> <p></p>  I elected not to fix this, because the return statement was put there to make a point."},{"location":"m0_introduction/s1_intro_to_rust/#3-rustfmt","title":"3\ufe0f\u20e3 rustfmt","text":"<p>rustfmt is a formatter for Rust. Surprise! You can install it by running <code>rustup component add rustfmt</code> in a terminal. From then on you can run commands like <code>cargo fmt</code>, which automatically changes the code in your current crate (subproject, or the entire project if you are standing in the root).</p>"},{"location":"m0_introduction/s1_intro_to_rust/#3-fix","title":"3\ufe0f\u20e3 fix","text":"<p>fix is a tool for taking as many of those pesky compiler warnings as possible, and fixing your code for you. You just enter <code>cargo fix</code>.</p>"},{"location":"m0_introduction/s2_basic_concepts_in_rust/","title":"2\ufe0f\u20e3 Basic Concepts in Rust","text":"<p>The following chapter is an absolutely barebones introduction to concepts in Rust which you will need to understand to read the guide's code. If you would like a more thorough introduction to Rust, there is a number of nice tutorials available.</p> <p>The real contents of this section is the project in <code>m0_introduction/code/basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/","title":"3\ufe0f\u20e3 Less Basic Concepts in Rust","text":"<p>The real contents of this section is the project in <code>m0_introduction/code/less_basic_concepts/</code>. Go into the file corresponding to each function being called in the <code>main</code> function in <code>main.rs</code> and read all of the comments in order. The code can also be found online.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#supplementary-comments","title":"Supplementary Comments","text":"<p>In this section, I'll take you through a few addendums, which aren't as much about a specific language construct, but some concepts it might help to know.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#aliasing","title":"Aliasing","text":"<p>Aliasing is a term for when two pointers or references refer to the same place in memory. That might not sound like much of a problem at first, but it allows the compiler to make optimizations. Take a look at this code -</p> <pre><code>fn compute(input: &amp;u32, output: &amp;mut u32) {\n    if 10 &lt; *input {\n        *output = 1;\n    }\n    if 5 &lt; *input {\n        *output *= 2;\n    }\n    // remember that `output` will be `2` if `input &gt; 10`\n}\n\nfn compute(input: &amp;u32, output: &amp;mut u32) {\n    let cached_input: u32 = *input; // keep `*input` in a register\n    if 10 &lt; cached_input {\n        // If the input is greater than 10, the previous code would set the output to 1 and then double it,\n        // resulting in an output of 2 (because `10&lt;&gt;` implies `5&lt;&gt;`).\n        // Here, we avoid the double assignment and just set it directly to 2.\n        *output = 2;\n    } else if 5 &lt; cached_input {\n        *output *= 2;\n    }\n}\n</code></pre> <p>You can also check the Rustonomicon for a better explanation of aliasing. It is where the code snippets above are from. The code has been reformatted to preference. It may be on the more advanced side however.</p> <p>Basically, whenever you write to a value and there are multiple references to that value hidden away in different places of the memory hieararchy, such as some threads registers, or even within the same function, everything becomes invalidated. This is one of the reasons for the borrow checker adamantly enforcing that there can be multiple shared (read-only) references to a value, but only one mutable reference (read/write), and if there is a mutable reference, there cannot be any shared references to that value. If there were multiple shared references and a mutable reference it would be impossible to guarantee correctness as just when a value is retrieved from RAM by a shared reference a write to that value may have ocurred, which in order to make the sequence of operations correct might necessitate another read, but what if it happens again? Another read! This is not what happens, you just get a program behaving \"weird\". In another case, it would also mean you could not read from a value and save that value in a local variable to do a bunch of operations before writing it somewhere. It already sounds very headscratching and like you should only ever do single threaded programs. But thankfully, the borrow checker is there to keep things in check for you. One recommendation, you should try to minimize the time that a mutable reference to a value will exist.</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#multiple-function-definitions-not-allowed","title":"Multiple Function Definitions Not Allowed","text":"<p>As opposed to languages like C++, you cannot have multiple functions with the same name in Rust. In C++ this is perfectly legal, and the compiler will attempt to deduce which one you mean based on the way you are calling function().</p> <pre><code>void function(int argument_a, int argument_b) {}\nvoid function(int argument_a, int argument_b, int argument_c) {}\nvoid function(int argument_a, int argument_b, float argument_c) {}\n</code></pre> <p>Rust seems to be designed in a way as to minimize the amount of ambiguity faced by the compiler (and you too). Sometimes in Rust code you will see several different constructor functions, such as <code>build</code>, <code>build_from_ints</code>, <code>new</code> and <code>default</code>. In one way, that is a pain in the ass. In another way, it's quite nice. It forces the programmer to be explicit about how the functions behaviours are different, instead of being unwritten, implicit, or 'well, you can just read the code, it's not that complicated'. If you ever think or say that. Remember this... ahem RED FLAG! Fix your stuff so people don't have to guess, it will probably make the next person to read your code hate you slightly less. Which is a good thing!</p>"},{"location":"m0_introduction/s3_less_basic_concepts_in_rust/#index-checking","title":"Index Checking","text":"<p>Whenever you access an element in an indexed collection such as a Vec:</p> <pre><code>for index in 0..data.len() {\n    do_something(data[index]);\n}\n</code></pre> <p>Whenever you do this in Rust, there is a runtime check to make sure this index is not outside of the memory of the vector. This does have some performance cost, but unless you are absolutely sure this processing is happening in a hot region (a region of your code where a lot of time is spent), it is not recommended to try and circumvent this.</p> <pre><code>for index in 0..data.len() {\n    unsafe {\n        do_something(data.get_unchecked(index));\n    }\n}\n</code></pre> <p>It requires an unsafe region, which is a region in your code where you tell the compiler to allow you to do some things it would otherwise not allow you to, and call the function <code>.get_unchecked(index)</code>. An unsafe region does not turn off all checking, but in general, if you are at the level of reading the guide, you don't need it and we won't be talking about it more. If you really want to read more about unsafe, the Rustonomicon is the defacto standard introduction to unsafe in Rust.</p> <p>The two above functions are equivalent to</p> <pre><code>for(int index{0}; index &lt; data.size(); ++index) {\n    do_something(data.at(index));\n}\n</code></pre> <pre><code>for(int index{0}; index &lt; data.size(); ++index)  {\n    do_something(data[index]);\n}\n</code></pre> <p>Note however, that square bracket indexing is the defacto standard way of accessing an array element in both languages. This showcases a core difference between the two languages. One being safety opt-out, and another being safety opt-in.</p>"},{"location":"m0_introduction/s4_exercises/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Exercises","text":"<p>In order to get a hang of the basic of Rust I recommend doing Advent of Code problems until you feel comfortable moving forward.</p> <p>You won't be confronted much with the borrow checker in these small problems. To get better practice structure, smart pointers, dynamic dispatch and traits, I recommend moving on to doing Ray Tracing in One Weekend in Rust. There is a code snippet which shows your image on screen instead of writing it to file in <code>m1_memory_hierarchies::code::image_on_screen</code>.</p> <p>Once you have completed this part 1 out of 3, I suggest modifying the resulting code in the following ways -</p> <ul> <li>Remove the use of <code>dyn</code>. (Hint: Use enums)</li> <li>Remove the use of smart pointers. (Hint: My solution was to use indices and a geometry service provided by dependency injection)</li> <li>Parallelize computation of pixels through <code>rayon</code>.</li> </ul>"},{"location":"m1_memory_hierarchies/","title":"1\ufe0f\u20e3 Memory Hierarchies, Computational Graphs and Compilers","text":"<p>Not all efficiency comes from optimizing the various computational details like multiplications, divisions and such of a function. Quite a large part of it, in fact, comes from optimizing how much you write to and read from memory. 'Which memory?' you might ask, rightfully. The answering the where, when and what of memory will be the focus of this module. We can almost always get more cores to throw at a problem, we can also, at least on the CPU, quite easily get more memory, but that does not change the amount of time it takes to get a piece of memory, only how much data we can have in memory before we have to go to a lower level, e.g. go from RAM to disk. This is even more important given the relatively slow improvement of memory over time.</p> <p></p>  Image credit <p>For an introduction to how a computer works, see this explanation by Andreas B\u00e6rentzen. I will continue assuming you have read it.  </p>"},{"location":"m1_memory_hierarchies/#perspective","title":"Perspective","text":"<p>The further you move from simple, albeit heavy, problems such as a matrix-matrix problem to more heterogenous problems, such as training a neural network, the harder it can be to get good performance. How do you know or reason about what is where and when in complex systems like PyTorch, Tensorflow, JAX, Numba and Taichi? All of these frameworks, compilers and domain specific languages have to nudge you in different directions to give them the restrictions and hints needed to let them run your code as efficiently as possible. Nudges like defining your neural network as a computational graph. If you're unsure about what a computational graph is, the basic version is that you define a bunch of operations and how they relate to each other. Like input layer, followed by linear layer, followed by ReLU. But more on that later! Other advances include PyTorch, after several attempts with various degrees of success, finally introducing a compiler for optimizing the neural network you just defined. Or the functional programming style used by JAX in conjunction with the XLA compiler.</p>"},{"location":"m1_memory_hierarchies/#memory-hierarchies","title":"Memory Hierarchies","text":"<p>So what is memory anyways? Memory in a compute context is represented in several stages, all having their own capacity and speed. In order from smallest capacity and highest speed to largest capacity and lowest speed we have the registers, the L1-L3 caches, the main memory (RAM) and the disks. The registers are the fastest and smallest of the bunch. They reside right next to the parts of the CPU that does the computations. As a rule of thumb, most of the variables you declare in the scope of your function, unless there is A LOT of variables, will be kept in registers. The caches and the main memory all work in conjunction with each other as an invisible way of speeding up the accesses to the main memory (RAM).</p> <p></p>  A simplified view of the CPU memory hierarchy.  <p>Say you load in a file from the disk. If small enough, that entire file can be kept in memory. Which is great! We could keep all of the values in a great big array which we could access, like <code>current_value = data[index]</code>. But if you just wanted to read the first 5 values in the file in a loop, it would be incredibly slow to load those 5 values over and over again all the way from memory. What happens instead is that those 5 values might have separate copies in the L3, L2 and L1 caches, perhaps even in the registers. That would speed up things greatly. Whenever we asked for the first value we would first ask the L1 cache, do you have this value? If yes, that would be a cache hit, and we would pay a small amount of time to retrieve the value. If the L1 cache did not have a valid copy of the value, we would ask the L2 cache, and so on and so on, until we reach memory. If our file was too big to fit in memory, the operating system might even virtualize (don't worry about it) the memory and go all the way to the disk or to the internet to retrieve our value. Which is even slower than it sounds.</p> <p></p>  An example view of what the CPU memory hierarchy can look like with 2 cores.  <p>To further complicate things, multicore CPU's have each CPU sharing the disk, memory and L3 cache, sometimes they also share the L2 cache with a few other CPUs. We are also at risk of each core not just reading from the same values, but some of them could even modify one or more of the 5 values. At any point in time a value loaded from memory, to L3 cache, to L2 cache, to L1 cache, to the registers of thread A, might be invalid because thread B wrote to that value. This may have updated the value in memory and in thread B's registers, L1 and L2 caches, hopefully, it also updated it in an L2 and/or L3 cache it shared with thread A, but even then we would still need to move the value from L2/L3, to thread A's L1 cache and registers for it to be valid. Which is probably not happening. Multiple threads reading from a piece of data, which one or more threads are writing to is also known as a data race. Most likely thread A will end up with a stale version of the data and will continue as if the value had never been modified. Thread A will then write its own new version of the value, or just be working off an old version, resulting in incorrect results.</p> <p></p>  An example view of what CPU memory hierarchy can look like with 8 cores.  <p>Nudging the programmer (that's you!), to better define your program, not just line-by-line, but as a whole, to constrain these sorts of contentions, is one of the myriad reasons why frameworks like PyTorch can greatly speed up your code, if you help it along.</p>"},{"location":"m1_memory_hierarchies/#expanding-the-memory-hierarchy","title":"Expanding the Memory Hierarchy","text":"<p>To top it off we can expand this memory hierarchy with additional components, such as accelerators, networking and the internet! Let's start off with the GPU. It is an accelerator originally made for just computing graphics as fast as possible. It has a whole bunch of threads in it, meaning it can do very parallel work, like making every pixel of an image slightly darker. At the end of the 2000's, Nvidia saw a bunch of academics hacking the GPU to do stuff like fast fourier transforms using the fragment shader. Don't worry about what a fragment shader is, but shader basically means GPU program. So Nvidia releases CUDA as a pure compute (no graphics) API for using your GPU. It only runs on Nvidia GPU's though. Transfering memory from the CPU to the GPU and back, can be a quite explicit process. Not only does the CPU need to reserve some memory for copying to the GPU, the CPU and GPU have to be synchronized which can take a while, and then the data is usually transferred across the slower (compared to memory and cache) PCIe bus. This transfer is something you should always be thinking about if you are using a GPU for your program. Neglecting transfers is one of the fastest ways to slow code. The GPU also has its own memory hierarchy.</p> <p></p>  Image credit  <p>As you can see, this being representative of the Nvidia H100, there are 2 L2 caches and a whole bunch of smaller sections. Each of these smaller sections are a streaming multiprocessor (SM).</p> <p></p>  Image credit  <p>Here we have an L1 data cache and shared memory (more on shared memory later), shared between 128 threads. Each of these warps, Nvidia terminology for one of these four sections, have 32 threads with an L0 instruction cache, which is not matched for data. Additional accelerators exist, such as the neural engine featured in quite a lot of Apple products, and dedicated image and video processing hardware.</p> <p>You can even go outside of your current system. Two CPU's and eight GPU's could be tightly interconnected in a node, such as in the Nvidia DGX system. In the case of a DGX system everything is tightly interconnected with specialized hardware to minimize the time it takes to transfer data from one component to the other.</p> <p>Taking things even further, we could be sending data between more than one node, requiring yet another layer of communication, which is going to be slower than communicating internally in your CPU or in your node. When running on clusters with multiple nodes, the data you work from might have to be fetched from one or more storage nodes, which keeps your data between batch jobs. Taking neural network training as an example, if your data set is small enough to keep fully on the compute node you only need to load the dataset to the compute node before you begin training. Even better, the data set can be small enough that it fits, along with your model, completely on the GPU, meaning less transfers, less communication and better performance.</p> <p>If you wanted to make things worse however, and have your local systems administrator put you on speed dial, you would fetch your entire data set from the internet every time you launched a job. I am absolutely certain no one has ever just downloaded a Hugging Face data set whenever they launched a job... The internet can in this way be thought of as yet another, even slower, component of the memory hierarchy. Not much good comes from the internet. Try to avoid it. Except for this guide of course, which is very helpful.</p> <p></p>  Image credit"},{"location":"m1_memory_hierarchies/#wrapping-things-up","title":"Wrapping Things Up","text":"<p>Hopefully, this teaser hasn't scared you away from charging ahead and learning more about memory hierarchies and computational graphs. Memory hierarchies are at the center of getting good performance in pretty much all programs and it is worth spending some time on having at least a tenuous grasp of how to use them.</p>"},{"location":"m1_memory_hierarchies/#additional-reading","title":"Additional Reading","text":"<p>For a more in-depth explanation on the memory hierarchy see this chapter on Memory Hierarchy Design.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/","title":"2\ufe0f\u20e3 Soft Memory Hierarchies","text":"Memory hierarchy of the AMD Athlon.  Image credit  <p>As mentioned in the module intro, the CPU's memory hierarchy is represented by a series of hardware components with different sizes and speeds. But don't fret, memory hierarchies and their hardware design subtleties won't be the primary focus of this module. This section will focus on the various programming constructs to better use the memory hierarchy. First off we will start bridging hardware and software.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#getting-to-the-pointer","title":"Getting to the Point(er)","text":"<p>One of the core mechanisms in using memory is the pointer! All it does is point to pieces of memory. Why? Because a pointer is basically just an address. Anti-climactic, I know, but as one of the core building blocks of computing, we need to really examine these concepts from the bottom and up. If you have ever tried programming in C, you will invariably have been introduced to the pointer. The examples in this heading will be in C, but don't worry, we won't even define an entire function. Pointers are rife with opportunities for getting into trouble, to a degree where in Rust, which is made to be a reasonably safe language, you can't directly interact with a pointer unless you have an unsafe region around the pointer interaction. Yikes! On the other hand, you can get some of the most extreme performance by using raw pointers. So let's take a look!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#allocation","title":"Allocation","text":"<p>First of all, how do we get a pointer? Please note that checks for whether we have been given a valid pointer have been omitted. In the example below we get a pointer to a piece of memory which can hold up to 42 elements.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n</code></pre> <p>Let's break it down!</p> <pre><code>int element_count = 42;\n</code></pre> <p>We assign the number of elements to a variable in order to not have magic numbers.</p> <pre><code>int* integer_array;\n</code></pre> <p>This is actually bad practice. We have an uninitialized variable here. We could try and dereference the pointer, more on that in just a second, and try to access memory which we either don't have the right to access or which doesn't exist. The pointer at this point is likely to either be 0 or complete garbage. <code>int*</code> reads as \"a pointer to integers\" or \"address of one or more integers\".</p> <pre><code>integer_array = malloc(element_count * sizeof(int));\n</code></pre> <p>We ask for a memory allocation (malloc) from the operating system. What we get back is just a runtime dependent address. The address itself is what is known as a word. The size of the word dictates how much memory you can address in a system. If you have a 32-bit, 4 bytes, word and you use byte addressing, meaning one byte for every address, we can at most address 2GB of memory with a single word. If we have 64-bit words we can address more memory than we could possibly get. When you see something is a 32-bit or 64-bit operating system, this is why! It is also why we all of a sudden started using more than 2GB of RAM per computer in the 2000's. The address given by <code>malloc</code> will be different every time you run your code. Usually, any call to the operating system will be a very slow operation and should happen as little as possible. This can be stuff like writing to a terminal, accessing a file on disk, and so on. What we give malloc as an argument is the number of BYTES, as in 8-bits per element, we want. We want <code>element_count</code> elements which should each have a size of 32-bits (4 bytes). <code>sizeof(int)</code> returns 4. In total we ask for 168 bytes. <code>malloc</code> itself returns <code>void*</code>. Since C allows for implicit casting, what happens is that C, without us asking, changes the type to <code>int*</code>. Underlying it is the exact same thing. It is an address where 168 bytes allocated for us begins. What changes from <code>void*</code> to <code>int*</code> is how we dereference the pointer and what happens when we do.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#dereferencing","title":"Dereferencing","text":"<p>A pointer is a reference to another place in memory. Quite literally it is just a number. Dereferencing is a term for following the address to what it points to.</p> <pre><code>int element_count = 42;\nint* integer_array;\ninteger_array = malloc(element_count * sizeof(int));\n\n*integer_array = 0;\n*(integer_array + 1) = 1;\ninteger_array[2] = 2;\ninteger_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>In this example there's three different ways of dereferencing shown.</p> <pre><code>*integer_array = 0;\n</code></pre> <p>In C, we use the <code>*</code> operator in front of the pointer to follow the address to the memory. The base pointer we got from <code>malloc</code> is the address of the first of the 42 elements in our memory. Another way of seeing it is that <code>integer_array</code> holds an address, let's say... 42. Our program now asks the CPU to write to the address 42, the number 0. So far so good. But then this happens.</p> <pre><code>*(integer_array + 1) = 1;\n</code></pre> <p>This is one of the myriad reasons why we needed to have an <code>int*</code>. If the address in <code>integer_array</code> is 42, to get the next integer element, we don't go to the address 43, which would just be the second byte of the first element. No, we want to go to the address 46, where the second element in the array begins. Since <code>integer_array</code> has the type <code>int*</code>, we have defined that each element is 4 bytes and we now have a stride of 4 bytes. We also need to keep track of the size of our allocation close to the pointer itself, as trying to access an element outside of our allocation will be catastrophic, and likely result in a segmentation fault. So, no <code>integer_array[42]</code>. Back to the line on hand. We put our <code>integer_array</code> in a parentheses to make sure the dereferencing doesn't happen until after we have changed the address. So we increment the base pointer (42) with a stride of 4 (46), and then dereference (*) to assign a value of 1 to the second element in our array.</p> <pre><code>integer_array[2] = 2;\n</code></pre> <p>A short hand for the previous line, is this line. <code>integer_array[2]</code> is shorthand for <code>*(integer_array + 2)</code>.</p> <pre><code>integer_array = integer_array + 3;\n*integer_array = 3;\n</code></pre> <p>With these lines we manipulate the base pointer itself, by reassigning a value of the base address (42), incremented by 3 (54), before doing a simple dereferencing and assigning a value of 3. This is not a recommended way of doing things. How do we ensure that we always have the pointer to the base address? The least you can do is to copy the base pointer and increment that. Why?</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\n\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n</code></pre> <p>Because we need the address to give the memory back to the operating system.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#deallocation","title":"Deallocation","text":"<p>Once we are done with the section of memory we have so graciously been granted by the operating system, we should remember to return it to the operating system. If we don't we might get a memory leak, which is when our program uses more and more memory until the program is stopped or crashes. The operating system might keep track of the memory though and clean up once our less than stellar code terminates.</p> <p>In C, we can return our memory like this, using the free function.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\n\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n\nfree(integer_array);\n</code></pre> <p>Spot the error?</p> <p>We had two pointers and forgot to <code>free</code> using the base pointer, <code>base_integer_array</code>. This is undefined behavior, which means that there are literally no definitions of what will happen. It is really bad. What we should have done was this.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\n\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n\nfree(base_integer_array);\n</code></pre> <p>Note that <code>free</code> takes a <code>void*</code>. Our <code>int*</code> is cast, without us asking explicitly, to a <code>void*</code>. The operating system just wants an address. This allows the operating system to mark the section, denoted by the start of the section, and probably by its own record of the length. Note also that the address (42) held by <code>base_integer_array</code> is still in play. It is what is known as a 'dangling pointer'. We could try to dereference it after giving it to <code>free</code>, which is the notorious <code>use after free</code>. This is also undefined behavior as we try to access memory that is no longer accessible by our program. What we could do is to set <code>base_integer_array</code> and <code>integer_array</code> to new values to denote that they were invalid.</p> <pre><code>int element_count = 42;\nint* base_integer_array = malloc(element_count * sizeof(int));\n\n*base_integer_array = 0;\n*(base_integer_array + 1) = 1;\nbase_integer_array[2] = 2;\n\nint* integer_array = base_integer_array + 3;\n*integer_array = 3;\ninteger_array[1] = 4;\n\nfree(base_integer_array);\nbase_integer_array = NULL;\ninteger_array = NULL;\n</code></pre> <p>This does not however, stop us from trying to dereference those pointers, but it does allow for a more general check to see whether the pointers are still valid.</p> <pre><code>if (base_integer_array != NULL){\n    free(base_integer_array);\n}\n</code></pre> <p>If this all seems a bit scary, that's because it is. Anytime a system depends on humans just not making any errors and being rockstars at everything, it's a dangerous system and you should be on guard.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#access-patterns","title":"Access Patterns","text":"<p>While it is import that you increase your understanding of what it takes to get valid, predictable, boring code. Which is the best kind. What you are most likely interested in is for you to write more performant code. An absolutely essential part of getting performant code is how we access the underlying memory. Yes, we can address memory a single byte at a time with byte addressing. But, whenever we ask for a byte, the memory is transported as a cache line through the memory hierarchy. As in, the L3, L2 and L1 cache all receive an entire cache line. That cache line is usually 64 bytes.</p> <p>What is in the cache line is dictated by cache line alignment. If for example you had made a struct (it's like an object, but just the data) like the one below and you elected to turn off the auto-alignment with <code>__attribute__ ((packed))</code></p> <pre><code>struct __attribute__ ((packed)) my_struct\n{ \n    short first; // 2 bytes \n    int second; // 4 bytes\n}\n</code></pre> <p>and you made allocated an array of <code>my_struct</code> like so</p> <pre><code>int element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 6\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>If you had an alignment of say, 8 bytes, the last two lines would result in two cache lines being retrieved.</p> <p></p>  Bad cache alignment.  <p>Which is not good. What we could do instead would be to pad our struct a little bit, which is the default behavior in C.</p> <pre><code>struct my_struct\n{ \n    short first; // 2 bytes \n    short _pad; // 2 bytes\n    // Usually in C it will fix this automatically, padding\n    // every element to a multiple of a value. This could for example\n    // be 4 bytes.\n    int second; // 4 bytes\n}\n\nint element_count = 4;\nmy_struct* structs = malloc(element_count * sizeof(my_struct)); // 4 * 8\nstructs[1].first = 0;\nstructs[1].second = 0;\n</code></pre> <p>Then our alignment becomes this.</p> <p></p>  Better cache alignment.  <p>And we now only involve a single cache line. Which to remind you, is quite a bit smaller than the more standard 64 byte cache line.</p> <p>Now that we have learned a bit about cache lines, we are equipped to actually talk about access patterns. I have made some Rust code for you which is located at <code>m1_memory_hierarchies/code/access_patterns/</code> or online.</p> <p>First off is sequential access. It is the one we usually strive for. We start at one end and go through every element until the end, from index 0 to the end. If everything is cache aligned, great! If not, the cost of not being aligned will probably be as low as it can be, when we aren't reusing any retrieved elements. If a value, say a 4-byte integer is spread across two cache lines, that specific value may have to be reconstructed which can be expensive.</p> <p>Next up is strided access. With strided access we only read every N elements. Based on the size of the stride and the size of the elements, it might result in each cache line only being used for a single element. In the implementations in the code there is both a non-wrapping and a wrapping stride implementation, meaning once we step over the end we wrap back around using a modulo operator. This is to ensure that it accesses the same amount of elements as the sequential access. With the non-wrapping stride we only access every N elements, but we also end up doing much less work.</p> <p>Finally, we have random access. This is basically the worst case scenario. We randomly select an element to access the same amount of times as the number of elements in the array.</p> <p></p>  Timing access patterns in Rust.  <p>Given that we just talked about cache lines, most of these numbers make good sense. Random access is catastrophic, wrapping strided access is bad, but most interestingly non-wrapping strided access, which actually accesses less elements than the others, is slower than sequential access for strides 2 and 3. With stride 4, where we are only accessing one fourth the elements of the sequential access pattern, we begin to get faster. But what do you know, sometimes the nice and predictable path, which might seem like we are doing more work actually runs faster. What a time to be alive!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#stacking-heaps-of-trouble","title":"Stacking Heaps of Trouble","text":"<p>If you aren't familiar with the stack and queue data structure types, this would be a good time to follow the link and familiarize yourself.</p> <p>The stack is not just a data structure, but also a core part of how all of the variables in your local scope are kept track of when the program enters into a function. The stack is a designated part of the memory allocated to your program. It starts at size 0. Once you enter a function, each local variable is pushed unto the stack. The stack generally requires that sizes are known at compile time. Once you call a function from within your function, the local variables are no longer accessible to the function you just entered, but once you return from that function, they are. When you enter that function, a pointer to where you called the function from is added to the stack and that function has its own local variables.</p> <p></p>  The call stack.  Image credit  <p>If you push enough of these frames unto the stack, you can get a stack overflow. This can for example happen if you write a recursive program that doesn't terminate. In general, using variables from the stack will be much faster than using variables from the heap. But we also can't return pointers to a stack variable as it might disappear or be overwritten at any moment.</p> <p>The heap, in this context, is not the actual data structure known as a heap. Instead it is a bunch of unstructured memory living in the same reserved space as the stack.</p> <p></p>  The stack and the heap sharing memory.  Image credit  <p>Thus if either one becomes too big they begin encroaching on the other. Everytime you ask for dynamically sized memory, it is allocated on the heap. This is a slow process and you have to remember to deallocate the memory to not get a memory leak. But the memory survives across functions. If you remember the pointer examples from earlier - the memory segment we asked for lived on the heap, whereas the pointer (address) itself lived on the stack. We are allowed to keep the pointer on the stack because a pointer is a known size at compile time. We can also have arrays on the stack, but they generally need to have a size known at compile time. Moving a pointer from place to place, is also a lot cheaper than copying every single element of a large array every time ownership changes hands.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-dynamic-array","title":"The Dynamic Array","text":"<p>The dynamic array is ubiquitous in C++ and Rust. It is quite often what we think about, when we think of arrays in those languages. C++ has <code>vector&lt;T&gt;</code> and Rust has <code>Vec&lt;T&gt;</code>. I highly recommend reading the first parts of the Rust Vec page. They are basically the same though and I will refer to them as vector from here on out. A dynamic array bundles up the behavior we saw earlier with the pointers, allocations and deallocations, but adds the ability to automatically create a new array that is larger (usually by a factor of 2) than the old array and move the old values over to the new array. The vector has three values. How much memory is in its allocation, <code>capacity</code>, how much of the memory is currently in use, <code>length</code>, and a pointer to the data which lives on the heap. The vector itself can live on the stack and make sure to free the memory it points to once the vector is dropped from the stack. The vector supports quite a few operations, but the core ones are <code>push</code>, <code>pop</code>, array access <code>[]</code>, <code>reserve</code> and <code>shrink_to_fit</code>.</p> <p>Let's start off though with how we allocate a vector (in Rust).</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\n</code></pre> <p>In this case we should get a completely empty vector. It will have a default <code>capacity</code>, because we didn't specify any capacity it should start with. Let's just say this <code>capacity</code> is 4. However, if we want to print the current size</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nprintln(\"{}\", data.len());\n</code></pre> <p>we would get an output of 0! We have a <code>capacity</code> of 4, but a <code>size</code> of 0. Meaning, we have 4 integers of 4 bytes each on the heap, but they are unitialized (containing garbage values), and we have not used any of them. If we however use <code>push</code> to add some actual data and then print</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\n</code></pre> <p>we would print the number 2. Now we have live, initialized values on the heap at indices 0 and 1. We can print them by accessing the values directly.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\nprintln(\"{}\", data[1]);\n</code></pre> <p>In this case we print 2, 0 and 1. Push finds the first unused index, which is conveniently indicated by the <code>size</code> value, increments <code>size</code> and puts the value into the designated index. If we pushed 5 values however, once we reached the 5th push, assuming the default capacity was 4, we would see the 5th push taking a lot of time compared to the other 4 pushes. In this case the vector would allocate a new memory segment on the heap with a size of 8, copy all of the values from elements 0-3 and then add the 5th value to the vector. Conversely, we can also use the <code>pop</code> function.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(0);\ndata.push(1);\ndata.pop();\nprintln(\"{}\", data.len());\nprintln(\"{}\", data[0]);\n</code></pre> <p>Now we end up printing the values 1 and 0. In theory, a dynamic array should move to a smaller array at some point. Such as, when at a quarter of the reserved capacity. But in practice, Rust doesn't move to a smaller array unless explicitly asked to do so using the \u00b4\u00b4\u00b4shrink_to_fit\u00b4\u00b4\u00b4 function. In that case it will allocate and move to an array that is exactly the size of <code>size</code>, thus also making <code>capacity</code> the same. In practice, you should only do this for large arrays which are unlikely to see more elements added to it.</p> <p>But, in the case of knowing how many elements we actually we want to put in our vector, or at least an expcected minimum amount, we can just create the vector in a way where it has already reserved that amount of capcity. If you can at all do this, it is one of the easiest ways to get better performance as you remove a whole bunch of allocations, deallocations and copying. There's a variety of ways to control how allocation happens. The simplest one, if you know how many elements you want in your vector in advance, is to just create the vector with that capacity.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::with_capacity(5);\ndata.push(0);\ndata.push(1);\ndata.push(2);\ndata.push(3);\ndata.push(4);\n</code></pre> <p>In this case, we have been unambigously upfront about how many elements we will put in the vector. It was created with a <code>capacity</code> of 5 and a <code>size</code> of 0. We can also tell the vector to make sure we have a <code>capacity</code> of at least N. If it already has <code>capacity</code> to meet the minimum, nothing happens. If it doesn't it will allocate, copy and deallocate.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\nlet element_count: usize = 42;\ndata.reserve(element_count);\nfor index in 0..element_count {\n    data.push(index as i32);\n}\n</code></pre> <p>There are more idiomatic ways to do this in Rust, which might also be faster, but you get the gist!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#the-vector","title":"The Vector","text":"<p>But, we aren't just interested in single lists of numbers, sometimes, we would even like a matrix. In Rust we can have fixed size arrays defined like so:</p> <pre><code>let data: [i32; 2] = [0, 1];\n</code></pre> <p>If the sizes given to the array definition are constants, known at compile time, the array will be stack allocated. From what we have learned previously, the elements will be stored in memory in the order of 0 and 1. But what if we create a two-dimensional array?</p> <pre><code>let data: [[i32; 2]; 2] = \n                        [\n                            [0, 1], \n                            [2, 3]\n                        ];\n</code></pre> <p>In Rust the elements will be ordered in memory 0, 1, 2, 3. But that is not a universal truth. This is called row-major ordering and is the standard layout in C, C++, Rust, Python and most modern languages. The alternative is column-major which is seen in Fortran and Matlab. In column-major ordering the elements would be ordered in memory as 0, 2, 1, 3. With row-major ordering the memory will be most tightly packed in the last dimension from the left. To iterate through a 3 dimensional vector, this triple for-loop would access the memory in order.</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = \n                            [\n                                [[1, 2], [3, 4]],\n                                [[5, 6], [7, 8]]\n                            ];\n\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\n\nfor x_index in 0..x_dimension {\n    for y_index in 0..y_dimension {\n        for z_index in 0..z_dimension {\n            println!(\"{}\", data[x_index][y_index][z_index]);\n        }\n    }\n}\n</code></pre> <p>Where as if Rust favored column-major ordering the in-memory-order traversal would be</p> <pre><code>let data: [[[i32; 2]; 2]; 2] = \n                            [\n                                [[1, 2], [3, 4],\n                                [[5, 6], [7, 8]]\n                            ];\n\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\nlet z_dimension: usize = 2;\n\nfor z_index in 0..z_dimension {\n    for y_index in 0..y_dimension {\n        for x_index in 0..x_dimension {\n            println!(\"{}\", data[x_index][y_index][z_index]);\n        }\n    }\n}\n</code></pre> <p>If you think back to stride and cache lines, traversing our 3-dimensional array like the above in the actual case, where Rust is row-major, would be like the stride access we looked at earlier. We could also do this with nested vectors.</p> <pre><code>let mut data: Vec&lt;Vec&lt;i32&gt;&gt; = Vec::&lt;Vec&lt;i32&gt;&gt;::new();\ndata.push(vec![0, 1]);\ndata.push(vec![2, 3]);\n\n\nlet x_dimension: usize = 2;\nlet y_dimension: usize = 2;\n\nfor x_index in 0..x_dimension {\n    for y_index in 0..y_dimension {\n        println!(\"{}\", data[x_index][y_index]);\n    }\n}\n</code></pre> <p>This is even worse though. We now have a 2-dimensional array, which is highly flexible, but we have to dereference two pointers for every access.</p> <p>There is another way of doing this with a vector, which is the way I will be using multi-dimensional arrays in this module. It involves using a single dimensional vector as if it had more dimensions.</p> <pre><code>let mut data: Vec&lt;i32&gt; = Vec::&lt;i32&gt;::new();\ndata.push(vec![0, 1, 2, 3]);\n\nlet column_count: usize = 2;\nlet row_count: usize = 2;\n\nfor x_index in 0..row_count {\n    for y_index in 0..column_count {\n        println!(\"{}\", data[x_index * column_count + y_index]);\n    }\n}\n</code></pre> <p>We just create a vector with as much room as we need and then access it with a bit of calculation. We've flattened our matrix and can now both have it dynamic and with arbitrary dimensions. We can even dynamically decide to see the matrix in a different way, for example by deciding to swap the number of columns and rows. The formula to access each element is to multiply the index by the dimensions that come after it and add it to the next index. For example with three dimensions <code>x</code>, <code>y</code> and <code>z</code>, the index would be calculated by</p> <pre><code>x_index * y_size * z_size + y_index * z_size + z_index\n</code></pre> <p>and for the two dimensions <code>x</code> and <code>y</code>, we would access the 2-dimensional matrix with</p> <pre><code>x_index * y_size + y_index\n</code></pre> <p>I really hope this makes sense. Once it clicks it is a very simple formula, if a bit wordy. Some libraries will work like this under the hood but wrap it in an interface for you to simply access it like it was a multi-dimensional array.</p> <p>To wrap it up I have made a performance test of these approaches. The code doesn't match completely as we need bigger dimensions to get a good test. The code is at <code>m1_memory_hierarchies/code/the_vector/</code> or online.</p> <p>Implementing all of the methods described above in both row-major and column-major form, as well as an element-wise version, where we flatten the multidimensionality to save the administration of two of the for-loops, so we just get one for-loop running across a vector, we get the following numbers.</p> <p></p>  Access times for multidimensional arrays.  <p>The functions named Multi-Array are stack allocated instead of heap, which is why they are that fast. I was however unable to run them for 64x64x64 and 128x128x128. Rust refused citing a stack overflow. Interestingly as well, the element-wise function can be quite fast as it saves two of the for-loops. So, if you can, use element-wise. Otherwise, the row-major single vector function seemed to work the best. How much is saved by not having the two extra for-loops depends on how much work you are actually doing in each iteration. In this benchmark we do pretty much nothing.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#move-copy-clone-soldier-spy","title":"Move, Copy, Clone, Soldier, Spy","text":"<p>Now that we have examined how we can deal with a more expensive type, compared to the simpler integer or float, let's expand the scope a little bit. How do we actually move around these vectors as data? In each language there are some implicit rules, which can have wide reaching consequences, both in terms of correctness and performance.</p> <p>In Python, variables are all references to an underlying object, which is freed when there are no longer any references to said object. Don't worry about it too much, it is a 3\ufe0f\u20e3 concept I will introduce further down the page. But, it does have consequences when this happens</p> <pre><code>x = [5, 5, 3, 42]\ny = x\n</code></pre> <p>There aren't actually two lists, but two references to a list which has some data on the heap. This can be a bit problematic, as you now have two variables, which can both write to the same list without the other knowing. Once both <code>x</code> and <code>y</code> go out of scope, the list on the heap will be deallocated (eventually).</p> <p>In C and C++, the following actually results in two different lists on the heap, kept by two different variables.</p> <pre><code>vector&lt;int&gt; x{5, 5, 3, 42};\nvector&lt;int&gt; y = x;\n</code></pre> <p>C++ is copy by default, and this is a deep copy. Which is what Rust would call a clone. Rust however, is move by default.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x;\n</code></pre> <p>Once the values in <code>x</code>, the <code>capacity</code>, <code>size</code> and the pointer to the memory on the heap, have been moved from <code>x</code> into <code>y</code>, <code>x</code> is no longer accessible. The Rust compiler will complain. We can however, move it right back.</p> <pre><code>let mut x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x;\nx = y;\n</code></pre> <p>Now, <code>y</code> is inaccessible at the end. We could also create a scope, after which <code>y</code> is dropped, but the ownership is not moved back to <code>x</code>.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\n{\n    let y: Vec&lt;i32&gt; = x;\n}\n</code></pre> <p>Unless we move the values back ourselves.</p> <pre><code>let mut x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\n{\n    let y: Vec&lt;i32&gt; = x;\n\n    x = y\n}\n</code></pre> <p>To actually create two lists, like we did in the C++ example, we have to explicitly ask for a deep copy - a clone in Rust terminology.</p> <pre><code>let x: Vec&lt;i32&gt; = Vec::from([5, 5, 3, 42]);\nlet y: Vec&lt;i32&gt; = x.clone();\n</code></pre> <p>Usually, in Rust at least, adding lots of clones everywhere is the way to get around the borrow checker and have everything be correct. But once your first prototype is finished, one of the easiest improvements to your performance will be to search for all instances of .clone() and see whether there is some other solution that might work better. Rust isn't fighting you in this case, even if it can be strict, it is trying to protect you from having multiple write-enabled references to the same data, as in the Python example, which could make for incorrect code. C++ does have these move operations as well, it is even highly recommended a lot of the time. It is however, not the default behavior of the language.</p> <p>Rust has something called traits (don't worry about it). One of these traits is the <code>Copy</code> trait. If a type implements the <code>Copy</code> trait, it will be copied rather than moved when assigned to a new value or passed as an argument to a function. It is sort of like an implicit version of <code>.clone()</code>, except in the case of deeper structures, such as <code>Vec&lt;T&gt;</code>, in that case, it would copy all of the stack values, <code>capacity</code>, <code>size</code> and the pointer to the memory on the heap.</p> <p>But hold on a minute! That is illegal! We would have two pointers with full write rights. Which is illegal in Rust! Which is also why <code>Vec&lt;T&gt;</code> doesn't implement <code>Copy</code> and this has all been a ruse, for your edification.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#stacks","title":"Stacks","text":"<p>Now let's start looking at a couple of fundamental data structures. Next up is the stack. It isn't an array, but most implementations are just an array used in a restricted fashion. A stack is what is called a Last In, First Out (LIFO) data structure. The usual example is, imagine a stack of cantina trays. If you put a tray into the stack, in order to get a tray, you have to take the top tray, you can't remove a tray that is below the top tray.</p> <p></p>  Pushing a value on to the stack. The states are from before the push.  <p>If we implement this using a vector, we need at least the following 3 functions - <code>push</code>, <code>pop</code> and <code>peek</code>. <code>push</code> you might already know as the default mechanism for adding an individual element to a vector. The element to push is inserted at index <code>size</code> and <code>size</code> is incremented. With a <code>pop</code>, the element at index <code>size - 1</code> is returned and <code>size</code> is decremented. With a call to <code>peek</code>, either a copy or a reference to the element at <code>size - 1</code> is returned. Most, if not all functions are already implemented on the vector types, but if we want to maintain the invariant that all of the elements from indices 0 to <code>size - 1</code> are all valid, you need to make sure that only the stack related functions are called. In that way, if you need a stack, you should use not just a vector type, but a stack type, which might just be a wrapper around a vector, but also restricts anyone using that type to maintain the invariants needed for a valid stack. In that way sending a <code>Stack&lt;T&gt;</code> from function to function, instead of a <code>Vec&lt;T&gt;</code>, will communicate how the value is supposed to be used.</p> <p></p>  Popping a value from the top (end) of the stack. The states are from before the pop, and were the result of the previous push.  <p>Stacks scale well and all operations would be constant time, except when enough values have been pushed to necessitate a resize. However, the cost of this is low enough that across all of the operations it averages out and becomes amortized constant time.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#queues","title":"Queues","text":"<p>Queues, just like stacks, are a fundamental data type centered around constant time operations mostly implemented on top of dynamic arrays. Queues maintain a First In, First Out (FIFO) principle, just like queues of people. The first person to enter a queue, should be the first person to leave it. Now we no longer have <code>pop</code> and <code>push</code>, but <code>enqueue</code> and <code>dequeue</code>. Enqueueing is basically the same as <code>push</code> on a stack. An element is added to the index at <code>size</code>, except, the queue needs two new variables, <code>front</code> and <code>back</code>. Once the <code>back</code> index extends beyond the <code>size</code> or <code>capacity</code>, it can just wrap back around and starting again from 0, as long as it does not become equal to the <code>front</code> value. If it does so and <code>capacity &lt; back - front</code>, it can resize itself and adjust.</p> <p></p>  Enqueueing a value from to the back of the queue. The states are from before the enqueue.  <p>Resizing is just one way to handle the overlap. In quite a few real-time systems, we don't want the system to be overwhelmable. If data comes in too fast to process, and it keeps coming in faster than we can process, we might instead say that the <code>front</code> will move with the <code>back</code> if they become equal, thus letting the older data be overwritten. Other options could be to have whatever is trying to submit an element, wait until a spot opens up in the queue or the element could be \"added\", but not actually added to the queue. You'd of course like to be certain of how your queue type would handle being full. It's a central property and you should make sure if you are constructing systems with lots of data that you use a queue with the right behavior for your system.</p> <p></p>  Dequeueing a value from to the front of the queue. The states are from before the dequeue.  <p>Just like the stack, your local vector type probably has the functionality, but if you use it as a queue, you should probably just use a queue type, restricting any usage to maintain and communicate that it's a queue.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-smart-pointers","title":"3\ufe0f\u20e3 Smart pointers","text":"<p>Ok, so I promised previously, that I would explain how Python, and most other garbage collected languages, deal with assigning one variable to another. If you recall the previous example</p> <pre><code>x = [5, 5, 3, 42]\ny = x\n</code></pre> <p>We start by making a list and assigning a reference to <code>x</code>. In this case <code>x</code> is not the actual owner of the list. Instead, the system takes ownership of the list, and <code>x</code> is a live reference to that list. The system keeps track of how many live references there are to the list. Once <code>x</code> goes out of scope, the live reference count for the list decreases by one. Once the live reference count reaches 0, it is deallocated or marked available for future deallocation.</p> <p>Until we hit the end of the scope, and <code>x</code> and <code>y</code> disappear, there are two live references to the the list created at line 1. While a fine enough solution at first glance, sometimes, answering the question \"what is alive\" can be quite difficult. More on that in the garbage collectors section.</p> <p>When dealing with raw pointers, like we saw earlier, once a system grows beyond absolute simplicity, sharing multiple pointers to the same object becomes a bit complex. If you have 5 pointers to the same object floating about how do you ensure it isn't used after freeing? Who deallocates the pointer and who ensures that the pointers are no longer valid? This at the absolute crux of safety and your program not blowing up in C and C++.</p> <p>In C++11+ and Rust, we can elect to use something called smart pointers. Which can handle some of the intricacies for us. First off there is the unique_ptr, as in C++, or the Box in Rust. I will just refer to <code>Box</code> from here on out, their behaviors seem to be more or less the same. <code>Box&lt;T&gt;</code> is like a <code>T *</code> in C (pointer to object of type T). With two notable exceptions. It cannot be copied. As in, you cannot have multiple instances of <code>Box</code> pointing to the same underlying object. Thus <code>Box</code> in Rust, as well as in C++, requires that ownership is moved, and not copied. The other notable difference from a raw pointer is that once the \u00b4\u00b4\u00b4Box\u00b4\u00b4\u00b4 goes out of scope, the object on the heap that it is pointing to is deallocated automatically.   <pre><code>let box_variable: Box&lt;i32&gt; = Box::new(42);\nlet mut other_box: Box&lt;i32&gt; = box_variable; // box_variable no longer accesible due to move\nlet copied_variable: i32 = *other_box; // Dereference and copy the underlying value, this is not a move\n*other_box += 1;\nprintln!(\"{}\", copied_variable); // prints 42\nprintln!(\"{}\", *other_box); // prints 43\n</code></pre> <p>Next up are the shared pointers. They are essentially what Python is using in the example from earlier. In C++ it is called shared_ptr, in Rust it comes in two versions; Rc and Arc. <code>Rc</code> stands for reference counted. It is only made for single threaded usage as the reference count itself is susceptible to a data race, which you may recall, is several reads and/or writes to the same value. This could result in the count of live references being incorrect and the underlying value never being deallocated. <pre><code>use std::rc::Rc;\nfn main() {\n    let shared_reference_a: Rc&lt;i32&gt; = Rc::new(42); // Live references = 1\n    println!(\"{}\", Rc::strong_count(&amp;shared_reference_a)); // prints 1\n\n    let shared_reference_b: Rc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 2\n    println!(\"{}\", Rc::strong_count(&amp;shared_reference_b)); // prints 2\n\n    {\n        let shared_reference_c: Rc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 3\n        let shared_reference_d: Rc&lt;i32&gt; = shared_reference_b.clone(); // Live references = 4\n\n        println!(\"{}\", *shared_reference_c); // prints 42\n        println!(\"{}\", Rc::strong_count(&amp;shared_reference_a)); // prints 4\n\n        println!(\"{}\", *shared_reference_d); // prints 42\n        println!(\"{}\", Rc::strong_count(&amp;shared_reference_d)); // prints 4\n\n    }\n        // shared_reference_c and shared_reference_d are now dropped\n        println!(\"{}\", Rc::strong_count(&amp;shared_reference_b)); // prints 2\n\n        // Live references = 2\n        println!(\"{}\", *shared_reference_a); // prints 42\n        println!(\"{}\", *shared_reference_b); // prints 42\n}\n</code></pre> <p><code>Arc&lt;T&gt;</code> is here to solve exactly that issue. It uses atomic reference counting. Atomics will be introduced in the parallelism module. But in this context, it means that the reference counting is thread-safe, but a bit slower.</p> <pre><code>use std::sync::Arc;\n\nfn main() {\nlet shared_reference_a: Arc&lt;i32&gt; = Arc::new(42); // Live references = 1\nlet shared_reference_b: Arc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 2\n\n{\n    let shared_reference_c: Arc&lt;i32&gt; = shared_reference_a.clone(); // Live references = 3\n    let shared_reference_d: Arc&lt;i32&gt; = shared_reference_b.clone(); // Live references = 4\n\n    println!(\"{}\", *shared_reference_c); // prints 42\n    println!(\"{}\", *shared_reference_d); // prints 42\n}\n    // shared_reference_c and shared_reference_d are now dropped\n\n    // Live references = 2\n    println!(\"{}\", *shared_reference_a); // prints 42\n    println!(\"{}\", *shared_reference_b); // prints 42\n\n}\n</code></pre> <p>While <code>shared_ptr</code> from C++ allows you to mutate the value it refers to <code>Rc</code> and <code>Arc</code> do not. They require a synchronization primitive wrapped around your underlying value, like <code>Arc&lt;RwLock&lt;i32&gt;&gt;</code>, but that is more advanced usage, and don't worry about it right now. Other than the atomicity, and being shareable between threads, <code>Rc</code> and <code>Arc</code> work more or less the same.</p> <p>Finally, we have the weak pointer. This basically exists to weaken cyclical references. If object A refers to another object, object B, with an <code>Rc</code>, while the object B refers to object A, we have a problem. When either, or both go out of scope, they will not be deallocated as there is live references to both.</p> <p>Try to take a second and imagine this and the things that can go wrong when there are multiple references interconnected.</p> <p>Go on.</p> <p>I'll wait.</p> <p>To solve this issue, the weak pointer comes to the rescue. It is along for the party, but doesn't actually keep things alive. In Rust it is called Weak. It can reference the same underlying object as the shared pointer it comes from, but does not contribute to the live reference count. As such, it can allow you to have cyclical references, without causing a memory leak. If object A points to object B with an <code>Rc</code> reference, but object B points to object A with a <code>Weak</code>, once object A goes out of scope, both object A and object B can safely be deallocated. <pre><code>use std::rc::Rc;\nuse std::rc::Weak;\n\nfn main() {\n    let shared_reference: Rc&lt;i32&gt; = Rc::new(42); // Live references = 1\n    let weak_reference: Weak&lt;i32&gt; = Weak::new(42); // Create a weak reference from nothing\n    let weak_shared_reference: Weak&lt;i32&gt; = Rc::downgrade(&amp;shared_reference);\n\n    println!(\"{}\", Rc::weak_count(&amp;shared_reference)); // prints 1!\n}\n</code></pre> <p>For more information on smart pointers in Rust, there is a nice example here and another example about reference cycles, which is what we needed weak pointers for.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-the-vector-reloaded","title":"3\ufe0f\u20e3 The Vector Reloaded","text":"<p>This isn't meant to be a one-to-one representation of how tensors work in <code>numpy</code> or <code>PyTorch</code>, but combined with creating different views on the same underlying one dimensional memory as we learned about earlier, we can look at a few other fun concepts in different ways to arrange tensors.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#strided-access-and-transposition","title":"Strided Access and Transposition","text":"<p>One of the most used operations is the matrix-matrix multiplication. If we assume two 2D matrices as input and output into another 2D matrix, one of those input matrices will be accessed with a stride access in a column major form.</p> <p></p>  Matrix-matrix multiplication. The numbers indicate access order.  <p>There is a solution for this. We can just transpose the second input matrix. Transposition, as you may remember, is flipping a matrix around the diagonal. Another way to do this is to flip all coordinates. (0, 0) becomes (0, 0), but (3, 1) becomes (1, 3). Transposition is an expensive operation however, and we have to create additional code for whether the second input matrix is transposed and the other multiplication code for just that case. We also need to keep track of which matrices are transposed. In a more general, flexible system, or one in which the system does a lot of optimization without user input, we also need to evaluate when and where to tranpose matrices. But, if the matrix is fairly static and is read from often, it can definitely be worth the time and effort.</p> <p></p>  Matrix-matrix multiplication with the second matrix transposed.  <p>Now, lets try out a simple example! Checkout the code at <code>m1_memory_hierarchies/code/strided_access_and_transposition</code> or check it out online .</p> <p>Interestingly, when running the code there doesn't seem to be much of a difference until the matrix sizes become quite big. Why do you think that is?</p> <p></p>  Difference gets bigger as the matrices get bigger.  <p>One guess would be a combination of the compiler aggresively optimizing the code, the branch prediction of the pipeline (don't worry about it) being really good at guessing these very uniform workloads, but most importantly, the caches doing a lot of the heavy lifting for us. Once the caches run out of space we begin to see a gap between the two ways of doing it. This might be more pronounced on the GPU. In most cases you should probably start with making the simplest and easy comprehendable code and try out, and measure, potential optimizations before spending your time going down rabbit holes. This is will be a bit of a theme in the next few sections. There won't be much of a difference between techniques until the caches begin running out of space. At least if you aren't coding something really terrible, like randomized access.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#permuted-arrays","title":"Permuted Arrays","text":"<p>Sometimes we might want to change around elements in a matrix, without permanently executing the change. Not permanently executing these changes may also allow for several different views of the same data. So let's take a look at how permutations work.</p> <p>In the example below, the permutation is kept track of with the data in one vector and the index changes in another. The second of the two indices we need to map from one index to another is implicit. Thus for our permutation vector, index 0, means that at index 0 in our new permuted array resides at index 4 in our original data.</p> <p>This is likely to be quite a bit slower compared to normal sequential access as we now have to follow more than one pointer to get to our data.</p> <p></p>  Create permutations of an array by creating a list of indices and permuting that list.  <p>If we only view the data through the lens of a single permutation array anyway and we read from this vector alot, we might as well execute the permutation. If we wanted to be able to undo the permutation, we could just keep track of the permutation we executed and undo it later. But we should now be able to get back to sequential access performance.</p> <p></p>  If reading a lot from the same array with the same permutations, go ahead and execute the permutations.  <p>There is a middle ground however, which is if we are just permuting rows. As long as the rows are long, we should be able to get partially sequential access, at least if we are moving through the elements in order.</p> <p></p>  Offset some of the cost of permutations, by just permuting rows.  <p>Now, lets try out a simple example! Checkout the code at <code>m1_memory_hierarchies/code/permuted_arrays</code> or check it out online</p> <p></p>  Huh, that's weird. There doesn't seem to be much of a difference.  <p>It seems pretty much the same.</p> <p></p>  The differences appear as the cache runs out of space with bigger data sizes.  <p>Once we run out of cache however, the executed permutation is quite a bit faster. Permuting just the rows can also give quite a performance boost.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#jagged-arrays","title":"Jagged Arrays","text":"<p>A weird form of array is the jagged array. A 2D matrix can't simply be expressed as having dimensions NxM, but Nx? or ?xM dimensions. As in N rows, each with their own individual lengths, or M columns, each with individual lengths. It's a highly flexible scheme, but unless you are absolutely sure you need it, you should absolutely avoid it.</p> <p>In the example below, we attain this complete flexibility by using a vector of vectors, which as you may recall is really bad for performance.</p> <p></p>  Create a jagged array by using a vector of vectors.  <p>If the difference between the smallest row and the largest row isn't too big, we can sacrifice a bit of additional memory for allocating all rows as if they had the same length and keep track of the length of the active sections in each row in a separate vector.</p> <p></p>  Slightly better now with the data in a single vector.  <p>If we really wanted to compact the jagged array above, we could remove all of the non-active segments (denoted -1) and use the auxiliary array to indicate where each new row starts. Just like the first permutation scheme, we are dereferencing two pointers for access.</p> <p>Finally, we could do all of this, still under the constraint that we have a reasonable ceiling on the max length of each row, by interleaving the auxiliary array with the data array.</p> <p></p>  All of the data in a single vector, with the blue values being the amount of active data in the row.  <p>We can do this either compacted or non-compacted.</p> <p></p>  All of the data in a single vector, with the blue values being the amount of active data in the row. Compacted data.  <p>We've now removed what was a consistent implicit form. We no longer have random access to the row lengths. We also now have to translate from whatever type is in the data array to valid integers for indexing. If the data is integers, casting won't be much of a problem, but for floating point numbers we have to be sure to get it right. If a number is not a whole number we are likely to have the number floored to the nearest whole number. Instead we have to go from row length to row length and find out how many indices we have to move forward to get to the next indicator. As such, to get to the lower right corner element (42), we would first have to read index 0, jump 4 spots forward to index 4, read the 4, jump 5 spots forward to index 9, and then jump forward 2 elements to get to what in a dense array would be index [2, 1].</p> <p>This sort of makes me miss the auxiliary array. We can sum up the jumps to denote where each row starts. This would allow for compaction of the data while keeping us to just two jumps. Note that we now keep track of the length of each row by taking the difference between the starting index of the row we are looking to find and the beginning of the next row. Which is also why I have inserted an extra starting index, which points to the end of the array. Otherwise, we can't get the length of the last row.</p> <p></p>  As we compacted the data, we can keep track of the starting index of each row in an auxiliary array.  <p>Now for a simple performance benchmark. Checkout the code at <code>m1_memory_hierarchies/code/jagged_arrays</code> or check it out online</p> <p></p>  Huh, that's weird. There doesn't seem to be much of a difference.  <p></p>  There we go, we ran out of cache!.  <p>Note that the only version which is not extremely slow for inserting values is the naive one. But in most other cases our final optimized version JaggedArraySizeCompactedAux seems to be the winner. It doesn't take a lot of memory compared to the other solutions and it seems to be in some cases on-par with the fastest (with a reasonable variance) or the fastest. In most other cases the NaiveJaggedArray seems just fine. Again, don't overcomplicate things and measure the differences for your case. In any case, you should avoid a jagged array if you can. And especially the CompactedJaggedArray, which costs the least memory, but has a catastrophic access time due to needing to accumulate the indices needed to find the row index. Plus, having the indices be interleaved with the values is problematic as we mix control flow and data, as well as needing to accomodate casting a data value to an index value. Please don't do that!</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#sparse-arrays","title":"Sparse Arrays","text":"<p>Finally, we have the sparse array. In the case of huge matrices with lots of values we don't care about, especially 0's, we can use the run-length encoding we just saw to encode values. This usually results in having to reconstruct where the indices are on the fly. The method below is ok for singular values, such as a very large matrix with just diagonal values. If we have a band around the diagonal we could modify the strategy from the last example in the jagged arrays section.  </p> <p></p>  A sparse array created with run-length encoding. We could of course also just linearize the indices to get a single number.  <p>For this to be more efficient than the dense version, you usually need at least 90% sparseness, or an array so big that you are having issues with memory. Sparse matrices also require their own separate implementations and can be hard to parallelize.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-hash-maps","title":"3\ufe0f\u20e3 Hash Maps","text":"<p>Another fundamental data structure is the hash map. The hash map takes a key type and a value type. The value type can pretty much be anything, don't worry about it! But where things get really interesting is the key value. What a hash map does is to take a key value and translate it into an array index using something called a hash function. A very simple hash function takes a number, adds a number, multiplies by a very big prime number and then modulos that number by a number representing how much space we have available. The base recommendation is that a hash map should have at least twice the space needed to densely represent the same number of elements.</p> <pre><code>// Not actually a good hash function\nfn example_hash_function(key: Vec&lt;char&gt;) -&gt; usize {\n    const PRIME: usize = 6457;\n    let mut hash: usize = 0;\n    for element in key {\n        hash = ( (hash * 31) + key as usize ) ^ PRIME;\n    }\n\n    hash\n}\n</code></pre> <p>Generally, a hash map will have constant time lookup and insertion. The reason for the recommendation of at least a factor 2 in space is collisions! A collision is when two different keys hash to the same value in our storage. Remember that we can have both the initial key that we queried with, and the post-hash key used for indexing into storage. One way of resolving the collision is to keep searching our storage until we find an empty spot. But then if we query our hash map and the first index we look at in storage, we iterate the array until we find a key that matches the one we queried with. Much like vectors, the hash map could dynamically expand to accomodate inserted data. Once we are done with insertions, we might have a fragmented performance. If we know we are done and have a significant amount of elements which need to be queried a lot, we can usually ask the data structure to <code>.shrink_to_fit()</code> or <code>.rehash()</code>. Rehashing will reconstruct the structure to be made as if it had only been the elements currently stored, all along.</p> <p></p>  A number of keys have been inserted in random order. We try to find the entry corresponding to the key \"Ni\" at index 3. But its natural spot was already taken by a spillover from the index 2. We find the entry in the next index instead. This is also known as open addressing.  <p>I will reiterate a theme here - if it can be done with a basic array, it should probably be done with a basic array. Of course there are different, more optimized methods for implementing hash maps, you can usually find a few different ones based on the specific needs for your usage, i.e. if you need better insertion performance or better read performance, but this is basically what you need to know. In Rust it is <code>HashMap&lt;K, V&gt;</code>, in C++ it is <code>unordered_map&lt;K, V&gt;</code>, in Python and C# it is called a dictionary. You can use anything for the key in Rust, as long as the type implements the <code>Hashable</code> trait. You can even use strings. This can be very useful for keeping an assortment of random data which you need to distinguish between. For example, if you needed to keep track of different layers of a neural network with random access, you can just create a new string \"Linear0\" and use that as a key for the first linear layer and its contents, and then \"ReLU0\", \"Linear1\", \"ReLU1\", \"Softmax0\" and so on. If possible, it is more efficient to use small types as your key. Such as an integer.</p> <p>Now for a simple performance benchmark. Checkout the code at <code>m1_memory_hierarchies/code/hash_maps</code> or check it out online</p> <p>As you can see the hash map using integers clearly outperforms Strings. To be fair, every insertion in the string based map, requires a clone of the original string, the read and update only requires a reference. But we can expect just about a factor 2 performance difference by using the simpler type with the simpler hashing function. It should however, be noted that the string keys were all just the integer keys as strings, which might have an influence on the distribution in the hash table. What we could do in our previous neural network layer example would be to have an integer value representing each layer type and then the id. We could relegate them to different parts of an integer. This could for example be the first 20 bits reserved for the layer type and the last 44, or perhaps just 12, bits reserved for the layer id. This does however incur a significant amount of extra code and the code will become more complex and implicit, so it's probably only worth it if you are doing A LOT of accesses for each layer.</p> <p>Generally, hash maps have an alright performance. C#'s dictionary lookup performance will usually go down hill at around 30k entries though. This doesn't happen for arrays. You can read more about different hash table implementations here.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-graphs-and-trees","title":"3\ufe0f\u20e3 Graphs and Trees","text":"<p>Now that we have dicked around with variations on a theme (that theme was arrays if you were in doubt), let's look at a different fundamental data structure. Graphs! Not the kind with the lines... wait these have lines too, uuuh, not the kind that has an x and a y axis, but the kind that has some circles with some arrows between them. \"But wait!\" you say, \"The heading says 'Graphs and Trees'\" you say, well, trees can be seen as a subset of graphs, while all graphs are not necessarily trees.</p> <p>Graphs and trees are some of the absolutely fundamental data structures which you need to be acquainted with. Along with arrays, queues, stacks and hash tables, they are the fundamental building blocks with which you can make pretty much anything. Graphs and trees are a bit special, however, in them being potentially easy to implement, but also very easy to mess up. Languages like C and C++ let you implement them with relative ease, but implementing graphs and trees without cyclical references (which can cause memory leaks), without data races, without dangling pointers and other robustness issues, is actually quite hard. Sometimes even fundamentally unsafe.</p> <p>I have used Rust as one of the primary languages for demonstrating and benchmarking things for you. The examples under this header will be more along the lines of toy examples as Rust code for graphs and trees can get quite hairy if you don't want to just sprinkle <code>Arc</code> everywhere. And even then you might end up having to battle cyclical references. It's really nice that the compiler puts on guard rails for you and herds you towards safe behavior. Implementing graphs and trees in Rust is notoriously difficult for this exact reason. Which is not to say that it is easier in C/C++, the compiler just doesn't stop you from doing something problematic.</p> <p>Anyways... the rest of the module will be about how using data structures like computational graphs, which is essentially what is created when you define your entire neural network on a single object in PyTorch, can speed up your code immensely as the structure allows the library/framework/compiler to reason about your program. Essentially, computational graphs communicate the intention of your program ahead of time before you start running everything in a loop. It can help the library/framework/compiler to optimize your code, optimize where the data should be located, when the data should be moved to/from the GPU, when two operations can be fused and so on.</p> <p>Additionally, I will take a look at one of the simpler trees, the binary tree, and if you are interested in graphics or computer vision, the octree is recommended reading.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#graphs","title":"Graphs","text":"<p>Ok, so let's get this show on the road. Graphs are primarily made up of two things, nodes and edges. Edges are references from one node to another. In a diagram they are usually represented by a line, some times with one more arrows on the ends. Edges can be represented by indices, pointers, smart pointers or something else that I can't think of right now. The node on the other hand, can be \u2728 whatever you want \u2728. It can even be just a number or an index to the corresponding data payload if you have seperated the graph structure from the data payloads.</p> <p></p>  A bidirectional graph. Each edge points both ways.  <p>Graphs come in lots of different flavors, but the three most important, and fundamental, are bidirectional, unidirectional and DAGs. Bidirectional means that the edges go both ways. If node A points to node B, node B also points to node A. Unidirectional graphs, you guessed it, means that the edges only point one way. That does not dictate that node A and B can't point to each other, but that it's not the default and it requires inserting two edges into the graph. Note that edges can also have weights or other values themselves.</p> <p></p>  A unidirectional graph. Each edge points one way. Note that edges can also have weights.  <p>Finally, the DAG, which stands for directional acyclical graph, is a unidirectional graph which does not contain cycles. A cycle is not just node A pointing to node B, which points to node A, it can also be node A pointing to node B pointing to node C pointing to node A, and so on an so forth until we have an infinite number of nodes to traverse until we get back to node A again, like going all the way to Mordor just to go back to the friggin Shire. No eagles will save you. You will just have to walk home. As you can imagine this can be a costly property to assert, unless we devise mechanisms to prevent this from happening in the first place.</p> <p></p>  The unidirectional graph is verified as being a DAG through a topological sorting. No edges points backwards.  <p>In the diagram, I have sorted the previous graph topologically. As long as none of the edges go backwards, we have a DAG. In general, if you are reading this, you should try to avoid graphs with cycles. It's a headache and you'll end up down a headscratching rabbit hole. It's also a good source of memory leaks if you haven't implemented your graph or tree in a certain way.</p> <p></p>  A neural network formulated as a computational graph.  <p>Note that formulating a neural network in advance like this, also allows us to perform dimension checking between all layers before running the network.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#trees","title":"Trees","text":"<p>Trees can be seen as a subset of graphs. They can be both bi- and unidirectional. Typically, there is a root node which will point to one or more child nodes. If the tree is bidirectional, the children will be pointing back. Leaf nodes are nodes which are not pointing to any children. Nodes which are not the root, but also not a leaf node are usually called internal nodes.</p> <p></p>  A binary tree where parent nodes point to children nodes, but children nodes don't point back.  <p>Typically, a tree can be really good for sorting data, like getting the biggest value, it can be good for finding things spatially, like, give me all of the nodes in a 3D scene which can be seen by the camera, or give me the closest number to some query. The hierarchical nature of the tree lends itself well to getting approximately <code>log(N)</code> performance in a situation which would typically have <code>N</code> performance. This typically requires that the tree is fairly balanced. Meaning that the maximum length from root node to any leaf node is reasonably close.</p> <p></p>  A balanced and an unbalanced binary tree. Note the sparseness and the differences in minimum and maximum height (distance from root node).  <p>One key difference which makes trees very powerful, compared to the more open definition of graphs, is that we need rules to define what makes a tree. Once we know these explicit rules, we can sometimes take advantage to make implicit assumptions of the structure, which can save quite a lot of space, reduce the amount of indirections we need to follow in order to traverse the structure and make it easier to serialize (write it to a file on disk) the tree.</p> <p></p>  A bidirectional tree. Note if the pointers pointing from children nodes to parent nodes are strong pointers, the tree is rife with cyclical references."},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#binary-trees","title":"Binary Trees","text":"<p>Binary trees are some of the simplest trees. Any node has at most two children. These are usually called <code>left</code> and <code>right</code>. In C and C++, they could be raw pointers or smart pointers, and you would have to check whether they were <code>NULL</code> or <code>nullptr</code> whenever you were considering whether child nodes were available. In Rust, you might have something like <code>Option&lt;Arc&lt;Node&gt;&gt;</code> and you would have to check whether the child was <code>None</code> or <code>Some(child)</code>.</p> <pre><code>struct BinaryNode {\n    payload: i32,\n    left: Option&lt;Arc&lt;BinaryNode&gt;&gt;,\n    parent: Option&lt;Weak&lt;BinaryNode&gt;&gt;,\n    right: Option&lt;Arc&lt;BinaryNode&gt;&gt;,\n}\n</code></pre> <p></p>  A unidirectional binary tree with weak pointers from child to parent. In this case, due to the regular structure of the binary tree, we could have made do with indices.  <p>The baseline definition doesn't go much further than that. But, some variations built on the binary tree, like the heap (not the same as the one we talked about earlier), enforces that the binary tree is sorted and allows you to insert variations. Allowing the min or max value to bubble up, requires a sorting of the tree, but it allows you to very quickly get the minimum or maximum value from a list of nodes. The very predictable structure of the binary tree also allows for easy, memory efficient, implementation using just an array and no pointers. Especially if it is sorted as we need less array elements marked as empty.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#implementing-graphs-and-trees","title":"Implementing Graphs (and Trees)","text":"<p>Implementing graphs is generally considered hard in Rust specifically, which makes sense, because of the many caveats and potential issues in graphs. Dynamic graphs especially are problematic and you should consider very carefully whether all the logic is correct. To make things more difficult, constructing a graph, even if it has to spend the vast majority of its time as a read-only artifact, has to have a construction phase where pointers can be used and you can end up creating cyclical references. Uni-directional DAGs are easier, as long as you don't have to verify their correctness, but if implementing trees where you would like a pointer from the child to the parent, you can use a strong pointer from parent to child, and a weak pointer from child to parent. With graphs, in general, you cannot easily make a constraint that enforces that each node in your graph is only ever pointed to by a single strong pointer. What you can do however, is to contain all of the nodes in a graph object which has a strong reference to every single node, and the connectivity between the nodes being dictated by weak pointers. This will tie the lifetime (when the object is alive and not deallocated) to the containing object. What is unresolved here is how you can then get writeable access to the nodes, which is significantly more complex and I won't go into the details here, as it could easily be its own page. Another thing is... we can do all of this without pointers. We still have to contain all of the graph's nodes in a containing graph object. This object can instead of holding a pointer to every single node and the connectivity being dictated by pointers, just use indices. If you have all of your nodes in a vector without being contained by a unique pointer, the connectivity can just be a list of indices. Node A points to node B and node C. Easy peasy. We do have to trawl which nodes point to which if we want to remove a node, or we can keep an additional connectivity list for node A, specifying all edges pointing to node A, but again, let's just keep to the case where we have a construction phase, and then a reading phase- where lots of actors can read from the graph. In that case, if lots of functions would otherwise pass around pointers to a node, they can just pass around the node index. They can then ask the graph object for access to node N.</p> <p>Finally with trees, if the structure and rules are well defined, we can use implicit rules and just skip connectivity. In the case of the binary search tree, we can simply use an array and the knowledge of its doubling nature. In that case we know index 0 will always be the root. Index 1 will always be the left child, index 2 will always be the right child. To access any node's (index N) children, we merely have to read from index <code>N*2+1</code> for the left child and <code>N*2+2</code> for the right. We can handle a node not being present in this otherwise dense structure, by having a means of representing an empty value, but the greater the sparseness, the more inefficient this linearized tree structure becomes. The implicit/predictable structure makes the linearized treeqv easily serializeable (writing it to a file on disk) or transferable to and useable on GPU's.</p> <p>A better explanation of graphs in Rust and graphs in Rust using indices </p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#octrees","title":"\ud83e\uddec Octrees","text":"<p>Octrees are elevant for all of the specializations that aren't deep learning, especially computer graphics. But it might be relevant for deep learning too if you do stuff related to geometry or spatial data.</p> <p>Octrees are mostly concerned with sorting space. For every node, there are 8 children. If it is sparse, there are up to 8 children. What cannot change however, is the regular structure. Every node covers a certain space. The space covered by the child nodes are strictly within this space and are halved on each axis based on the center point of the parent node. Child 0 would be the eighth of space with <code>x</code>, <code>y</code> and <code>z</code> starting from the minimum value of the parent space up to the center point of the parent space. Child 1 could be the eighth of space the same as child 0, except with the x range starting from the midpoint's <code>x</code> value, going to the maximum <code>x</code> value of the parent space. So on and so forth, all child nodes gets an eighth of space. But again, there doesn't need to be exactly 8 active children, they do all need to go into predictable slots. If the definition of child 0 is what I wrote earlier, that range ALWAYS needs to reside in child 0. It cannot be moved to other children or other slots. One nice property of the octree is that we can describe any path from root to leaf by a sequence numbers from 0 to 7.</p> <p>Now let's talk about payloads. A typical use case within graphics is to use an octree to reason about which scene geometry to render or to use for nearest neighbor queries. Let's start with the simpler payload, point clouds. We have a list of three dimensional points. We want to find the nearest one relative to our query point. This is quite useful for algorithms like ICP. We start with the whole array of points and then continually go through our points sending them to one of the 8 children until a child receives only a single point, at which point that child node becomes a leaf node. Once the octree is built we can traverse the tree keeping track of which points have been closest so far. There is one issues though, given a query point Q, we might have a current closest point A, found in cell 0. The euclidean distance between point Q and point A might be 350. That is great so far. But right on the other side of the spatial divide in cell 7, there is another point, point B, with a distance to point Q which is only, let's say, 42 units from point Q. We only find that point if we continually search all relevant cells to point Q within some cell distance, e.g. if we know point Q is contained by some cell, we always need to examine the neighboring cells. But just the neighboring cells. We still need to compare our point Q against a good number of points, but it is way less than the potentially hundreds of millions of points.</p> <p></p>  The blue star is our query point. The full arrow line is towards the closest point, if we do not search the neighbors.  <p>For nearest neighbor queries having a single point per leaf node is wildly inefficient though, and you should consider fattening up the leaf nodes to contain more points, and in some cases points in the interior nodes as well. These could be efficiently searched by sorting them into linearized octrees. More on those in a future module. Quite often a point is not much more than 4x32-bits, in which case it is wildly inefficent to have more than 1 pointer per node. You might also end up with a stack overflow if you try to build the octree recursively. Last time I tried that in C++ I got a stack overflow at a depth of 1000. If you absolutely need a pointer based tree, try to add nodes of interest to a queue instead and just process that queue. E.g. you arrive at node X, it has 8 children. You deem 4 of them to be of interest, add all 4 to a processing queue, then dequeue the next node for you to process. This might take you all over the tree though. Another option could be using a stack. For spatially larger payloads, like meshes, you might also need to keep a reference to that geometry across more than one node, ending up with some geometry being evaluated more than once. You win some, you lose some. But it's all the same to me. It's the eighth of space.</p> <p>Another use case where the octree is very useful is when deciding what to render and at what level-of-detail. It also makes for a useful abstraction over virtualized geometry. More on that in a later module.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-garbage-collectors","title":"3\ufe0f\u20e3 Garbage collectors","text":"<p>Garbage collection is a way of freeing the programmer of having to deal with which memory is and isn't relevant. It is usually implemented by most variables (especially the heap allocated ones) being reference counted or otherwise tracked, which we will see later in the tracing section. Once a variable is found to no longer be referenced it is either immediately cleaned up or cleaned up during a garbage collection pass. A full garbage collection pass can be quite expensive, and if the implementation is not particularly optimized, lock the whole system while it is being performed as to not have memory that has just been cleaned up referenced anew.</p> <p>Garbage collectors aren't really that relevant to the rest of the guide, but if you are coming from Python, C#, Go or Java this section will use some of the concepts previously introduced on this page to give you a quick perspective to how garbage collectors work. This post takes a look at how python handles garbage collection although, a bit light on the details for the generational garbage collection. In the following sections I will introduce three different types of garbage collectors, and finally set you up with a few tricks for working with the garbage collector.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#reference-counted-garbage-collectors","title":"Reference Counted Garbage Collectors","text":"<p>Reference counting garbage collection is one of the simplest forms of dealing with garbage collection. Imagine that there is an <code>Rc&lt;T&gt;</code>, like we saw earlier, wrapped around every heap-allocated variable. Once the amount of references reaches 0, the object is deallocated. Simple, can be handled locally, scales well, doesn't burden the entire system with a lockdown to clean up, which makes it good for real-time systems which need to be responsive at all times and not have noticeable freezes. What makes it not quite usable is, that it is up to the programmer to not create cyclical references. Node A and Node B cannot refer to each other without causing a memory leak, despite not being referenced by anything else. They cannot be cleaned, unless one of the references is a weak reference. Just like the <code>Weak&lt;T&gt;</code> type we saw in the smart pointer section. But it is up to the programmer to make sure that the weak references are used correctly throughout the system, which isn't necessarily non-trivial.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#tracing-garbage-collectors","title":"Tracing Garbage Collectors","text":"<p>Tracing garbage collection on the other hand follows every root, this could for example be the variable holding the pointer to the root node of your graph, if there even is such a thing, and then following every pointer making sure to mark all the objects it finds along the way as not being ready for clean-up. This does however require that all the memory is frozen. There can't all of a sudden be new references to some of the objects or some of them be removed. Once the marking process has completed, all of the objects are traversed and every object not marked is cleaned up.</p> <p>Another more sophisticated method, promises better performance by using a white, gray and black marking. All objects start marked as white, and are then moved to grey, and then finally to black. Objects marked in white are possibly accessible from roots and are candidates for collection. Gray objects are definitely accessible from roots and might have pointers to objects marked in white. Black marked objects are definitely accessible from roots and definitely do not have pointers to the white set.</p> <p>You can read more about tri-color marking here.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#generational-garbage-collection","title":"Generational Garbage Collection","text":"<p>Generational garbage collection is a different technique which sequesters allocated objects into different memory regions. These regions, usually 3, are based on the age of the object. If an object survives a garbage collection pass it is promoted from one region to the next, older region. The youngest region will usually be significantly larger than the two older regions and it is estimated that most garbage collection will happen in the youngest region. This strategy might not find all unreachable objects, however, and can be supplemented by an occasional expensive full mark-and-sweep to ensure that no memory leaks go undetected for too long. For more on generational garbage collection .</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#3-virtualized-memory-hierarchy","title":"3\ufe0f\u20e3 Virtualized Memory Hierarchy","text":"<p>A simplified definition of virtualized memory is a single address space that doesn't correspond 1-to-1 to physical memory. As we have seen earlier in jagged arrays and permuted arrays, if we have all of our data in memory the caches, compiler and branch prediction take care of hiding memory access latencies, quite a bit, however, what if we don't have all of our data in main memory?</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualized-memory-and-operating-systems","title":"Virtualized Memory and Operating Systems","text":"<p>The operating system itself can, and will, virtualize your memory. It may at some point decide to spare the main memory, probably because it doesn't have any more, and instead allocates temporary space on the disk to swap in and out of main memory. This is painfully slow, but happens seamlessly behind the scenes to be able to continue to allocate more memory for your program. The programmer does not have to do anything as the virtualization is hidden. Usually, there will be hardware support for the virtualization with components such as a dedicated memory management unit.</p> <p>Each process, your program would be its own process, is given its own virtual memory space. Meaning that your program might see its addresses start in very low numbers despite a number of other processes running concurrently on your computer. In face, while the address space given to your process might look continuous it is probably fragmented, scattered across diffent physical locations, but the virtualization makes it appear continuous. In general, it is a major security risk for programs to read memory outside of the memory allocated for it. This is also known as a segmentation fault. The operating system dislikes this concept so much that it is likely to just kill your program entirely. If you have ever programmed C or C++, you have probably tried this making this mistake and your error has been met with swift and uncompromising reprisals. The virtual memory space allocated for your process, for stuff like heap and stack will typically look as below.</p> <p></p>  The stack and the heap sharing memory in their own virtual address space.  Image credit."},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualizing-your-own-application","title":"Virtualizing Your Own Application","text":"<p>As I just described in the preceding virtualized memory section, the operating system will store temporary data on the disk if it runs out of space in main memory, keep track of what is not in memory and in disk instead and, when needed, invisibly load requested data into memory while swapping some other piece of data unto the disk. But we can make our own virtualized memory too! We could for example have a dataset for training a neural network that is astronomically big. Terabytes even! We have for some reason decided it is of the utmost importance that we always random sample the entire dataset. So we randomly pick 20 samples. 4 were already on the GPU, 2 were in main memory, 4 were on disk and the remaining samples are on the internet. It will be slow as all hell, but that is something we can optimize too. The easiest would of course be to limit how often we decide to sample the parts that are on the internet. We could for example choose to download a random block from the internet portion of our virtualized memory, random sample from that block for a while and then download a new block. We could hide this by defining data object structs for each sample which have an optional payload, along with a bit of additional bookkeeping. We could make a sort of address space by keeping the list of samples, which need to be A LOT smaller than the total data set for this to work, and using heuristics on this list of samples and associated metadata to optimize our virtualized data set. We could give a priority to each of the samples based on how long ago they were sampled last and in which block they were located, on which physical memory they were located (the cloud is just someone else's computer). Optimizing these types of systems can be quite a fun algorithms and systems optimization process. For more on working with data outside of your computers memory, like on disk, see the first three weeks of Algorithmic Techniques for Modern Data Models by Inge Li G\u00f8rtz, Eva Rotenberg and Philip Bille.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#virtualized-rendering","title":"\ud83e\uddec Virtualized Rendering","text":"<p>Another use of this is the rendering of data sets too large to fit in a users computer. You preprocess all of the data you need to visualize into a tree structure and then just keep the tree in memory at all times. You can then render render progressively, which is where as soon as the camera stands still you render across multiple frames into the same uncleared buffers, letting the user see progress while the system downloads, unpacks and renders the entire scene. This also allows for rendering of scenes which are too big to fit in GPU memory or even main memory.</p>"},{"location":"m1_memory_hierarchies/s0_soft_memory_hierarchies/#additional-reading","title":"Additional Reading","text":"<p>An explanation of memory allocation, stack and heap in C.</p> <p>A more rigorous explanation of the register, cache, main memory and virtual memory parts of the memory hierarchy. For even more virtual memory.</p> <p>Check out the memory and cache specs for Apple's M1 series.</p> <p>For an example of coding a tri-color marking garbage collector.</p> <p>For more about garbage collection in Python, more basic garbage collection in Pyton or garbage collection in Java.</p> <p>For more on implementing a heap with an array, priority queues, binary trees, binary trees using arrays in Python. These pages have implementation details in C/C++/Python.</p> <p>If you are into spatial data structures and/or graphics, computer vision, etc here's some links for octrees, BVHs, Kd-Trees, a comparison between kD tree and octree, levels-of-detail for point clouds (chapter 3) and levels-of-detail for meshes.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/","title":"2\ufe0f\u20e3 Computational Graphs","text":"<p>Conveying the importance of computational graphs to people who were probably using Python to program neural networks was the motivation for doing this whole thing in the first place. So now let's get to business.</p> <p>Computational graphs are more or less a way to communicate the flow of your program. It can allow a library or a framework to keep data at various levels of the memory hierarchy. It can allow it to check that all of the dimensions fit for the data, it can make assumptions about fusing nodes (combining them), remove redundancies and unused elements.  </p> <p>Let's take a look at this defined network from PyTorch's own documentation.</p> <pre><code>class Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n      self.fc1 = nn.Linear(9216, 128)\n      self.fc2 = nn.Linear(128, 10)\n\n    # x represents our data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = F.relu(x)\n\n      x = self.conv2(x)\n      x = F.relu(x)\n\n      # Run max pooling over x\n      x = F.max_pool2d(x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = torch.flatten(x, 1)\n      # Pass data through ``fc1``\n      x = self.fc1(x)\n      x = F.relu(x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = F.log_softmax(x, dim=1)\n      return output\n</code></pre> <p>In PyTorch the user does not fully create a graph, but if the user makes sure <code>x</code> is on the GPU by calling <code>x.to_device()</code> all of the functions will be executed on the GPU until the output is transferred back to the CPU. This reactive paradigm might be part of why the first complete iteration of a PyTorch training loop will be signifcantly slower than the subsequent loop. Not to mention all of the allocations behind the scenes for backpropagation.</p> <p>If you use <code>torch.compile()</code> it will do something called tracing behind the scenes. You don't need to worry about the specifics, but just know that it creates a computational graph from the Python code above and then optimizes that code to run faster and/or use less memory.</p> <p>So why does it need the graph? That is something the rest of this module will try to answer, along with a really basic introduction to fusion, where layers are combined to be more efficient.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#what-is-a-graph","title":"What is a Graph?","text":"<p>A graph is a type of structure, for our needs, a data structure. In s0 there is a more elaborate examination of the concept for 3\ufe0f\u20e3. So for right now, just give the link to graph's wiki page a quick look. You should get the gist just by looking at a few of the images.</p> <p>Once you have done this, just know, that the rest of this module won't actually use a real graph. The graph we will concern ourselves with will be defined as being one way, unidirectional, and each node can point to at most one other node. This reduces the entire graph to just being a list of operations which will be executed sequentially.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#the-network-we-want-to-support","title":"The Network We Want to Support","text":"<p>For illustrating how computational graphs can benefit your code we don't really need to support a lot of operators. We need transfers to and from the GPU (eventually), a linear operator (matrix-matrix multiplication followed by an addition), a ReLU operator (single call to a max function with 0) and a softmax operator. The softmax operator is the most complex part, don't worry I will show you some CPU code that is fairly easy to understand. The GPU version gets a bit complicated and is only constructed in a fairly simplistic version.</p> <p></p>  Our minimal computational graph example will only contain these 5 operations.  <p>For the computation graphs we will use, note that due to the constraints of transfer first and last, and softmax next to last, the only difference is the amount of linear and ReLU layers. Eventually, when we look at fusion, new operators will be produced, linear-ReLU and linear-ReLU-softmax. We will only use 32-bit floating point, <code>f32</code> in Rust, as the data type.</p> <p></p>  An example computational graph.  <p>The code for the rest of the module can be found at <code>m1_memory_hierarchies/code/computational_graphs/</code> or online.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#whats-in-a-tensor2d","title":"What's in a Tensor2D?","text":"<p>First of all we are going to start on the CPU. We are going to create a data type which will hold the data our operators consume on the CPU. Let's call it <code>Tensor2D</code>. Our 2D tensor will actually be a simple piece of one dimensional memory under the hood and we will keep track of the number of rows and columns to find out how to access each piece of data. If you are in the root directory for <code>computational_graphs</code> go to <code>src/shared/tensor_2d.rs</code> or online.</p> <p>Start by taking a look at the definition of the <code>Tensor2D</code> struct at the very top. The <code>derive</code> stuff at the top is asking some macros to automatically implement (derive) some traits (interfaces and behavior) automatically. <code>Clone</code> means we can call a Tensor2D element as below -</p> <pre><code>let some_tensor: Tensor2D = Tensor2D::new(0.1, 8, 8);\nlet copy_of_some_tensor: Tensor2D = some_tensor.clone();\n</code></pre> <p>This creates a complete and total copy of <code>some_tensor</code>. If we manipulate or move <code>some_tensor</code>, <code>copy_of_some_tensor</code> will not be affected as they no longer have anything to do with each other.</p> <p>Next, take a look at the <code>new</code> function. In it we create a new <code>Tensor2D</code> by creating a new <code>Vec&lt;f32&gt;</code> with size <code>row_count*column_count</code>. Each element is given a value of <code>scale*index</code>. This is just for testing purposes so I found it useful for this to not be all zeros and not all random numbers. This allows us to verify that the GPU implementations are functionally equivalent to the CPU implementations.</p> <p>We don't need to otherwise relate to the values of <code>row_count</code> and <code>column_count</code>. Even if we implement a two dimensional structure on top of a piece of one dimensional memory, when we are iterating through all elements, such as we do when setting all of the elements to some value or accumulating the sum of all elements we can do away with the two dimensional stuff. Keeping up that illusion unneccesarily induces extra cost in the form of more time and code spent on control flow statements like <code>for-loops</code> and <code>if-statements</code>.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#implementing-operators","title":"Implementing Operators","text":"<p>In this section I will be going through various implementations of the three operators and their fused variants and show benchmarks to show you how big of a performance impact these sort of first guess optimizations can have even without profiling or microoptimizations.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#linear","title":"Linear","text":"<p>There's some dimension checking functions, you can just ignore those. They use <code>debug_assert</code> statements to raise an error if the dimensions of the tensors given to a linear layer function don't match. <code>debug_assert</code> is the same as an <code>assert</code> statement, except it is only run in debug mode. I did it this way to incur only a small hit to performance. You probably passed the <code>linear_layer</code> function on the way down, it just acts as a wrapper around the <code>linear_layer_preallocated</code> function. If you haven't already allocated a tensor to use as output, it will make one for you. If you do this a lot however, such as in a loop, you should be using the preallocated version to not have memory allocations in your loops.</p> <p>Finally, let's go down to the <code>linear_layer_preallocated</code> function. There are three main sections. One is the call to the <code>debug_assert</code> function from earlier, to check for valid input and output dimensions, the second is the matrix-matrix multiplication which needs three whole for-loops and finally the bias section. Note the use of linearized accesses, if you need a reminder what that is all about, go back to <code>m1::s0::The Vector</code>.</p> <p>It's not too bad, but we could do better, although we won't do more efficient implementations of matrix-matrix multiplication, note that the read accesses of the weights tensor is strided. We could have implemented that some tensors could be transposed, but you get the point. So we have a triple for-loop and a double for-loop in our linear operator. Try to think, based on the contents of the last couple of sections, what would be good first optimizations for this function?</p> <p>While you think about that in the back of your mind, I'll take a quick detour to introduce a very simple but finnicky concept - inlining!</p> <p>Inlining in Rust, and most other languages, is done via an annotation to the compiler. It is usually more of a hint or request than an actual instruction. In Rust it looks like the derivation of traits we saw earlier - <code>#[inline(always)]</code>. In that case it actually is more of a command. There's other variants you can put inside the parantheses like <code>#[inline]</code>, which is more of a suggestion, or <code>#[inline(never)]</code>. Inlining is basically taking all calls to that function and substituting it with the code from the function. This is largely good for very small functions, such as if we made a function for making our linearized array accesses prettier to look at, but for large functions it either does nothing or makes the performance worse. So, in general, unless you were trying to examine the concept of inlining like we are now, you should stick with <code>#[inline]</code> and suggest inlining to the compiler, without demanding it. The compiler is pretty smart and will usually figure out what is best. As you will see in the function <code>linear_layer_preallocated_inline</code>, the function itself is not different in any way.</p> <p>Next up is the function <code>linear_layer_local_accumulation</code>. Now it's memory hierarchy time! While I didn't get rid of the strided memory access of the weights tensor, I removed some of that control flow overhead, by linearizing the bias part. Now it is just a single for-loop, instead of two, because the dimensions of the output and bias tensors match, and there are no transformations to be made. So we get to just iterate through all elements. I also elected to not accumulate the result of each output element directly in the output tensor. Instead I accumulate in a local variable, which will hopefully be kept in a register.</p> <p>Think back! Where is the register located? And why can it be problematic to accumulate the sum in the output tensor?</p> <p>The bias is the same. But that does mean we are writing to the same output element twice. Let's move the bias calculation into the matrix-matrix multiplication loop. <code>linear_layer_optimized</code> moves the bias to just before the writing of the accumulated result to the output tensor. If the bias calculation was a lot larger, this might not be faster. In some cases for highly complex loops, it can instead make sense to separate them into more loops, which is a process called <code>loop fission</code>. I have only used a small subset of loop optimizations, but you can read about more ways of optimzing a loop here.</p> <p>Ok, so try and run the code locally! To begin with go to the file <code>src/lib.rs</code>. Comment out all of the lines within and including the <code>if configuration.compatible_gpu_found {</code> line. Then in your terminal navigate to the root folder, the one containing the <code>src</code> folder, and write <code>cargo run --release</code>. Your computer will now run a bunch of benchmarks relevant to the rest of this section. You can find the output in <code>computational_graphs/outputs/benchmarks/stack</code>. The one that should have been generated on your computer that we want to look at now is called <code>linear_layer_cpu_benchmark_stack.png</code>. If you weren't able to run it locally, don't worry, I got you covered!</p> <p></p>  Benchmarking linear operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Huh, it seems our loop fission didn't really matter in the grand scheme of things! The graph is quite high resolution to allow you to zoom in. The x-axis is the size of the tensors. Only NxN matrices are used. The y-axis is time in nanoseconds averaged over 1000 runs. Note how the lines are piecewise linear. There are two points where all of the lines get quite a bit slower and scale worse with the size of the tensors. Why do you think that is?</p> <p>You guessed it! You are seeing the size of the tensor becoming too big for the different caches! It looks like the last bend happens at 4096 elements. This corresponds to a 64x64 matrix. 4096 elements of 32-bits, or 4 bytes, each corresponds to 16384 bytes, or 16 KB. We have 4 of these tensor structs, so we should have total allocations of 64 KB just for that data and then add in all of the other memory used by the application and everything else running on the laptop at the time. But then again, the sampling is quite sparse. You can try and add more data points and see if you can narrow down the sizes of your caches.</p> <p>This might be a good time to experiment with changing the values in <code>lib.rs</code> for</p> <pre><code>let loop_count: usize = 10;\nlet loop_range: Vec&lt;usize&gt; = (2u32..8u32).map(|x| 2usize.pow(x)).collect();\n</code></pre> <p><code>loop_count</code> is how many measurements are made per data point. <code>loop_range</code> is a vector of matrix sizes. In this case, the first element is 4, so the first measurement will be done with matrices sized 4x4. Currently, it takes a range between 2 and 8 (not including 8) and yields a vector of sizes of 2^N. So 4, 8, 16, 32, 64, 128. If you wanted all values in the range of 0 to 100 you could write</p> <pre><code>let loop_range: Vec&lt;usize&gt; = (0..101).collect();\n</code></pre> <p>You can zoom in on these parts of the graph by modifying <code>lib.rs</code> to just test values in these interesting ranges. Like right around the size of the last bend. Another thing to note is that only the versions of the linear operator that uses local accumulation significantly outperform the naive version. One surprise is that keeping the bias outside of the matrix-matrix loop, is better performing than moving the bias in. Sometimes it really is better to keep things simple. So from now on, the <code>linear_layer_local_accumulation</code> version will be the preferred one.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#relu","title":"ReLU","text":"<p>Next up, we have the ReLU function, which is embarrasingly simple. It returns the maximum of two numbers. One of the numbers is zero. Done.</p> <p>So, move your way down to the <code>relu</code> and <code>relu_preallocated</code> functions. Here I also completely flatten out the two dimensional tensor to save on the amount of time spent on control flow vs. actual computation. But then we get a new variant. The <code>relu_inplace</code> function. Because it is possible to do the ReLU operation directly on the input array, let's try that variant as well and see what the effect is. Finally, we have <code>relu_inplace_inline</code>. This function is definitely small enough that it might benefit from inlining.</p> <p>So now go back into the same folder where you found the linear benchmark output and look at what your computer could crunch out. Alternatively, if you couldn't run it, I got you covered here.</p> <p></p>  Benchmarking ReLU operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Note the huge difference between the naive version and the other ones. Why do you think there is this huge difference?</p> <p>You guessed it! All of the other functions have either preallocated the output matrix, or do the operations inplace. Since the ReLU operation is so simple, it becomes easily dominated by allocation and memory costs. The difference between the preallocated version and the inplace version is not as big, but still substantial enough to warrant the optimization. Inlining on the other hand didn't make a big difference in this case. It is still doing one read and one write after all. Go back and look at the how much was gained by inlining the much more complex linear operator in the previous benchmark! Go on!</p> <p>Inplace operations are also available in PyTorch. The ReLU actually has a flag for the inplace version.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#softmax","title":"Softmax","text":"<p>Now let's look at the softmax operator. It isn't that complicated, except as we'll see when we talk about the GPU version, the max and sum values are actually non-trivial to implement on a GPU using more than a single work group (a group of threads).</p> <p>Anyways, head down to <code>softmax</code> and <code>softmax_preallocated</code>. In <code>softmax_preallocated</code> we have 3 distinct sections. The first section is finding the global maximum value. Once that is found a modified sum reduction is performed using the max value. Finally, the sum and max are used to calculate an offset value which is used to modify all of the values in place. The sum of all the values should now be 1. Note that we only use one for-loop for all of the elements as softmax doesn't care about dimensions. This allows us once again to cut down on control flow overhead. Once again, we create an inplace and an inline-inplace version. Try and look at the code for a second and go through why we can an unproblematic inplace version of softmax.</p> <p>Got it?</p> <p>Finally, checkout the results in your output folder! Or if you couldn't run them locally I have some here -</p> <p></p>  Benchmarking Softmax operator functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As can be seen, the inplace and inline versions beat the naive and preallocated versions by an extremely wide margin. The different between the inplace and inline seems quite small, but the inplace version seems to consistently beat the inline version.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#fused","title":"Fused","text":"<p>Finally, let's see what we can do with combining the operators with fusion! First take a look at the functions <code>linear_layer_local_accumulation_relu</code> and <code>linear_layer_optimized_relu</code>. In these two I combine the linear operator with the ReLU operator. In the local accumulation variant, the bias and ReLU are handled in their own distinct loop (loop fission). In the optimized variant, they are in the same loop as the matrix-matrix multiplication.</p> <p>Now let's add in the softmax operator. Take a look at <code>linear_relu_softmax_fused_fission</code> and <code>linear_relu_softmax_fused</code>. In the fission version, the max value is found in the same loop as the bias and ReLU computation. In fused version, bias, ReLU and max are all moved into the ending of the matrix-matrix multiplication loop.</p> <p>Finally, checkout the results in your output folder! Or if you couldn't run them locally I have some here -</p> <p></p>  Benchmarking fused operators functions on the CPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As can be seen the naive version, which is just successive function calls to linear, ReLU and softmax operators is massively outperformed by the fused linear-relu-softmax operators, with the fissioned version with bias outside of the matrix-matrix loop winning out. Of course the linear-relu versions are the fastest, as they don't do softmax, but between the two of them it is again the version without the bias calculation at the end of the matrix-matrix loop which wins ever so slightly.</p> <p>It's hard to make a general conclusion based on that, without going a lot deeper, but in any case, you should always test and measure! Now, you can either continue on reading the 3\ufe0f\u20e3 material or go to the next section to get a general introduction to GPUs. We will be using the GPU on your laptop, with no CUDA involved, to see if we can make this even faster.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#3-testing-operators","title":"3\ufe0f\u20e3 Testing Operators","text":"<p>All of the operators have been tested to be numerically equivalent. Rust has some nice test facilities to help with this. If you look at the file next to <code>tensor2d.rs</code>, called <code>tensor2d_test.rs</code>, you can see a bunch of tests. This used to be in <code>tensor2d.rs</code>, but I moved it to this file during a clean up, because the file was getting massive.</p> <p>The generic way to run these tests is writing <code>cargo test</code> from the same spot you would otherwise write <code>cargo run --release</code>. This works just fine for all of the CPU based tests. The testing system launches lots of tests in parallel to speed up the amount of time it takes to run the tests. As we will see later, this parallel test launch can create some issues when testing our GPU functions.</p>"},{"location":"m1_memory_hierarchies/s1_computational_graphs/#3-graphs-in-graphicsgpu-programming","title":"\ud83e\uddec3\ufe0f\u20e3 Graphs in Graphics/GPU Programming","text":"<p>Computational graphs are even making their way into the way you can program the GPU! Ways to define computational graphs have been added to DirectX12 and Vulkan. This development seems to be lead by game and graphics workloads being increasingly compute shader driven.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/","title":"2\ufe0f\u20e3 Intro to GPU's","text":"<p>Now, I'll just give you a quick introduction to GPU's as the next section is about immediate mode GPU computation.</p> <p>GPU's are fairly ubiquitous at this point. They started off as purely for graphics, but around 2008, enough researchers had tinkered with workarounds to use them for general computing, that Nvidia put out CUDA, opening up GPU's for more general usage. GPU's still do lots of graphics, but the opaque black box parts are increasingly opened and even graphics API's such as OpenGL, Vulkan, Metal and DirectX have opened up. With modern graphics API's you don't even necessarily need a graphics output to use them. You can just use the pure compute capabilities. This guide won't get into graphics, except for the graphics specialization.</p> <p>Ok, so anyways, GPU's are pretty hot stuff right now as the software support becomes deeper and deeper, the hardware increasingly has hardware support for neural network specific operations and ChatGPT has increased the hype and demand for AI to exasperating levels.</p> <p>You can think of the GPU as an expansion of the memory hierarchies we have been examining earlier. It is not running in lock step, and you have to program more things explicitly, while also changing your mindset about how programming works. Memory transfers to and from the CPU and GPU will be relatively explicit, you have explicit control of a part of the L1 cache, you have to start programming in a warp oriented fashion and if-statements become quite dangerous.</p> <p>If the CPU, with its numerous cores is like a team of highly skilled specialists building a car, sure, they can build an amazing car, they can adapt to changing circumstances quite well, they can act independently, then the GPU is like a factory. Each path and process has to be carefully optimized, they might each only deal with a very small piece each and people have to work in lockstep. But. Their throughput is unmatched.</p> <p>At 3\ufe0f\u20e3 I will go into more detail as to how to actually write GPU code, but the guide is set up using Rust and a GPU API abstraction layer called wgpu. You don't need to understand how it works right now, but it means that you should be able to run all code, including GPU code, on your platform, even if it's made by Apple or AMD.</p> <p>In general, I will be using terminology specific to the compute part of the graphics API's and I will keep to <code>wgpu</code> and <code>wgsl</code> terminology. You might see significant differences in terminology if you follow up this guide with some <code>CUDA</code> programming.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-hardware","title":"GPU Hardware","text":"<p>First off, when dealing with the GPU, you will have to manipulate the GPU from the CPU with commands like \"allocate this much memory\", \"transfer this memory from the CPU to GPU\", \"execute this shader/kernel\" and \"synchronize\". These are all done in whatever language you are writing in on the CPU side, except for the actual program the GPU has to run. This is distinct from the GPU API, some GPU API's even accept shaders written in multiple shading languages, as they can either be transpiled (translated) from one language to another, or they can be compiled to an intermediate representation, such as SPIR-V, which they can then ingest.</p> <p>But once we have built up all of these commands, at least if they are non-blocking, as in the CPU program won't advance until the command has completed, we have to actually submit them to the GPU. We do this with a synchronization. The commands may/may not have already been submitted, but if you call a synchronization function, the CPU-side code will block and wait until any and all submitted commands have executed on the GPU and the GPU sends the all-clear signal in return. Imagine you are at a horse track. You have to give instructions to a jockey on a race horse. You stand on the periphery of the big oval race track. You tell the jockey to make some adjustment and do a lap. The horse first has to accelerate and then once it nears you, it slows down and you can talk again. What would be more efficient was, if you could leave notes for the jockey to pick up whenever he was coming around and the horse could just continue at speed. In some API's the GPU can just be set in motion and then whenever you have a change to the loop it is running, adjust or change. Or you can set work in motion and come back at a later time, checking whether the work might be done.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#transfer","title":"Transfer","text":"<p>When transferring memory, you should have the following model in mind, nothing gets transferred without a staging area. When transferring from CPU to GPU, at least in the CUDA programming model, it will pin an area in memory. That memory won't be movable until it is unpinned. You basically transfer some memory from say, a vector you want transferred to the GPU, to this pinned memory staging area. That pinned memory area means the GPU can work in peace without interruptions. In CUDA, if you don't explicitly do it, CUDA will create a pinned memory area and do it for you. If you do it yourself and optimize this process you are likely to see around 2x improvement in transfer speed. The same thing happens on the GPU, a staging area visible from the CPU is where the transferred memory is stored, and then moved from the controlled area to the rest of GPU memory, where the GPU is free to do what it wants with it, without interruptions and guarantees.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#threads-warps-and-blocks","title":"Threads, Warps and Blocks","text":"<p>Threads are sort of like a CPU core, except a CPU core is a physical entity, whereas a thread is more like a set of variables (think back to the stack and function calls) which is following its own set of instructions. Thread 1 could be running program A with various states in registers and local variables X. It makes a call to something expensive, like a cache-missing memory access. While waiting, thread 1 is swapped for thread 2. Its state is of course saved, but thread 2's program B and state Y are swapped in for it to do some work. This keeps the CPU core itself occupied with work.</p> <p>Threads on a GPU, will usually be executing the SAME program, unless several calls are overlapped, but let's just focus on you having called a single operation. In that case all of your threads will launch, running the same program. They might however, go down different branches (think if-statements!), but this is more expensive on the GPU than the CPU, and should in general be avoided as much as possible. Each thread will have its own local variables. Threads on a GPU are launched in groups. Depending on the platform and the API they will be called something different. In wgpu, which is what we will be using, it is called a workgroup, while in CUDA terminology it is called a warp. On Nvidia GPU's it will be at most 32 threads per workgroup and on AMD it will be at most 64 threads. The \"at most\" might seem a bit weird, but there is something called register pressure. All of the execution units that can run those 32 or 64 threads at the same time, share a lot of the same physical memory, so if your program uses lots of memory, you might have to decrease the amount of threads to have enough memory to run your program.</p> <p>Anyways. Once you decided to write a matrix-matrix multiplication shader, you need to figure out which threads are gonna go where. In that case, I would begin by launching 1 thread for every output element.</p> <p>When programming for a GPU you have some maximum amount of threads you can launch. This is usually defined in three dimensions. Yes! You can define these threads in three dimensions. It doesn't actually have much of an effect, but it makes sense to tailor how you launch threads to your problem area. If you are performing image processing or matrix multiplication, by all means, launch a 2D grid. If you are summing an abitrary list of numbers, a single dimension will probably suffice.</p> <p>So, we should launch a 2D grid, matching the output elements of our problem. Next up, how do know which thread does what work? Each thread will usually begin its program by asking built-in variables, which thread it is. This can be which thread it is within its own workgroup, or it could be globally. Once it knows that, it should usually check whether it is within legal bounds of the problem. We almost always want n^2 threads in our workgroup, and it wouldn't be very flexible if the problem size always had to match exactly. So usually, you should launch too many threads and then have an if-statement following the thread ID calculation. If within acceptable range, do work, otherwise, don't do work.</p> <p>It cannot be assumed that all work groups are running concurrently. The GPU might need to launch waves of work groups because there aren't enough physical execution units. As such, we can only synchronize between threads inside the warp.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#gpu-memory-hierarchy","title":"GPU Memory Hierarchy","text":"<p>The memory hierarchy on a GPU looks a lot like the memory hierarchy on the CPU. Here it is exemplified by the Nvidia H100, which is a very expensive data center GPU and most definitely not the one residing in your laptop. But the bandwidth (how much data per second can be transferred) internally on the card is a lot higher than on the CPU. All of the streaming multiprocessors share the L2 cache and each streaming multiprocessor shares an L1 cache. On Nvidia GPU's the streaming multiprocessor is a number of, in this case 4, units which can each execute a work group, or in Nvidia terminology, a warp.</p> <p></p>  The layout of a H100 GPU. Note that connectivity to the memory (HBM3) is on the left and right sides.  Image credit  <p>Take some time to study these two diagrams and think about how data moves first from the CPU, to the GPU's main memory, then to the L2 cache, then to the streaming multiprocessor which needs its L1 cache until it finally is loaded up into the registers of the 32x4 threads executing on different, but adjacent, segments of the same data.</p> <p></p>  The layout of a single Streaming Multiprocessor. It can execute 4 work groups or warps at a time.  Image credit  <p>The threads accumulate their data into their own registers until they are done and write the result to main memory. The CPU waits for the GPU to be finished, until the GPU is, transfers to the CPU and signals that it is finished.</p> <p>It's not always as clear cut, though. If you are using a laptop, you probably have an integrated graphics card. The CPU and GPU coexist and share the same memory. There may be sections where there is higher bandwidth than just normal CPU-based memory, but overall the integrated GPU has access to the same memory the CPU has. This makes for faster transfers, but probably slower overall computation. This has become quite useful recently with most consumer grade GPU's having around 8 GB of memory and locally run neural networks like diffusion models easily being able to use more than that. A desktop GPU with more than 16GB of RAM would probably still outperform an integrated graphics card with 16GB of RAM available, but it would be very expensive.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-introducing-wgpu-and-wgsl","title":"3\ufe0f\u20e3 Introducing wgpu and wgsl","text":"<p>The guide will for all GPU purposes make use of the graphics library wgpu, but only the compute parts. wgpu is based on the WebGPU spec, which is supposed to be the new web GPU API, as well as not being particularly creative with their naming, the actual support in browsers for WebGPU is nascent. Chrome supports if you fiddle with some settings, but for most systems, especially if you aren't actually running in a browser, wgpu will default to using different, more powerful backends. For example, at the time of writing this, I am using an HP laptop, with an Intel integrated graphics card running Windows 10. Whenver I run a program with wgpu, wgpu tells me it has chosen Vulkan as my current backend. We could of course just write Vulkan, but it would be a bit more complicated, as Vulkan is slightly more low-level than wgpu, but it would also be more powerful. But attaining ultimate performance isn't the purpose of the guide. It's to get as many people as possible started as soon as possible. It has to run on an Apple computer and it has to be easy to install. So, wgpu it is. While any API which has to cover as many platforms as wgpu does will usually be hampered by the lowest common denominator, it is possible to query wgpu for hardware support for various features, such as fp16. While wgpu is still quite new, it has some exciting features on the way, such as a hardware accelerated ray tracing extension.</p> <p>The default shading language (the language you use to write the code the GPU will run) is wgsl, which was defined along with the WebGPU specification. It is possible to use other shading languages, such as glsl and hlsl, which also have more info and general documentation, but because of the increased code complexity in building the files to SPIR-V and then ingesting them, I elected to just use what was simplest.</p> <p>We can add wgpu to a project by going into the <code>Cargo.toml</code> file in the root directory, and under <code>[dependencies]</code> write the line <code>wgpu = \"*\"</code>. It will pull down the latest version of wgpu. You can of course also get a specific version of it, such as <code>wgpu = \"0.16.3\"</code>.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-basic-gpu-programming","title":"3\ufe0f\u20e3 Basic GPU Programming","text":"<p>GPU programming, as has previously been mentioned, has two major elements. Host (CPU) code and device (GPU) code. We'll start off with the basics of the host code and then move on the GPU code. Just enough for you to be able to read the following sections and understand what is going on in this entire module, as it doesn't go into the finer details of GPU programming, but is centered around a GPU-oriented paradigm.</p> <p>The rest of this section will be make use of the code location at <code>m1_memory_hierarchies/code/gpu_add/</code> or online. Make sure to go and actually read the code. It is full of comments! And they're made just for you! If you want to learn more about wgpu you can visit Learn Wgpu.</p> <p>Be sure to read through the code! Do this before you read the rest of this section, which will go into greater detail.</p> <p>Starting in the <code>main</code> function, first we initialize the environment logger with <code>env_logger::init()</code>. This will get us more helpful feedback from wgpu. This should only happen once in your code, so by putting it as the very first line, we should be sure that it shouldn't need to happen anywhere else.</p> <p>Next up, we call <code>pollster::block_on(self_test())</code>. The <code>self_test</code> function, is a function I made, and use elsewhere to make sure your system is compatible and to print the system info so you can see what GPU is being found and what backend is being used. <code>pollster::block_on</code> allows us to call asynchronous code from a normal function. If you don't remember what asynchronous means, just think of it as being non-blocking. Meaning, we can launch an asynchronous function and just continue on to the next line of code. But the way we do this is different depending on whether we are inside a normal function or an <code>async</code> function. An <code>async</code> function definition example - <code>pub async fn self_test() -&gt; bool {</code>.</p> <p>If we are in a normal function and we call an <code>async</code> function, we have to wait for it to complete. As in, block on the function call, which is of course <code>pollster::block_on()</code>. Inside the <code>async</code> function itself it can either block on async function calls by using <code>await</code> - such as <code>let result = async_function().await;</code> or you can store what is known as a future. We could set in motion the loading of a number of files, and then once we were done and actually genuinely NEEDED to use the files for something, <code>await</code> on the future. The <code>async</code> function, when called from a normal function also returns a future, but we can't use <code>.await</code> on it.</p> <pre><code>pub async fn load_four_files(path_a: &amp;str, path_b: &amp;str, path_c: &amp;str, path_d: &amp;str) -&gt; (File, File, File, File) {\n    let file_future_a = load_file_async(path_a);\n    let file_future_b = load_file_async(path_b);\n    let file_future_c = load_file_async(path_c);\n    let file_future_d = load_file_async(path_d);\n\n    let file_a = file_future_a.await; // Block on the future\n    let file_b = file_future_b.await;\n    let file_c = file_future_c.await;\n    let file_d = file_future_d.await;\n\n    (file_a, file_b, file_c, file_d)\n}\n</code></pre> <p>Ok, so why do we need <code>async</code> when dealing with the GPU? In some cases, we don't care about synchronization. We just want to keep transferring data to the GPU as fast as we can get it, the GPU might output to the display or we might get some data transferred back, but if we are doing this in a real-time setting, we might not care to synchronize, as in block, and we just need things when they are ready. Anything to do with gpu - <code>async</code> will be involved. At least in Rust.</p> <p>Let's move on. We set up our CPU-side data. This is a simple vector addition, and I elected to make the data in a way that was easily verifiable as correct for humans. Input A and B are just vectors of 32-bit floats with values equal to their index. The correct result in the output vector should of course be double the index value then.</p> <p>Finally, we call <code>initialize_gpu()</code> and block on it. Let's go into that function!</p> <p>First we get an <code>Instance</code>. The <code>Instance</code> is a wgpu context which we will use to get <code>Adapter</code> and <code>Surface</code>. The <code>Adapter</code> corresponds to your GPU. We specifically request the adapter with high performance. If you are on a system with more than one GPU, such as a laptop with an integrated GPU, which shares memory with the CPU and a more powerful dedicated GPU, it should try to get access to the dedicated GPU. We also request <code>None</code> for <code>compatible_surface</code>. Surfaces are what you would render to if you were doing graphics. Think of an image with extra steps, which you could show on your display. If we don't need to do graphics, not having one is less work. It also means we can run on data center GPU's, which might not even have a display port. So we just get the <code>Adapter</code>. We use the <code>Adapter</code> to get <code>Device</code>, which will be our handle to the GPU from now on. Whereas the <code>Adapter</code> is more of a raw connection, which we can't do much with. The <code>Device</code> is a handle that has some guaranteed features. The <code>Adapter</code> tells us what features we can get. Once those features are guaranteed, it is much easier for wgpu to open up for more functionality with the <code>Device</code>. We actually don't need the <code>Adapter</code> after we get the device, but I keep it around in the GPUHandles for you to tinker around with auto-complete to see what it can do. We do need the <code>Device</code> though. We also need the <code>Queue</code>. The <code>Queue</code> is where we can submit the work we want the GPU to do.</p> <p>Note that when defining our <code>DeviceDescriptor</code> for getting a <code>Device</code> that lives up to our needs our current requested <code>features</code> is <code>wgpu::Features::empty()</code>. We just want the absolute basics. But we could request, or at least see whether we could get them, features like 16-bit floating point support.</p> <p>Now back to the <code>main</code> function!</p> <p>We now have our bundled GPU-related handles. Now we calculate how many threads we need to launch for our problem size. <code>let element_count: usize = 100;</code>, so we need to launch AT LEAST 100 threads if each thread only processes one element of our problem. Which it does, in our simplified case. Given that we would like to fill up our work groups, I have elected to use 32 threads per work group. <code>let block_size: usize = 32;</code>. Given that the register pressure is likely very low for our shader, this should be no problem. Finally, we calculate how many blocks to launch. This simple calculation is found all of the place when doing GPGPU programming. <code>let launch_blocks: u32 = ((element_count + block_size - 1) / block_size) as u32;</code>. The basic premise is that we add one element less than the full work group size and then use integer division to make sure we always have at least as many threads as we need. In the worst case of a work group size of 32, we will have a work group at the very end of the vectors with 31 idle threads.</p> <p>Next up, we compile our shader code <code>add_vectors.wgsl</code> with <code>.create_shader_module()</code>. Compiling shaders is quite expensive, so if you are programming a bigger system than this, you might want to save the compiled code or do it as part of your build step. When running you can load it from disk as needed. Once we have compiled our shader code we can create a compute pipeline with a specific entry point. The entry point is just the function that will be called when dispatching the shader call later on. Once we have a <code>ComputePipeline</code> we can begin doing our bind group layouts. In CUDA you can pass device side pointers to your CUDA functions when dispatching. Or phrased differently, when using CUDA you can pass along memory addresses for buffers you have explicitly allocated on the GPU. When using the graphics APIs the most basic thing to do, if you are not going bindless, which is... well, don't worry about it, is to use bindings. There is a certain amount of bind slots available in a shader depending on the API and perhaps the system. What can be a bit tricky is the binding slot you declare on the CPU for buffer X, has to match the exact binding slot in the shader. E.g. if you bound your input buffer to binding slot 0 on the CPU, it has to be bound to binding slot 0 in your shader code. Additionally, the compiler will complain if you don't use that buffer in the shader. Finally, you can have multiple sets of bindings in the same shader. These are called bind groups and each has N binding slots.</p> <p>When I created the <code>GPUVector</code>s earlier, the <code>new</code> function allocated a storage buffer, which is visible to the shader and transferred the contents of the given vector to the GPU. This can be done more effectively, but it's a nice and easy way to start things off. We don't have to keep track of whether we remembered to transfer our data to the GPU or not, which makes sure we don't use initialized data. In the case of the output vector, we have also allocated a <code>staging_buffer</code> to more explicitly transfer data back to the CPU. This <code>Buffer</code> has also been flagged as readable from the CPU.</p> <p>The <code>storage_buffer</code>s we have created, when creating the <code>GPUVector</code>s from before, can be bound. I add these binding references to a vector and send them to a convenience function <code>create_bind_group()</code>, which binds the array of bindings in order. Do note that we don't specify what can and cannot be done at this step. It was specified at the creation of the <code>storage_buffer</code>s and it will be specificed locally in the binding of the buffers in the shader.</p> <p>Once we have our bindings set up, we create a <code>CommandEncoder</code>, which we get from <code>Device</code>. The command encoder is a buffer of commands. We can add stuff like render and compute operations, they are sort of like a collection of operations and state, and transfer operations. The command encoder needs to be finished, before it is submitted to the queue. Remember the <code>Queue</code> we got earlier? This is what it was for. We submit finished <code>CommandEncoder</code>s to our <code>Queue</code>, which submits the jobs to the GPU. For this specific program we add two commands to the <code>CommandEncoder</code>. We dispatch our compute shader, enclosed in a <code>ComputePass</code> and launch the appropriate number of threads. Note also the <code>label</code> field. This field permeates wgpu usage. It is mostly for debugging. It helps us identify what is causing an issue. Once we have finished our <code>ComputePass</code>, due to it going out of scope, we add a transfer operation. We use the <code>staging_buffer</code> on our <code>output</code> vector, to read the output back to the CPU. Then we finish our <code>CommandEncoder</code> and submit it to the <code>Queue</code>.</p> <p>We then setup a <code>oneshot_channel</code>. Don't worry too much about this. It is a connection which can only be used for sending data once. We map the <code>staging_buffer</code> and send its data using the sender/receiver pair. Once we have done this <code>map_async</code> call, we wait for the GPU to be finish all operations currently in its queue. Once it has finished we block on the receiver. Until the receiver sends the <code>Ok</code> signal, we wait. Once we get it we retrieve the data. This is raw data in bytes, <code>u8</code>, which we recast to the type we know it is, which in this case is <code>f32</code>. We do a bit of clean up, and don't you know it, that's the program!</p> <p></p>  Adding our two vectors, it should be easily verifiable that it is correct.  <p>Maybe now might be a good time to go back to the code and try to run through it again.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-remove-the-loop-where-you-say","title":"3\ufe0f\u20e3 Remove the loop where, you say?","text":"<p>When writing GPU programs, you should usually start writing a CPU-based version. Once that works, you have something to verify your GPU program against. Often the part of your program that you want to offload to the GPU, will have loops. For example, in a vector addition snippet you might have -</p> <pre><code>for index in 0..ouput.len() {\n    output[index] = input_a[index] + input_b[index];\n}\n</code></pre> <p>When transferring your program to a GPU shader, as a way to get comfortable with thinking about this sort of parallelism, you should start with writing a single threaded version on the GPU. You can do this by dispatching a single thread <code>cpass.dispatch_workgroups(1, 1, 1);</code>. It WILL be slower than the CPU version, but it allows you to get all of the transfers and synchronizations out of the way first. Once you have done that, and you have verified that it works, mind you, you can start adding, or rather removing dimensions. You do that by removing one of the for-loops in your code and replacing it with added dimensionality in your shader dispatch. So in your first version of your vector addition shader, it might look like this sketch (don't know if it compiles) -</p> <pre><code>@compute @workgroup_size(32, 1, 1) \nfn main(\n    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n    ) {\n    let thread_id: u32 = global_id.x;\n\n    if (thread_id &lt; 1) {\n        for (var index: u32 = 0u; index &lt; dimensions.element_count; index += 1u) { \n            output[index] = input_a[index] + input_b[index];\n        }\n    }\n}\n</code></pre> <p>When that works, you can begin thinking about how to remove that pesky loop. You do that by removing a dimension in your shader, but adding one in your dispatch and then making accomodations in your shader. We can take that and transform it by instead dispatching more 1D threads: <code>cpass.dispatch_workgroups(launch_blocks, 1, 1);</code>. Then we change the shader to have each thread work on a single element -</p> <pre><code>@compute @workgroup_size(32, 1, 1) \nfn main(\n    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n    ) {\n    let thread_id: u32 = global_id.x;\n\n    if (thread_id &lt; dimensions.element_count) {\n        output[thread_id] = input_a[thread_id] + input_b[thread_id];        \n    }\n}\n</code></pre> <p>If there had been more dimensions we could have continued expanding and removing dimensionality. We can continue until the third dimension, usually you can launch less threads in the third dimension than in the first two. You also have to remember to check whether the thread is outside of the valid range for each dimension. You should always look up your graphics cards and GPU API to see how many threads you can launch. You might have to break it into several passes. It's not actually quite this simple, as, well you remember how we learned stride had a negative impact on performance earlier? Well, that is not quite the same on GPU's.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-coalesced-accessing-and-strides","title":"3\ufe0f\u20e3 Coalesced Accessing and Strides","text":"<p>Because of the way threads and work groups share memory on a GPU, and each thread executing the same line of code at the same time, if thread A calls for memory at indices 0, 1, 2, 3 and thread B, which is right next to it in the same work group, calls for indices 4, 5, 6, 7, they will be asking for two different cache lines at the same time. Imagine the whole work group doing this at the same time. They will all be waiting, while requesting different cache lines. What is normally faster, is if, given a work group size of 32, thread A calls for indices 0, 32, 64 and 96, with thread B calling for indices 1, 33, 65 and 97. This allows for the work group to call for a minimum of cache lines in lock step and each getting a piece of the cache line. This is called coalesced accessing and if you ever say that to a GPGPU programmer, you will see a faint smile on their face. Think of a jigsaw puzzle, where the pieces are slowly being adjusted. Eventually, they all snap into place. All of the pieces fit exactly right.</p> <p>Here's a small example, if we for some reason were intent on turning our vector addition shader into 2D matrix addition, but we were deadset on keeping the thread grid for our dispatch one dimensional we could do something like this -</p> <pre><code>const BLOCK_SIZE: u32 = 32u;\n@compute @workgroup_size(32, 1, 1) \nfn main(\n    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n    ) {\n    let thread_id: u32 = global_id.x;\n\n    if (thread_id &lt; dimensions.first_dimension_count) {\n        for (\n            var index: u32 = thread_id; \n            index &lt; dimensions.second_dimension_count; \n            index += BLOCK_SIZE\n        ) { \n            output[index] = input_a[index] + input_b[index];\n        }\n    }\n}\n</code></pre> <p>Again, not verified/compiled code. But hold on for a second! We have to remember that there are other work groups too. We can't necessarily just stride through the single dimension in the same way. We would be reprocessing elements that had already been processed by a different work group. What we could do instead would be to step along the rows instead.</p> <pre><code>@compute @workgroup_size(32, 1, 1) \nfn main(\n    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,\n    ) {\n    let thread_id: u32 = global_id.x;\n\n    if (thread_id &lt; dimensions.first_dimension_count) {\n        for (\n            var index: u32 = thread_id; \n            index &lt; dimensions.second_dimension_count; \n            index += dimensions.first_dimension_count\n        ) { \n            output[index] = input_a[index] + input_b[index];\n        }\n    }\n}\n</code></pre> <p>In other cases, using a stride of the work group size can work as well. In this case, stepping along the rows made better sense, but keep thinking in these terms, implement different versions and test them! It's the only way to be sure! Once you have made a couple of different versions and done simple timing you can always add in a profiler, m4 has got you covered!</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-divergence-overlap-and-occupancy","title":"3\ufe0f\u20e3 Divergence, Overlap and Occupancy","text":"<p>One statement I tried to sweep under the rug in the last section was - \"each thread in a work group executes in lock step\". It is highly desirable for a work group to have each thread executing in lock step. That is each thread is executing the same line in your program. If you have branches, like if-statements, some threads might execute path A and some threads might execute path B. This will lead to divergence. Divergence will result in the group of threads A executing while group of threads B will wait until A is finished, and then executed. Finally, they might join again.</p> <p></p>  An if-statement causes a work group to diverge into two.  Image credit  <p>As you can imagine, this expands the timeline of executing the code compared to a non-diverging execution. But if you were within a workgroup where all threads take the same branch there wouldn't be an issue. Thankfully, recent hardware takes less of a performance hit when work groups diverge.</p> <p>Once you have most of your work groups not diverging, are you sure your threads aren't just sitting around waiting? Whenever a thread wants to load a piece of data all the way from memory, it can take quite a long while to retrieve. If however, you have dispatched enough work, the threads waiting around for memory can be swapped out for another work group which might be able to do some work, once this work group has time for a nap, like when it is also requesting data from memory, the first work group can be swapped back in, when the requested data has, hopefully, arrived. Without this overlap, GPU programs are likely to seem a lot slower than they need to be. If however, you launch a lot more threads than there are physical execution units, you are likely to see this overlap resulting in higher occupancy. The higher the occupancy the more time a physical execution unit spends on doing actual work and not just stalling until everything is ready. So you can either launch a lot of independent work, or use a lot of elements in your data. Like really big matrices!</p> <p>In machine learning terms, if you have pipelined and made your computational graph relatively independent, you might see a big increase in occupancy by using less layers and make the ones left very wide.</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#3-shared-memory-and-synchronization","title":"3\ufe0f\u20e3 Shared Memory and Synchronization","text":"<p>Just two final pieces are missing before we go back to memory hierarchies. Shared memory and synchronization. GPU's have more programmable pieces of the memory hiearchy, such as sharing directly between threads, sharing between work groups and more, but WGSL just has the primitives for shared memory, which is the only one I will present for you. Shared memory is a programmable section of the L1 cache. If a cache miss, resulting in retrieving data all the way from memory costs 100's of cycles, quite often somewhere around 250-300, accessing data from shared memory costs around 10 cycles. This is very useful if each piece of data is accessed more than once. It could for example be overlaps in convolutions or storing preliminary results in shared memory for the work group to finally reduce the results internally in the workgroup, before one of the threads writes the final result to global memory.</p> <p>Typically using shared memory, you will first see a section where each thread loads one or more pieces of data into shared memory, followed by a synchronization primitive. This synchronization primitive is available in wgsl and is called <code>workgroupBarrier();</code>. It is available in most shader languages, although it will likely be named something else. It is a barrier ensuring that all threads in the workgroup will stall and wait until each thread has signalled that it is ready to proceed. This is very handy when you are loading data into shared memory for reuse between the threads. A small example snippet -</p> <pre><code>var&lt;workgroup&gt; shared_data: array&lt;f32, BLOCK_SIZE&gt;;\n\n@compute @workgroup_size(32, 1, 1) \nfn workgroup_phase(\n    @builtin(workgroup_id) group_id: vec3&lt;u32&gt;, \n    @builtin(local_invocation_id) local_id: vec3&lt;u32&gt;,\n    ) {\n    var tid: u32 = local_id.x;\n    if (group_id.x == 0u) {\n        // In this first section we can use all 32 threads\n        var elements_left: u32 = sum_uniform.block_count;\n        var i: u32 = tid;\n        var sum_value: f32 = 0.0;\n        // How do we handle the odd case?\n        while (BLOCK_SIZE &lt; elements_left) {\n            sum_value += data[i];\n            elements_left -= BLOCK_SIZE;\n            i += BLOCK_SIZE;\n        }\n        if (0u &lt; elements_left) {\n            if(tid &lt; elements_left) {\n                sum_value += data[i];\n            }\n        }\n\n        shared_data[tid] = sum_value;\n        workgroupBarrier();\n    }\n</code></pre> <p>As usual, this code isn't very well tested and there might be some cases where it isn't fully functional, but you can see the primitives for declaring shared memory, accessing it and synchronizing. Now back to the memory hierarchies!</p>"},{"location":"m1_memory_hierarchies/s2_intro_to_gpus/#additional-reading","title":"Additional Reading","text":"<p>The GPU Memory Hierarchy, GPU Memory Hierarchy, GPU Programming, Hopper Architecture In-Depth and GPU architecture and CUDA Programming. The last entry is highly recommended.</p> <p>A slightly more detailed explanation of asynchronous memory transfers for GPUs.</p> <p>If you want to learn more about wgpu, this is the most used tutorial - Learn Wgpu.</p> <p>To learn more about optimzing shaders with shared memory.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/","title":"2\ufe0f\u20e3 Immediate GPU computation","text":"<p>Now let's look at speeding up our operators with GPU, after this we will take a look at building an actual computational graph, both on the CPU and the GPU.</p> <p>The first version of our GPU implementation, will be for immediate mode computation. In this version the behavior will be treated a bit like executing a Python script. Every command will be processed one at a time, no optimizations can be executed outside of a single operation, and all data has to be ready for a new and completely different operation to be executed afterwards. The result of this being that if the system is told to execute a linear operator on some data, it will compile the code needed to run on the GPU, allocate all necessary buffers on the GPU, transfer the needed data, execute the operation, synchronize with the CPU and then transfer back all of the necessary data to the CPU. If that linear operator is then followed by a ReLU operator, it will have to do the whole thing over again. Compile the ReLU code for the GPU, allocate buffers, transfer, execute, transfer back. Then possibly deallocate.</p> <p>This is not a good way to accomplish this. But it is highly flexible and we're gonna do it anyways! Don't worry about the 'how' too much, 3\ufe0f\u20e3 goes into greater detail.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-the-linear-node","title":"Building the Linear Node","text":"<p>Okay, so let's try building the linear operator again, but this time on the GPU! Don't worry too much about the particulars. The setup is quite a bit like what is described in the 3\ufe0f\u20e3 section of m1::s2.</p> <p>There are three central files for this. <code>src::shared::tensor2d_gpu.rs</code>, <code>src::shared::shaders::linear_layer.wgsl</code> and <code>src::immediate::nodes.rs</code>. If you don't have them locally you can them here, here and here respectively.</p> <p>First of all, let's go directly to the shader (GPU) code in <code>linear_layer.wgsl</code>. The version of the two functions we are interested in is <code>main</code>. At the top there is a struct called <code>TensorDimensions</code>, it is bound as something called a <code>uniform</code>. The <code>uniform</code> is a struct, it can also just be a single value, which is read only for the duration of our shader. As such all threads can safely keep it entirely in their registers or in whichever cache they have room, without fear of the data getting stale. It also means that each thread will be accessing the same data, and not just overlapping data. But let's stay with the definition. We now have the <code>group</code>, which is 0. A binding group is a set of bindings. There are a limited amount of binding slots available so you could have one set of buffers bound for group 0 and another set of buffers bound for group 1. In the <code>var</code> declaration, you declare how the bound data is used. Uniform is what I just wrote, you don't need to specify whether it is <code>read</code> or <code>read_write</code>, it is always just <code>read</code>. For the more general <code>storage</code>, which is your normal buffer, you can specify whether it is <code>read</code> or <code>read_write</code>. Whether there is a performance impact for one or the other might depend on your system, but the <code>wgsl</code> compiler is likely to complain if you declare <code>var&lt;storage, read_write&gt;</code> without actually writing to it. That's not just a performance thing, that is also good software engineering. Don't give stuff more rights than it needs to. That allows the compiler to give you helpful reminders that, at the very least writing to this buffer wasn't your original declared intention. We have several instances of <code>array&lt;f32&gt;</code> which is the raw data from our <code>Tensor2D</code>s. You can reconstruct the entire <code>Tensor2D</code> by combining this data with the dimension sizes from the <code>TensorDimension</code> struct.</p> <p>So now we get to the code itself. For the matrix-matrix multiplication, the dimensionality of the problem lends itself to a two dimensional solution. So above the <code>main</code> function, which can be named anything by the way, we define that our work group will have a size of 8 on the x-axis and 8 on the y-axis. This changes the way our threads individual ID's are computed. Don't worry about it!</p> <p>Then we define a number of built-in variables we would like to have access too. There are more available than we declare here. These are various ID's such as, what is this threads number inside this work group, which work group is this thread in, out of all threads which number thread is this thread, that sort of thing. These are <code>vec3&lt;u32&gt;</code> because we actually dispatch our GPU programs with three dimensional grids of threads. In the case where we only use a single dimension, such as a sum reduction shader where we don't care about dimensionality, we still launch it three dimensionally, we just set the two last dimensions to one and then ignore them from then on.</p> <p>Next, the thread will calculate the correspondence between its global ID and the element of the output matrix this specific thread will calculate. Then we check whether these specific indices are outside the valid range of indices, remember that we often launch more threads than we need. Then we calculate the linearized index (multiple dimensions collapsed into one) in our one dimensional output array. We declare and initialize a local variable to hold our running result, which will hopefully ensure that we keep it in a register. Then we just loop through the input row and weight column, multiplying and adding, until we have accumulated our result, add our bias and then store the result in the output tensor.</p> <p>Given the immediate mode usage, we will always be paying for a full transfer to and from the GPU. This implementation of the linear operator is quite suboptimal, it has been left as an optional 4\ufe0f\u20e3 exercise to implement a more optimal version using tiling and shared memory.</p> <p>If I get the time, at some point I might put up a performance benchmark comparing it to the CPU, but I have one later on for the more complex case of handling arbitrary graphs, which is a bit more representative of the real use case. Just know that immediate mode is highly suboptimal.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-relu","title":"Building ReLU","text":"<p>We then implement ReLU, Softmax and the fused operators in the same way. ReLU you can just check out yourself in <code>shaders::relu.wgsl</code> or online along with an inline implementation in <code>shaders::relu_inline.wgsl</code> or here .</p> <p></p>  Benchmarking ReLU operators on the GPU. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Interestingly, when done this way, the inline version is significantly slower than the normal, more functional, version.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-softmax","title":"Building Softmax","text":"<p>Next up, we have the softmax operator. You will find the three shaders needed for the softmax operator in <code>shaders::softmax.wgsl</code> or online . In this case, finding and communicating the maximum value and the sum is a lot more complicated on a GPU. The implementation provided is not even using all the possible threads, but just a single work group to make the code more readable. Implementing a tree reduction with iterative calls to max and sum shaders is left as a 4\ufe0f\u20e3 exercise. So you don't need to know what that is right now, just know that it is not just implemented suboptimally, but even without more than 32 threads. I did however cheat a little bit and use shared memory, to make it a bit faster. Don't worry about shared memory, I will introduce it in 3\ufe0f\u20e3.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#building-fused-operators","title":"Building Fused Operators","text":"<p>Finally, the fused operators are basically implemented through doing a single transfer to and from, and calling the kernels and bindings in succession. This is done CPU-side and there are no unique shaders for them. Again, don't worry about the stuff happening CPU side. Just know that it is implemented slighty suboptimally, and these shaders aren't implemented optimally.</p> <p>The only really interesting of the performance benchmarks, as we don't have many different implementations of each operator is the fused ones.</p> <p></p>  Benchmarking fused operators on the GPU. If there is no underscore between the operators they were fused. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, using fused operators, especially when we are in immediate mode, save us a whole bunch of performance, having 2 transfers instead of 6 makes a big difference. The fully fused operator wins by a large margin. Now, let's start building some graphs!</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#3-setting-up-to-dispatch-multiple-shaders","title":"3\ufe0f\u20e3 Setting up to Dispatch Multiple Shaders","text":"<p>If you actually delve into how these immediate mode operators are implemented on the CPU side, go to <code>src::immediate::nodes.rs</code>. The code is almost the exact same as in the <code>add_gpu</code> example from <code>m1::s2</code>, except now we will dispatch several shaders in a row, sharing data. Especially the softmax and fused operators will be significantly different on the CPU (host) side.</p>"},{"location":"m1_memory_hierarchies/s3_immediate_gpu_computation/#3-caching-shaders","title":"3\ufe0f\u20e3 Caching Shaders","text":"<p>One other thing that is fundamentally suboptimal about this, is that we compile the shaders for every operator, every time we use the operator. If you do lots of small matrix calls, this will incur a significant overhead. Shader compilation is expensive. What you could do instead is to cache your compiled shaders for reuse later on. This is done with the graph implementations, but it has been left as an optional 4\ufe0f\u20e3 exercise for you to implement this for immediate mode operations. This works just fine when you have 4-10 shaders to compile and keep track of, but what if you had in the 1000's of combinations? In that case you might need some form of cache eviction mechanism, such as LRU.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/","title":"2\ufe0f\u20e3 Building a Computational Graph","text":"<p>Ok, so now we have the basic building blocks ready. We also have a very simplifying set of constraints. This allows us to just represent this graph as a straight line. So we can just represent our graph as a series of nodes in a list. This list needs to be runnable on both the CPU and the GPU, so we'll look at how we can make a CPU graph runner and a GPU graph runner which can interpret the same list of commands and still work just fine. First we are going to do this on the CPU, then the GPU. Then we are going to run the GPU graph in a loop instead of reconstructing it with every iteration. This also the part where we really start to let loose, creating graphs of different sizes, creating arbitrary permutations of linear and ReLU nodes and seeing what the benchmarks can show us.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-the-cpu-graph","title":"Building the CPU Graph","text":"<p>The CPU version of the computational graph is found in <code>src::graph::nodes.rs</code>, <code>src::graph::graph_runner.rs</code>, <code>src::graph::graph_validation.rs</code> and <code>src::shared::graph_operators.rs</code>. First of all, a graph is given as a <code>Vec&lt;GraphOperator&gt;</code> whether it is headed for the CPU or the GPU. It can be found in <code>src::shared::graph_operators.rs</code>. <code>GraphOperator</code> is an enum and has all of the operators we are used to, <code>Linear</code>, <code>ReLU</code>, <code>Softmax</code>, <code>LinearReLUFused</code> and <code>LinearReLUSoftmaxFused</code>. Additionally, it has <code>HostToDevice</code> and <code>DeviceToHost</code>. But hold up, you might think, the CPU part of the code shouldn't do any transfers to device (GPU) or back, well, I'll get to that in a second. In benchmarking we will define this <code>Vec&lt;GraphOperator&gt;</code> the same for both CPU and GPU. For this to work, the list has to live up to the highest minimum requirements, which is the GPU in this case. Each graph runner (that's just a term I came up with, I don't know what the standard term is) has to interpret that list and translate it to its own set of instructions.</p> <p>In general, it is a way to sequester state by changing the type from layer to layer in your pipeline. Imagine a physics engine. Sure, you could just represent everything as a float of a high enough precision and \"just name our variables correctly\", which is once again a red flag as our systems should never rely on the absence of human errors, or we could create new types which makes sure that we don't mix up Newtons per second and meters per second. We can even ensure that only certain operations are available for a <code>NewtonsPerSecond</code> type. One of those operations could be a <code>GarbblediGook(&amp;self, meters_per,_second: &amp;MetersPerSecond) -&gt; NewtonMetersPerSecondPerSecond</code> function (sorry, I'm not a physicist), which would nudge us towards correct type usage. Getting back to our graph system, in much the same way, our <code>GraphRunner</code> found in <code>src::graph::graph_runner.rs</code> can take in a <code>Vec&lt;GraphOperator&gt;</code> and output a verified <code>Vec&lt;Node&gt;</code> which contains not only verified data and dimensions, but additional information. So what happens for the <code>HostToDevice</code> and <code>DeviceToHost</code> on the CPU is basically just dimension verification. Just keeping track of which buffer goes where. You can almost think of it as a passthrough operation. It is used when verifying the dimensions and when creating the buffers needed to run the graph, but at run time, encountering an <code>Input</code> or <code>Output</code> operator does nothing.</p> <pre><code>#[derive(Clone, Debug, Eq, Hash, PartialEq)]\npub enum NodeOperator {\n    Input,\n    Output,\n    Transfer,\n    LinearLayer,\n    ReLU,\n    Softmax,\n    LinearReLU,\n    LinearReLUSoftmax,\n}\n\n#[derive(Debug)]\npub struct Node {\n    pub name: String,\n    pub operator: NodeOperator,\n    pub buffer_indices: Vec&lt;usize&gt;,\n}\n</code></pre> <p>In this case I decided, when first verifying and then processing the input vector to generate the requisite buffers, put them in a vector of buffers and for each <code>Node</code> to just carry around the indices of the buffers in our buffer list. This puts some restrictions on how dynamically we can treat our list of buffers. We can't just remove a buffer and move all of the succeeding buffers one slot to the left. <code>buffer_indices</code> is <code>Vec&lt;usize&gt;</code> because each <code>NodeOperator</code> type, has a different amount of buffers it needs to reference. Using indices like this also means we don't need any smart pointers and the transfer of data from one operator to the next is handled by having indices to the same buffer, as well as our graph being executed sequentially, so we don't need to worry about only one access happening to a buffer at a time as only one operator will be running at a time.</p> <p>Ok, so now we've rummaged around a little bit, try to go back to the files relevant to the CPU graph and see what they're doing. Don't worry about understanding the <code>sorted_mutable_references</code> function.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#building-the-gpu-graph","title":"Building the GPU Graph","text":"<p>Ok, so now let's take a look at how the GPU graph does almost the same. Verification, translation and allocation. Start by perusing the relevant files - <code>src::graph::nodes_gpu.rs</code> and <code>graph_runner_gpu.rs</code>. The validation is the exact same.</p> <p>Once again, the GPU graph runner, takes a vector of nodes, it validates the correctness, it translates the nodes to its own intermediate representation (see below), and allocates any and all buffers. It uses the indices to share data between operators.</p> <pre><code>#[derive(Clone, Debug, Eq, Hash, PartialEq)]\npub enum NodeOperatorGPU {\n    HostToDevice,\n    DeviceToHost,\n    DeviceToDevice,\n    LinearLayer,\n    ReLU,\n    Softmax,\n    LinearReLU,\n    LinearReLUSoftmax,\n}\n\n#[derive(Debug)]\npub struct NodeGPU {\n    pub name: String,\n    pub operator: NodeOperatorGPU,\n    pub buffer_indices: Vec&lt;usize&gt;,\n}\n</code></pre> <p>Note the <code>DeviceToDevice</code> operator is in there now. It is just a transfer of data from one operator to the next. It actually does nothing in itself, but it is used by operators to get the previous operators output and it outputs a <code>DeviceToDevice</code> itself to send its output to the next operator. This simplifies our handling of the inputs and outputs and makes the communication between operators explicit instead of depending on implicit communication. This isn't actually that much of an issue with this setup as we know there is only ever one preceding operator and none of the operators writes to more than a single buffer, but the second either of those cases were needed we would have to rewrite it or accept that we had to handle a lot of cases. The <code>DeviceToDevice</code> operator doesn't cost anything when running the graph and you can verify that the runner does nothing when it sees a <code>DeviceToDevice</code> operator.</p> <p>Finally, you can see that for every operator, the necessary bindings are made and commands are generated and enqueued in a command buffer. Once all operators have been added to the queue note that there is not a synchronization, but the commands are submitted to the queue. The synchronization does not happen until the result are retrieved.</p> <p>In the next section, s5, we will look at how a system could take the computational graph and aside from verifying and translating, actually optimize, given the knowledge of the graph. I will also run a benchmark so you can see the differences in our assortment of implementations.</p>"},{"location":"m1_memory_hierarchies/s4_building_a_computational_graph/#3-borrow-checking-a-graph","title":"3\ufe0f\u20e3 Borrow Checking a Graph?","text":"<p>Ok, so what if we actually wanted a more complex, and applicaple in real-world circumstances? We ironically need to deal with the issues that Rust enforces through the borrow checker. If we have one node writing to multiple other nodes, that is fine.</p> <p></p>  One node writing to multiple buffers is fine.  <p></p>  One node reading from multiple buffers is fine. The dashed line is a synchronization barrier. Nodes A and B can write to buffers, node C just has to be sure that nodes A and B are done before reading.  <p></p>  Multiple nodes writing to the same buffer is not fine.  <p>Multiple nodes writing to the same buffer is not correct, unless they either write only to specific sections, such as if concatenation takes place and node A might exclusively write to indices 0-16, with node B exclusively writing to indices 17-32. Either that or we have to use synchronization through mechanisms like atomics (more about that in m2) to ensure that the calculations are correct. This is essentially what the borrow checker enforces in Rust. You either hand off one part of memory to one writer and another part of memory to another writer (slice references) for exclusive access or you use the more expensive synchronization primitives to make the reads and writes to those elements sequential.</p> <p>Unfortunately, <code>wgpu</code> and <code>wgsl</code> don't have a borrow checker, and we have to do that analysis ourselves. Sometimes it can make sense to actually do this contentious writing to a shared buffer anyway, as the synchronized version can be expensive enough that absolute correctness might not be worth the cost. But if you feel you need to introduce this sort of incorrectness to your system, my recommendation is that you make absolutely sure that the incorrectness is confined to as small a pocket of your system as possible.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/","title":"2\ufe0f\u20e3 Building a Computational Graph Compiler","text":"<p>Ok, so we are almost done with this module. Let's put it all together and execute the graphs, except I have three more variations to explain before I present the benchmark graph.</p> <p>Compiling the shaders for each operator is actually quite expensive, so I put a flag on the GPU graph runner which if <code>true</code> will cache all of the compiled shaders for reuse later.</p> <p>Next up, we have the fused variant. This is our own little mini-compiler. This one is for both the CPU and the GPU graph runners. If <code>true</code> the graph runners will replace any instance of <code>Linear/ReLU/Softmax</code> or <code>Linear/ReLU</code> with a fused version. But as softmax can only ever be the final operation and a ReLU the one before the softmax, and thus there will always be at most a single instance of a fused <code>Linear/ReLU/Softmax</code> operator, we will mostly see the effect of <code>Linear/ReLU</code> fused operators. Remember, that for a network of depth N we have N - 2 operators that are randomly either a linear or ReLU operator.</p> <p>Finally, there is the graph loop variant. This variant is not just creating a computational graph, but moves the loop from the measuring function closer into the graph runner itself. One thing to note though is that while we do cut down on some transfers, the implementation is still suboptimal. I will elaborate about why in 3\ufe0f\u20e3. So just take this as an indicator of why you should use computational graphs and computational graph compilers when you can.</p> <p></p>  Benchmarking randomly generated graphs of depth 64 at various tensor sizes across 10 samples. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p></p>  Benchmarking randomly generated graphs of various depths and a size of 256 across 10 samples. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As we can see from the benchmarks, if you have small enough matrices, it can at some point be more efficient to just use the CPU. But the graph running in a loop seems to work to be better over all. If you were a machine learning researcher, these graphs are sort of an overview of why you should use a system that formulates a computational graph, and why you should use optimization on that graph if it is available. The difference in performance you pay for. Either with your time or your budget. So if you use something like PyTorch, do be sure to optimize, and be sure to check whether the <code>torch.compile()</code> function works for your setup. Next up is the parallelism module where you will be introduced, very superficially, to a couple of different concepts in parallelism. Hopefully, this will help you to understand stuff like model distributed parallelism and data distributed parallelism.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-seeing-the-memory-hierarchy","title":"3\ufe0f\u20e3 Seeing the Memory Hierarchy","text":"<p>Ok, so what we just did, with the graphs and the operator fusions and the what not, what did it do in terms of the memory hierarchy? For the CPU, it didn't do a whole lot other than adding another layer of complexity, but let's focus on the GPU. When we went from the immediate mode, to the graph version, it saved us from transferring to and from the GPU for every operation. It also meant that instead of adding each operation to the queue and then waiting for it to complete, we could just add all of the operations of our graph and then wait. So far so good. But what did the fused versions do?</p> <p>When calculating the linear operation, it kept the output in the threads register and applied the ReLU function once the data in register before storing it in the GPU's RAM and dispatching a new compute shader. One of the suggested exercises is to optimize the linear shader. In any case that should involve tiling and shared memory. That would mean that the matrix-matrix multiplication would need the work group to act in coordination and load in tiles of the matrices into shared memory (L1 cache) and synchronize, in order for them to get more out of memory they might otherwise have overlapping accesses of. Finally, when we use the graph loop, we don't actually tell the GPU to run this same queue for N iterations, but add to the queue, submit and wait N times. If we found the correct tools to tell the GPU to run some program for 100 times, it might completely set the GPU free for a while.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-the-results-and-the-caveats","title":"3\ufe0f\u20e3 The Results and the Caveats","text":"<p>Ok, so we just saw some results previously. I am mostly concerned with showing you the relative timings, otherwise I probably wouldn't be benchmarking on a laptop, but there a number of caveats that might make the benchmarks look slightly different. Some of these I might fix if/when I get the time, some are left as potential exercises, as again, this is more of an introduction to a bunch of concepts, and you get the point at this point hopefully. Point.</p> <p>Anyways, the <code>Tensor2DGPU</code> currently allocates a staging buffer, even for the ones that don't need it. If this was made optional, immediate mode computation would do an allocation less, so would all of the other GPU paradigms, but immediate mode is probably the one hurting the most here.</p> <p>Another pain point is the use of the one shot receiver, instead of being a reusable receiver. I tried to change it but I wasn't quite able to debug its behavior. If I get the time this is definitely something I would like to change. This results in the <code>graph_loop</code> versions only actually transferring back to the CPU once. So, fixing this is likely to make the <code>graph_loop</code> versions a bit slower. Not remaking the receiver each time and mapping buffers, but keeping them around might make that part of the process a bit faster.</p> <p>Finally, for the umpteenth time, the max, sum and linear shaders are completely unoptimized. The impact of max and sum are neglible due to only being called once per graph, but optimizing the linear operator would probably see all of the GPU based implementations run significantly faster.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#3-metaprogramming","title":"3\ufe0f\u20e3 Metaprogramming","text":"<p>Finally, I am going to introduce you to another way of representing these operators. Instead of having an operator with a full implementation of each operator and a lot of hardwired rules like, if a linear operator is followed by a ReLU operator, fuse them, you can attain a bit more flexibility by realizing one thing...</p> <p>Programs are just strings!</p> <p>We can decimate all of our neatly written shaders into something called op codes. You start by defining all of the data that goes in, you have a few lines of the thread figuring out its own ID and so on. Peruse the directory <code>src::op_code_compiler::runner.rs</code> or online . This is just a toy example, it didn't make sense to make the whole thing and I won't be benchmarking it since the results will be the exact same as the operator version. Each op code is just a string. Each operator is just a list of op codes. In this op code example we do operator fusion by adding our ReLU op-code to the list.</p> <p>This is sort of like ordering a standard cheese burger at a restaurant that ONLY SERVES BURGERS. You realize that you want pickles. So you can either order an entirely new cheese burger, the kitchen has to make a new one from scratch for this one, or you can order pickles between two buns, which technically qualifies as a burger. This will be delivered quite fast. But its frowned upon in a restaurant to play with your food so you have to eat the pickles as they come. But you do technically get your pickles. Another option is to realize that a burger is just a stack of ingredients, or a list of op codes, and it would be much easier to send the burger back to the kitchen (compilation) for them to just slide in a few pickles. Using op codes makes our code so much more complex, but it allows us a great amount of flexibility. If we found out we were running on a GPU with a much bigger L1 cache, we might change how we handled shared memory programming. If we were doing a render graph with a number of image-based single pixel operations such as tone mapping, changing hues or saturation, we might use op codes to merge these several different calls, keeping the data as close to the registers as possible.</p> <p>Another thing, often done in graphics is to have various defines in your shader code like</p> <pre><code>#ifdef USE_SHARED_MEMORY\n// something with shared memory\n#endif\n</code></pre> <p>Then if at runtime you find out it would be optimal to use shared memory you can merely append a</p> <pre><code>#define USE_SHARED_MEMORY\n</code></pre> <p>at the top of the shader file and then compile. This makes your code less readable, but not as unreadable as fully using op codes.</p>"},{"location":"m1_memory_hierarchies/s5_computational_graph_compilers/#5-additional-reading","title":"5\ufe0f\u20e3 Additional Reading","text":"<p>Fun and hackable tensors in Rust Massively Parallel Fun with GPUs Compute Shader Glossary Torch.fx torch.compile Getting started with PyTorch's compiler </p>"},{"location":"m1_memory_hierarchies/s6_exercises/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Exercises","text":""},{"location":"m1_memory_hierarchies/s6_exercises/#pen-paper-exercises","title":"Pen &amp; Paper Exercises","text":""},{"location":"m1_memory_hierarchies/s6_exercises/#vector-memory","title":"Vector Memory","text":"<p>Write out the stack and heap memory of THIS sequence of vector operations. You can represent unitialized memory with a *. The vector must double its capacity and copy over each element once it gets a push operation while at capacity. The vector must resize to half its size and copy over each active element once it gets a pop operation while at quarter capacity.</p> <ul> <li>Create new vector of capacity 4</li> <li>.push(5)</li> <li>.push(2)</li> <li>.push(1)</li> <li>.pop()</li> <li>.push(42)</li> <li>.push(350)</li> <li>.push(1337)</li> <li>.push(1)</li> <li>.push(2)</li> <li>.push(5)</li> <li>.push(10)</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> <li>.pop()</li> </ul> <p>Barring a few errors here and there this should be a simple exercise. Except. Are you sure you got all of the undefined (*) values right?</p> <p>Which common operation could have replaced the last sequence of .pop() operations in the case where you wouldn't be using the popped values?</p>"},{"location":"m1_memory_hierarchies/s6_exercises/#linearized-indexing","title":"Linearized indexing","text":"<p>You have an array of dimensions <code>MN</code> in a linearized array <code>data</code>, write pseudo code that iterates through all data elements using two for-loops and doubles the value in place.</p> <p>You have an array of dimensions <code>MNK</code> in a linearized array <code>data</code>, write pseudo code that iterates through all data elements using three for-loops and triples the value in place.</p>"},{"location":"m1_memory_hierarchies/s6_exercises/#killing-the-garbage-collector","title":"Killing the garbage collector","text":"Where would you add a pointer to hurt garbage collection the most?  <p>Adding which pointer would result in the most nodes not being properly garbage collected? If the garbage collector implements cycle detection to depth 2 adding which pointer would break it? The nodes can't point to themselves.  </p> <p></p>  How could you make this sort of general graph, with very few restrictions, safe for garbage collection?  <p>In the case of the multiply connected nodes, can you come up with a structural solution which allows us to make arbitrary graphs in a garbage collected setting or safe in a C++/Rust setting?</p>"},{"location":"m1_memory_hierarchies/s6_exercises/#programming","title":"Programming","text":"<p>Extend the computational graph with an inplace operation for the ReLU operator (only for the non-fused ReLU)</p> <p>The following list is sorted by expected complexity - do at least 1</p> <ul> <li>Add reusable buffers to the computational graph system (for the intermediate activations)</li> <li>Implement a shader cached version of the immediate mode GPU operators and add it to the benchmark</li> <li>Implement a version of the linear layer functions which uses shared memory and tiling</li> <li>Change the <code>Tensor2DGPU</code> to have switchable access details on its buffers. It should be able to accomodate some tensors being exclusively read-only. Do you see any performance differences for whether they are read-only or not?</li> <li>Implement the tree reduction version of the sum function and add it to the softmax function. Also compare the single pass and the tree reduction performance graphs. Reference</li> <li>Implement a max pooling operator, as well as fusing with ReLU, in all levels and implement tests</li> <li>Implement a convolution operator, as well as fusing with ReLU, in all levels and implement tests</li> </ul>"},{"location":"m2_concurrency/","title":"1\ufe0f\u20e3 Concurrency","text":"<p>Ok, so in the past module we looked at parallelism in the form of GPU-parallelism. In many ways, I find it to be an easier introduction to the concept of parallelism. Parallelism and concurrency are often used interchangably, but they aren't necessarily the same. Concurrency is when we run several calls at once, but they aren't necessarily running on two different processors. This could for example be the downloading of several files at once. Things are happening in the background, the process doesn't necessarily need to sit and wait for the first file to download and then request the second file for download and so on. Instead it can ask to download all of the files and then wait for all of them to be done, or for the first one to be done so it can begin processing the files.</p> <p>Parallelism on the other hand implies that we are actually running different cores and threads. So far I have introduced parallelism in small pockets inside a function which cannot do anything too complicated. The programs aren't long running and we choose a specific subset of problems to use the GPU for. In this module, I'll mainly introduce you to CPU based parallelism with different mechanisms. In creating longer running CPU-based parallel programs you will likely need to combine a bunch of these mechanisms along with your accrued knowledge of data races, as enforced by the borrow checker in Rust. Additionally, I will introduce a few more concepts in GPU programming in 3\ufe0f\u20e3.</p> <p>Anyways, why do we need parallelism in CPU's? Eventually, the clock frequencies, as in how many times per second a processor can do something, more or less flattened out. We get increased performance by either doing things in a smarter way or by increasing the amount of processors, either through a massive amount of parallelism in an accelerator, such as a GPU or through adding more processors.</p> <p>But parallel programming and parallel-friendly algorithms put a much greater cognitive strain on you, the programmer. The more you learn about parallel programming, the more you will see that the basic components are actually quite simple. The strain lies in thinking about parallelism and who owns what memory at which time. This is critical in not just getting faster programs, but retaining the correctness of your program from before you started parallelizing it.</p>"},{"location":"m2_concurrency/#algorithms-and-systems-design","title":"Algorithms and Systems Design","text":"Amdahl's Law  Image credit.  <p>Amdahl's Law is a fundamental concept in parallelism. Skim the link, but the concept is very simple. If 90% of your program is infinitely parallelizable, you will still be left with a runtime of 10% of the original runtime - if you take parallelization to the absolute limit. But how do you actually gauge which parts of your system are parallelizable? The answer is quite frustrating.</p> <p>It depends.</p> <p>It depends on what type of algorithms are in play in your system, what sort of hardware platform you are running on, it depends on what amount of development time and skill you have available. Sometimes when you think about optimizing your code you might visualize it as explosions and speed, flamethrowers and excess!</p> <p></p>  Witness Concurrency!  Image credit.  <p>But in actuality, working with parallelism takes restraint and consideration. Like a watchmaker placing tiny gears with a pincette. If we look back at the way we constructed computational graphs in <code>m1</code>, we were able to parallelize internally in each node/operator, but if we had very small matrices with a big depth, we would more or less be unable to do any parallelization, as the launching of threads to parallelize the matrices themselves, might cost more than simply having a single thread just handle the whole thing.</p> <p>Some elements in your system you might be able to parallelize lock free, wherein you find a solution without needing synchronization primitives like scopes, barriers, locks or mutexes. You might get away with having no synchronization, as the shared data might be read-only or you might have to use a simple synchronization mechanism with hardware support like atomics. Some parts of your system might be amenable to fine-grained parallelism, such as a matrix-matrix multiplication, whereas other parts might only be amenable to coarse grained parallelism, such as a SLAM system pipelined into 4 stages, thus only being able to utilize 4 threads.</p> <p>All of these put one thing into the center of everything. Can you guess it?</p> <p>Memory!</p> <p>Some ways of accessing memory can seem completely fine when single threaded, but break down under the scrutiny of parallization. Trees can be hard, especially if you also have to modify them. As one researcher found it, a hierarchical hash map performed siginifcantly better for some types of algorithms on the GPU.</p> <p>Once you have the correct CPU based implementation, you should start asking yourself, where is this going to run and how is the memory accessed in order to accomplish what I want to do?</p>"},{"location":"m2_concurrency/#here-be-dragons","title":"Here Be Dragons","text":"<p>Some of the things you have to get used to in concurrency programming is the sudden lack of serialized interactions. Thread 1 won't necessarily execute before thread 8, and the way you debug and verify your code will have to take that into account.</p> <p>Along the way, you will encounter a number of hazards. Especially race hazards are prevalent. The race condition happens when at least one thread is writing while one or more are writing or reading. Typically, these types of bugs can be very hard to find due to some part of your code being serialized once you try to find the bug or due to the multithreading, the execution might be non-deterministic.</p> <p>Take a few minutes to familiarize yourself with race conditions in software and data races here.</p>"},{"location":"m2_concurrency/#platforms","title":"Platforms","text":"<p>When you decide you want to parallelize your application, you almost always have to consider the platform you will be running on. Do you expect to run it on users' laptops, will you have GPUs available, will it be running in a data center with powerful, very expensive GPUs, will you be using an integrated GPU on a laptop, will it run on the edge or in the cloud? I will assume you are running on your own laptop or desktop for following along, but running on multiple data center GPUs seems to be all the rage these days, so I will keep that in mind.</p>"},{"location":"m2_concurrency/#2-rust-and-concurrency","title":"2\ufe0f\u20e3 Rust and Concurrency","text":"<p>Each language has its own implementations of concepts in concurrency, but I will focus on showing you the ones in Rust and WGSL. All of them exist in other languages, but some may for example be more ergonomic to work with for concepts like <code>async</code> or <code>channels</code>. What the other languages do not have is the borrow checker to ensure the validity of your code. Often this results in parallelized Rust code looking or feeling slightly different, as the borrow checker forces you down a certain path. Also Rust has traits, such as <code>Send</code> and <code>Sync</code>, but these are specific to Rust and I have tried to avoid getting too far into traits, so I won't be explaining them. If interested you are most welcome to read about them here. This is mostly relevant if implementing your own types which need to be shared by threads. In most cases, <code>Send</code> and <code>Sync</code> are automatically derived.</p>"},{"location":"m2_concurrency/s0_data_parallelism/","title":"2\ufe0f\u20e3 Data Parallelism","text":"<p>Now let's get into working with parallelism, I won't start from basic to advanced, but from easiest to most difficult to use. The absolute easiest to use, in Rust at least, is data parallelism. Data parallelism partitions the data into segments for each thread. The way we are going to look at it in Rust is very simple as it is a trivial extension of the idiomatic way of writing Rust, as in so trivial you need to import a library and prefix your iterator with <code>par_</code>. The library will take care of the rest for you.  However, as we'll see later, this isn't necessarily the fastest, but if we think about the amount of parallelism available in our system and the nature of our data, we can do something very very simple and easy to make data parallelism the fastest of the easy methods of parallelism. Using data parallelism requires that either there is no overlap between your data elements, or that you make your input read-only and each thread can output to its own segment of an output collection.</p> <p>The next sections will be heavily inspired by the book \"Programming Rust\"'s multiple implementations of Mandelbrot image generation. If you don't know about the Mandelbrot image, you can see what that's all about here! Ok, so I will start off talking about the parallel part of things. First off, lets look at Rayon, which is the easiest way of doing parallelism in Rust.</p> <p>To use Rayon, we just have to formulate our computations as iterators. Under the hood Rayon divvies up the work into chunks and distributes it to a fitting amount of threads. It also does something called work stealing where if one thread is done with its work sooner it gets work from one of the other threads. This is really good for very uneven workloads like generating Mandelbrot images or path tracing. Again, this is the easiest way of doing parallelism in Rust, both because it is such a small change from idiomatic Rust (using iterators) and the cognitive load, if there is no communication between threads, is so very small.</p>"},{"location":"m2_concurrency/s0_data_parallelism/#a-parallel-iterator-benchmark","title":"A Parallel Iterator Benchmark","text":"<p>You can find the code for this section in <code>src/m2_concurrency/code/data_parallelism</code> or online.</p> <p>First we define our data, how many elements we want, and how many iterations we will iterate through the data to do our benchmarks.</p> <pre><code>    let element_count: usize = 10_000_000;\n    let iteration_count: usize = 100;\n\n    let mut data: Vec&lt;f32&gt; = (0..element_count).into_iter().map(|x| x as f32).collect();\n</code></pre> <p>Now let's see what a non-iterator mapping of all the elements would look like.</p> <pre><code>    for _ in 0..iteration_count {\n        for element in &amp;mut data {\n            *element = *element * 3.14;\n        }\n    }\n</code></pre> <p>A key difference with this approach compared to the other two is that we are ensured this mapping happens in-place. The iterator version is here -</p> <pre><code>    for _ in 0..iteration_count {\n        data = data.iter().map(|x| *x * 3.14).collect();\n    }\n</code></pre> <p>Once we have done this reformulation, Rayon is made to be a drop in replacement. We just import the needed Rayon prelude and replace <code>.iter()</code> with <code>.par_iter()</code> or <code>.into_iter()</code> with <code>.into_par_iter()</code>.</p> <pre><code>    for _ in 0..iteration_count {\n        data = data.par_iter().map(|x| *x * 3.14).collect();\n    }\n</code></pre> <p>Finally, for reasons I will explain in a little bit, there is a variant of all three where they instead of a multiplication call this function -</p> <pre><code>#[inline(always)]\nfn map_function(x: f32) -&gt; f32 {\n    let mut x: f32 = x * x * x * x + x * x + x * x / x + x;\n\n    for _ in 0..62 {\n        x = x * 2.0 + 4.0 + 12.0 / 59.0;\n    }\n\n    x\n}\n</code></pre> <p>Now let's run the benchmark.</p> <p></p>  First a simple mapping operation, and then a slightly more complex map function. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Ok, so what actually happened here? For the first three lines, we have an extremely small workload per element. We are very likely just limited by memory bandwidth. The simplest implementation seems to win out. Rayon needs to make sure it has the amount of threads ready and to distribute the work. Think of it like adding a for-loop. It's extra administration. Much like real life, simple processes rarely need complex administration. But once we add a more complex workload the simple for-loop and the iterator seem to converge to the same performance, whereas the <code>.par_iter()</code> from Rayon begins to win out. If we gave each element an even heavier workload, it is likely that the performance gain from Rayon would increase. Personally, I have used Rayon to parallelize a path tracer, starting with a range of all the pixels and then having Rayon distribute the workload of path tracing every pixel. In that case we have a VERY complex workload and I saw an almost linear scaling compared to the amount of threads available. I wouldn't recommend it, but if you want to see a larger system you can check it out here. The parallelization can be found in <code>render_pixel()</code> here and the <code>render()</code> here.</p> <p>So, now that we can conclude that Rayon can be really good and easy to use for some things, let's move on to more explicitly define our own parallel system with, perhaps, longer running threads.</p>"},{"location":"m2_concurrency/s0_data_parallelism/#3-examples-with-rayon","title":"3\ufe0f\u20e3 Examples with Rayon","text":"<p>You can find the code for this section in <code>src/m2_concurrency/code/data_parallelism</code> or online.</p> <p>Ok, so I made two additional examples. There's lots of different adaptors for iterators and I'll just show two. <code>.filter()</code> and <code>.window()</code>. If you go back to the file from earlier and change the bool in the main function to <code>true</code>, it should now run these two 3\ufe0f\u20e3 benchmarks. First off let's look at the filter benchmark. Filter is like map, except it will only emit elements which result in a <code>true</code> evaluation inside the closure. First we generate a vector of random floats with values between 0.0 and 0.1. Then we filter on them using the following lines -</p> <pre><code>    sums += data.iter().filter(|x| if 0.5 &lt; **x { true } else { false }).count();\n</code></pre> <p>If the random number generator used to generate the floats in <code>data</code> is completely uniform, every time we use this filter, the filter should emit half the elements. Then we just count the amount of elements emitted. The parallel version is very similar -</p> <pre><code>    sums += data.par_iter().filter(|x| if 0.5 &lt; **x { true } else { false }).count();\n</code></pre> <p>If we run this benchmark we get the following -</p> <p></p>  Counting all floats greater than 0.5. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, once again, with a limited amount of work, parallelism isn't necessarily the answer to everything. Emitting a new list of elements also requires allocation of more data, which rarely helps multithreaded performance.</p> <p>Next up, I will run a small program to perform convolution. We generate a data vector of random floats. Then we have a number of filters of different sizes, to show the effect of a greater sized filter. To convolve, with a filter of size N, for this example let's say N = 3, for the first output element, we take data element 0, multiply it by filter element 0 and add it to a sum. Then data element 1 times filter element 1, add it to the sum. Data element 2 multiplied by filter element 2. Add it to the sum and then emit/store the output element. Then we move on and do the same for data element 1 times filter element 0 plus data element 2 times filter element 1 and so on. In Rust, it can look like this -</p> <pre><code>    //\n    // Convolution\n    //\n    let element_count: usize = 1920*1080;\n    let iteration_count: usize = 1000;\n    let filter_sizes: Vec&lt;usize&gt; = vec![3, 5, 7, 9, 11, 13, 15];\n\n    println!(\"Running convolution benchmark for {} elements with {} iterations!\", element_count, iteration_count);\n    println!(\"Filter sizes are: {:?}\", filter_sizes);\n    let mut rng: ThreadRng = rand::thread_rng();\n    let data: Vec&lt;f32&gt; = (0..element_count).map(|_| rng.gen_range(0.0..1.0)).collect();\n    let mut filters: Vec&lt;Vec&lt;f32&gt;&gt; = Vec::&lt;Vec&lt;f32&gt;&gt;::new();\n    for size in &amp;filter_sizes {\n        let filter: Vec&lt;f32&gt; = (0..*size).map(|_| rng.gen_range(-1.0..1.0)).collect();\n        filters.push(filter);\n    }\n    // Remove mutability to be sure.\n    let filters: Vec&lt;Vec&lt;f32&gt;&gt; = filters;\n</code></pre> <p>Note that you are completely free to just change around the first three values. You can also try running different filter sizes. Now that we have created our 7 filters we will apply them -</p> <pre><code>    for _ in 0..iteration_count {\n        let filtered: Vec&lt;f32&gt; = data.windows(*size).map(|x| {\n            x.iter().zip(filter).map(|(element, filter)| *element * *filter).sum()\n        } ).collect();\n        filtered.iter().sum::&lt;f32&gt;();\n    }\n</code></pre> <p>or in the parallel version -</p> <pre><code>    for _ in 0..iteration_count {\n        let filtered: Vec&lt;f32&gt; = data.par_windows(*size).map(|x| {\n            x.iter().zip(filter).map(|(element, filter)| *element * *filter).sum()\n        } ).collect();\n        filtered.iter().sum::&lt;f32&gt;();\n    }\n</code></pre> <p>Note that instead of the <code>.iter()</code> adaptor that we would normally use to get an iterator to a single element at a time, we now use <code>.windows()</code>, which takes an argument. This argument is the size of the window. If we for example give it the size 3 it will return elements 0, 1 and 2 for the first iteration, then elements 1, 2 and 3 for the second iteration. This isn't available in a <code>.windows_mut()</code> like <code>.iter_mut()</code>. Why do you think that is?</p> <p></p>  Random convolution on random data. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>In this case, with the greater computational complexity, Rayon easily takes the lead. We do however, still allocate space for a result. If we preallocated an output vector, we might get an even greater performance difference.</p>"},{"location":"m2_concurrency/s10_exercises/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Exercises","text":"<p>Describe the base architecture of the egui-winit-wgpu template. Found in <code>m2_concurrency::code::egui-winit-wgpu-template</code> or online.</p> <p>Which elements are in play? Think back to what you have learned in this and the previous module. Use words and diagrams!</p>"},{"location":"m2_concurrency/s10_exercises/#descriptions","title":"\ud83e\uddec Descriptions","text":"<p>Pick items worth a total of 3 points or more, write an interpretation of each item of at least 10 times the number of points lines. So an item worth 2 points requires a 20 line description.</p> <p>Suggestions for things to talk about:</p> <ul> <li>A description of the proposed solution</li> <li>Which elements you have learned about in <code>m1</code> and <code>m2</code> are at play?</li> <li>What performance implications result from the item?</li> </ul>"},{"location":"m2_concurrency/s10_exercises/#general","title":"General","text":"<ul> <li>1 - Data-oriented design - Entity component systems</li> <li>1 - Array of Structs, Structs of Arrays, Auto-Vectorization</li> <li>1 - Branch Prediction</li> <li>1 - Eytzinger Binary Search</li> <li>2 - A Mandelbrot program served 5 different ways</li> <li>2 - Custom memory allocators</li> <li>2 - SIMD optimization</li> </ul>"},{"location":"m2_concurrency/s10_exercises/#deep-learning","title":"\ud83e\uddec Deep Learning","text":"<ul> <li>1 - PyTorch - Data-Distributed-Parallelism</li> <li>1 - PyTorch - Model-Distributed-Parallelism</li> <li>1 - PyTorch - Optimizing inference</li> <li>2 - Flash Attention</li> <li>2 - Gyro Dropout - MLSys 2022</li> <li>2 - JAX</li> <li>2 - Fast as CHITA: Neural Network Pruning with Combinatorial Optimization</li> </ul>"},{"location":"m2_concurrency/s10_exercises/#computer-graphics","title":"\ud83e\uddec Computer Graphics","text":"<ul> <li>1 - Linearized octrees</li> <li>1 - Hierarchical Frustum Culling</li> <li>2 - Sorting kernels in divergent workloads - Wavefront path tracing</li> <li>2 - Shadertoy</li> <li>2 - Work Graphs in DX12</li> <li>4 - Nanite</li> </ul>"},{"location":"m2_concurrency/s10_exercises/#computer-vision","title":"\ud83e\uddec Computer Vision","text":"<ul> <li>4 - ORB-SLAM - design and a warning about trying to code it</li> </ul>"},{"location":"m2_concurrency/s1_threads/","title":"2\ufe0f\u20e3 Threads","text":"<p>This section is somewhat based on The Book. In the last section we looked at what a parallelization library like Rayon can do. Under the hood, it does quite a lot of stuff. It subdivides a collection, in the examples I showed you, these collections were all <code>Vec&lt;T&gt;</code>, and it then allocates these chunks of the collection to some threads it gets from somewhere. This allows us to create small pockets of parallelism, or if your formulate it slightly differently, even quite long running programs.</p> <p>But what if we wanted to control threads explicitly? Each thread might do something differently, do something long running, only act in reaction to an event or some data being sent, in that case, Rayon might get a bit confusing, and we don't necessarily want the thread to terminate until the very end of our program.</p>"},{"location":"m2_concurrency/s1_threads/#what-is-a-thread","title":"What is a Thread","text":"<p>While a process has it's own virtual memory space and is quite heavyweight, processes can't see each others data very well. A thread is not that. A thread is an execution unit. If your program is launched as a process (heavy), it has a thread, let's call it the main thread. That thread can then launch other threads which are contained within the process. Threads can each act independently, but aren't the same thing as the amount of cores in your CPU. More physical cores on the other hand, do allow you to run multiple threads in parallel, where as a single physical core can only run multiple threads concurrently.</p> <p>I really hope that makes sense. You should probably read it again...</p> <p>Lots of threads are running on your computer at any time. At the time of writing, I can open up Task Manager in Windows 10 and click the Performance tab too see that I have a whopping 4693 threads and 303 processes running. The number keeps changing, it won't stand still. I certainly don't have 4693 cores in my laptop. It is actually 4 cores with 8 logical processors, so if I were to partition my program into N number of threads, I would probably default to suggest the program to use 8 threads, maybe more.</p> <p>These threads can execute, wait for a bit, be swapped in and out for other threads. In our programs we can both have long running threads, which might even be alive for the entire duration of our program, or we can launch threads whenever we have some additional work to be done. Asking the operating system for more threads is a costly affair however, think back to memory allocation, any time we ask the operating system for anything it is going to be expensive. The remedy to this, much like bigger programs keeping memory around to not have it be deallocated, is to either make a thread pool yourself, or for the library/framework/whatever-you-are-using to maintain a thread pool. A thread pool is some data structure which keeps a bunch of threads around, keeping them from either running free or returning to the operating system, to quickly wake them up and give them some work to do, before they can be returned to the thread pool again.</p>"},{"location":"m2_concurrency/s1_threads/#launching-a-thread","title":"Launching a Thread","text":"<p>For this section I have a project ready in <code>src/m2_concurrency/code/threads/</code> or you can find it online.</p> <p>Go to the <code>basic_threading</code> function. In it we launch some threads using the standard libraries <code>thread::spawn()</code> and give it a closure to execute. Each thread, including the main thread will print their name in a loop, waiting for a bit after each print. I get the following output -</p> <p></p>  Naively launching 8 threads along with a main thead and printing three times. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>This is all over the place. Two things to note - the main thread, despite not printing until it has spawned all of the other threads, prints before some of them. Also, we don't have a handle to any of the threads after spawning them. Thus we don't have any means of synchronized execution or anything like that. The threads might still be alive when the main thread is done executing.</p>"},{"location":"m2_concurrency/s1_threads/#joining-wishing-waiting","title":"Joining, Wishing, Waiting","text":"<p>Another thing we could do, is to hold on to each thread handle. That would allow us to make sure that all threads were done before exiting. Go to the function <code>basic_threading_with_termination</code> and see how that can done.</p> <p></p>  Launching 8 threads along with a main thead and printing three times. We keep track of the handles and wait for execution to finish. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>So we keep the handles for the spawned threads, but at the end we do somethin called <code>.join()</code> on each handle. It is one of the simplest synchronization primitives. We just wait for the specific thread we are joining on to finish executing the function it was given. This is a blocking call, so if that thread has gotten into trouble, say, an infinite while-loop, we will be waiting on that thread indefinitely. But, we launched it with a very simple task, so we know we won't be waiting for too long. This is in a way, a manual barrier, except where with a barrier we could have multiple barriers in a threads program, we strictly wait for that thread to execute the given code in its entirety. Once it is finished it either rejoins the thread pool or its execution path joins the main threads execution path.</p> <p>Now, try and answer the following - why do we do the <code>.join()</code> calls in that order, and why is the program correct despite the threads being done with their small programs in random order?</p> <p>Next, go to the function <code>basic_threading_with_scope</code>. In this function I use the library <code>crossbeam</code>. It is one of the defacto standard libraries for parallelism in Rust. To launch the threads I use crossbeams spawner, which lets it keep track of the threads for me. I then proceeed to use a scope. It is just like the scope we normally have in Rust, but for executing threads until the end. This is equivalent to the loop of <code>.join()</code>s we just had.</p> <p></p>  Launching 8 threads along with a main thead and printing three times. We use the ```crossbeam::scope``` to enclose the lifetime of the threads. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Note the scope ends before the main thread starts printing. Using a scope is a more automatic way of keeping track of the lifetimes of our threads.</p> <p>Ok, so now we know a few ways we can launch some threads and wait until they are done executing, in later sections I will get into ways you can share data between threads and synchronize.</p>"},{"location":"m2_concurrency/s1_threads/#3-crossbeam-instead-of-rayon","title":"3\ufe0f\u20e3 Crossbeam Instead of Rayon","text":"<p>For this section I have a project ready in <code>src/m2_concurrency/code/parallelism/</code> or you can find it online. Once again, just like in the <code>data_parallelism</code> section, we are looking at a homogenous workload in two different versions, a VERY simple version that just doubles all numbers in a vector and one that does something slightly more complicated, but it is still very homogenous. Later on you will be asked to look at the presented techniques in a more heterogeneous context where stuff like work stealing has a much bigger effect.</p> <p>I will be looking at what happens if we know more about the problem than our library. We can both help Rayon along to become faster, and use facilities from crossbeam to improve our performance.</p> <p>So what is it we are doing?</p> <p>We are just going to split the workload ourselves. <code>parallelism()</code> contains 2 sections of with a number of benchmarks each. Each benchmark of the different approaches is hidden behind a flag in <code>main()</code>. The data is split into chunks, that is a vector of reference slices of size <code>chunk_size</code>. The last chunk will not have the same size as the others. The first section is the double function and the second section is the slightly more complex functions. Once again, the workloads are completely homogenous. It should take the same amount of time to execute any randomly chosen segment of size N, except the last one. The first benchmark is completely vanilla single threaded execution. The second benchmark is using Rayon to parallelize the execution of chunks. The third benchmark is using Rayon to parallelize execution of the original dataset, with no chunks. This is the normal way we would use Rayon. Finally, the fourth benchmark uses <code>crossbeam::scope()</code> to launch <code>thread_count</code> jobs with low administrative overhead. To enable these bencharks, set the following flags to <code>true</code> in <code>main()</code> - <code>single_thread</code>, <code>rayon</code> and <code>crossbeam_scope</code>. Let's look at the results -</p> <p></p>  Launching as many threads as chunks using ```crossbeam::scope``` to enclose the lifetime of the threads. This is clearly not the way to go, but as the workload becomes bigger, the performance of launching that many threads becomes slightly better. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, once we chunk our data, leaving Rayon with less administration and scheduling work for its work stealing paradigm, it actually gets quite good performance compared to crossbeam.</p> <p>I did not see that coming!</p> <p>In the next sections I will look at other ways of executing these workloads with crossbeam. They just require an introduction to mutexes and atomics first.</p> <p>Finally, try playing around with the amount of chunks, the amount of test iterations and the size of the dataset. I have a firm suspicion that Rayon does better with more iterations as it probably has a thread pool running in the back which will be kept alive across iterations or that Rayon, where we do not specify a number of threads is more free to optimize however it wants.</p>"},{"location":"m2_concurrency/s2_locks/","title":"2\ufe0f\u20e3 Locks","text":"<p>We just took a look at how to launch and distribute some work to multiple threads. But we did some extra work to separate the data into neat chunks, with no overlap between which thread read and wrote to which data. So, how would we ensure correctness and performance (in that order!) if we had data which more than a single thread had access to? This requires working with the shared memory model.</p> <p>As we saw earlier we can easily share read-only data between threads with the atomically reference counted construct in Rust, <code>Arc&lt;T&gt;</code>. But <code>Arc</code> cannot handle if even just one of the threads accessing some shared data needs write access. What we can do is to wrap our data in a lock. Locks can be used for all sorts of things, but this is just one example.</p> <p>Locks come in different shapes and sizes, an often seen variant is the Mutex, short for mutual exclusion. A lock is used to ensure that only one thread can access a given region of code or data at a time. One potential implementation of this could be a lock using a binary semaphore protecting a piece of data. Any thread that wants to modify or read that piece of data will need to try to acquire the lock. If it is a spin lock, the thread will actively wait until it can acquire the lock. Once a thread has acquired a lock it is free to act inside the critical region guarded by the lock. Once it is finished it should release the lock. If this fails to happen, the threads who are waiting to acquire said lock will continue to wait indefinitely. If the thread currently holding the lock is stuck waiting to acquire a different lock, which it also won't ever acquire, one or more threads will be in a <code>deadlock</code>.</p> <p>Any time we use a lock around a small piece of data or any kind of region, we incur a cost. Both in terms of allocating more memory, but also in terms of all of the time spent waiting to acquire and release locks. A lock is effectively a serialization of a region of code. Think back to Amdahl's law.</p> <p>The more threads are vying for a lock the more lock contention in your code. This is of course at odds with the lock overhead. The more locks, the more overhead, but the lower the contention. Dependencies between locks, like a thread needing to hold more than a single lock at a time, increases the chance of incurring a deadlock.</p> <p>I won't go much deeper about locks, you are definitiely welcome to read some of the links, but the basic conclusion is, you should understand what they do, you should be able to use elements from libraries that use them, but if you find yourself thinking you should use a lock for something... reconsider. Be absolutely sure that that is what you need, and if you do, you need to spend some more time thinking about how to implement it and you need to minimize your exposure by minimizing the regions where locks are used.</p>"},{"location":"m2_concurrency/s2_locks/#locks-in-rust","title":"Locks in Rust","text":"<p>I tried to come up with my own explanation for locks in Rust, but this page from The Book is quite good.</p>"},{"location":"m2_concurrency/s2_locks/#3-work-queue","title":"3\ufe0f\u20e3 Work Queue","text":"<p>So let's try to go back to how we could improve the performance of crossbeam when splitting our data into chunks and having threads work on a chunk each. This time we will use a mutex to create a task queue.</p> <p>You can find the code in <code>m2_concurrency::code::parallelism</code> or online.</p> <p>This time you need to set the flag <code>crossbeam_task_queue</code> to true.</p> <p>As you can see, the two added functions have gotten slightly more complex -</p> <pre><code>{\n        let mut total_time: u128 = 0;\n        let now: Instant = Instant::now();\n        for _ in 0..iteration_count {\n            let task_queue = Arc::new(Mutex::new(data_chunks.iter_mut()));\n            let iteration_now: Instant = Instant::now();\n            crossbeam::scope(|spawner| {\n                for _ in 0..thread_count {\n                    let task_queue_handle = Arc::clone(&amp;task_queue);\n                    spawner.spawn(move |_| {\n                        loop {\n                            match {\n                                let mut data = task_queue_handle.lock().unwrap();\n                                data.next()\n                            }\n                            {\n                                None =&gt; { return; }\n                                Some(data_chunk) =&gt; {\n                                    map_function(data_chunk);\n                                }\n                            }\n                        }\n                    });\n                }\n            }).unwrap();\n            total_time += iteration_now.elapsed().as_millis();\n        }\n        let elapsed_time: Duration = now.elapsed();\n        println!(\"{} ms for crossbeam task queue\", elapsed_time.as_millis() as f64);\n        println!(\"{} ms for crossbeam task queue when discounting queue creation\", total_time as f64);\n}\n</code></pre> <p>This time around we are making a task queue. This task queue will be emptied every iteration. The task queue, is an Arc (to share this between threads) wrapped around a mutex (to allow us write access) wrapped around a mutable iterator to allow us to go through the list of tasks. We are using iterations to better benchmark the function. To accomodate this I recreate <code>task_queue</code> every iteration. To showcase the effect of this I also added a timing specifically without the extra cost of the recreating the queue every iteration, which the other solutions aren't burdened with. Next up, we create a scope, which you should be familiar with by now. Inside the scope we give each launched thread a reference to the queue. Once the thread has spawned, each thread will continuously try to get another piece of data to work on, by acquiring the lock. Note that once the thread has acquired the lock and returned the next element from the iterator, the lock is released by the scope ending. If the iterator is empty the thread will jump out of the loop.</p> <p>Now let's look at a benchmark.</p> <p></p>  Crossbeam with a mutex around a task queue. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see there is a cost to recreating the task queue every iteration (the difference between the last two lines) and we now approach the non-fine grained Rayon performance. If we hand tune the parameters for the crossbeam task queue, we might be able to get closer to the coarse-grained Rayon performance. Try to play around with the four values in the main function to better understand when which solution is best under which conditions.</p>"},{"location":"m2_concurrency/s3_atomics/","title":"2\ufe0f\u20e3 Atomics","text":"<p>If a mutex is a horse, an atomic is a pony. Except, like a pony that's much faster than a horse. The smaller a region serialized through synchronization, the more potential concurrency we can have and the less likely things will go wrong. So what is an atomic?</p> <p>Atomic operations are a special kind of very small lock. Atomics are specific to certain data types and certain operations because there is sometimes hardware support for them. Now, whenever you see the term \"hardware support\" you should think \"SPEED\". This means that you can work quite fast using atomics, not as fast as not having any synchronization, but relative to the more cumbersome mutex, it is quite fast. You can even use atomics in GPU programming. One tricky thing can be if your language supports atomics, but your hardware doesn't. In that case the atomic operation will be emulated in software which will make your code inexplicably slower.</p> <p>First off, let's look at the concept of memory ordering. Afterwards I'll describe a few different atomic functions you can use, and then finally look at what atomics are like in WGSL.</p> <p>Anyways, memory ordering.</p> <p>Various hardware platforms have different types of support for atomics. If we for example had a counter variable, it could for example be used to sum an array, we could make sure that the sum was correct while still being manipulated and accessed in a multithreaded setting by placing the counter inside an atomic. The nature of how the counter is incremented is then dictated by the memory ordering.</p> <p>The function that we use for incrementing this summation counter can be called with different orderings. We'll look at three different types, sequentially consistent, acquire-release and relaxed. With sequentially consistent ordering, we can have certain guarantees in terms of which thread is getting to read and write to the atomic variable in which order. This is likely to be the most expensive ordering, but also likely to be the most consistent.</p> <p>Acquire-release is a paired ordering. It is especially good for implementing locks with atomics. If you used a bool or an integer (you could store which number thread currently holds the lock), inside an atomic you could implement a lock for a critical section which could be acquired and released. Having this implemented with atomics should ensure that there can only ever be one thread holding the lock.</p> <p>Relaxed ordering is the cheapest and has no guarantees about ordering. It is especially good for the case of summation. We don't care about which order was used to sum an array of numbers, just get it done. It is the easiest to comprehend, and to begin with I would recommend that you only use atomics in a situation where a relaxed ordering is good enough. Otherwise we get into mutex levels of complexity and you're gonna have a bad time.</p> <p>In terms of types you will be limited to integers and bools. Which types are available in Rust can be found here. Go to the page for <code>AtomicU32</code> and see which functions are available. There's some load, store and swap operations, followed by some compare/exchange operations and then a bunch of fetch and something operations. Try and read through them. In practice you would share the atomic variable between threads by wrapping it with an <code>Arc&lt;Atomic...&gt;</code> and cloning the <code>Arc</code>.</p> <p>WGSL, the language used to make GPU programs in this guide, only supports <code>atomic&lt;T&gt;</code> where <code>T</code> is either <code>i32</code> or <code>u32</code>. All atomic functions have <code>relaxed</code> memory ordering. It's really good for things like reduction operations. Once each work group has a local result computed, they can compare results, or add, across work groups through a global atomic variable. You can check out which functions are available, which is a bit more limited compared to what was available in Rust, here.</p>"},{"location":"m2_concurrency/s3_atomics/#3-lock-free-algorithms","title":"3\ufe0f\u20e3 Lock Free Algorithms","text":"<p>Lock free algorithms are a type of algorithms where there is a minimal amount of synchronization. Think of it as atomic, yes!, mutex/lock, no!, atomic lock, no! This requires a fair amount of thought, but a very simple version could for example to implement a queue of chunks this way. If we have an iterator or an index pointing to an array of data chunks, we can use an atomic operation to increment the index or the iterator and get the former piece of data. This is lock-free. There might some waiting time if 8 threads are all trying to get more data to crunch at the same time, but no locks have to be acquired. But this way of using atomics to acquire a new piece of data from an array is made possible with all these functions which don't just swap, compare or fetch, but also do one more thing. We can increment and fetch to get our new data chunk!</p>"},{"location":"m2_concurrency/s3_atomics/#3-crossbeam-and-atomics","title":"3\ufe0f\u20e3 Crossbeam and Atomics","text":"<p>Back to the storyline! Once again, go to <code>m2_concurrency::code::parallelism</code> or online. Set the <code>crossbeam_atomic_chunks</code> flag to true.</p> <p>So, I pulled in the <code>atomic_chunks_mut</code> crate from the mandelbrot example you will see later. It is a chunks iterator which uses atomics to increment an index. This allows each thread, whenever asking for a new chunk to increment the current index and get the last index value. This is now that threads data chunk. Thus we do almost the same as the one with mutex, but with an atomic and an index.</p> <p></p>  Crossbeam with an atomic chunk queue. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>As you can see, the <code>crossbeam scope</code> section still performs terribly since we are launching 257 threads with a chunk of data each. And while we can get good performance from the atomic chunks, the lower synchronization cost doesn't make much of a difference with the bigger chunk size. For my money, I would say that the best return on investment for your time, is probably experimenting with running either a fine-grained or a coarse-grained Rayon accelerated iterator. The performance is closer to self-tuning and you have less of a headache worrying about synchronization and ownership. You should of course always benchmark and use your own reasoning as to when you need to optimize this.</p> <p>But hold on a minute - I added some values. I separated the element counts into two different values to get the run time up for the simpler double function. When benchmarking, the rule of thumb is you should be running your benchmark for around 2 seconds. This allowed me to up the amount of elements for the double function, but decrease the number of elements for the more complex map function. I also added two more variables. <code>complexity</code> which dictates how many times the loop in the map function will run and <code>escape_probability</code> which is a new feature. Now, whenever <code>escape_probability</code> is greater than 0.0, for every iteration of the loop, a random number will be generated. If that number is less than <code>escape_probability</code> the loop will be broken out of. This to allow you to make the work load less uniform and more like generating a mandelbrot image or rendering an image with path tracing. If we weren't chunking the data and exclusively running fine-grained parallelism this should give a larger advantage to the work stealing techniques (Rayon).</p> <p></p>  Adding more loop iterations while also adding an escape probability. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively.  <p>Now try and spend some time playing around with the variables in <code>main()</code>. Especially stuff like chunk size, escape probability and complexity. In which cases does which techniques get better or worse? Try and think about the types of workloads that are the most relevant to you and to reason about the differences in performance.</p>"},{"location":"m2_concurrency/s3_atomics/#additional-reading","title":"Additional Reading","text":"<p>If you would like to learn more about atomics I can recommend you read about atomics and reordering in the Rustonomicon or in the the atomics chapter in Jon Gjengset's book Rust for Rustaceans which is even better.</p> <p>To read more about lock-free algorithms, go here, and here.</p>"},{"location":"m2_concurrency/s4_message_passing/","title":"2\ufe0f\u20e3 Message Passing","text":"<p>Now let's look at a different way of doing parallelism. What if we didn't share memory at all and just sent it between threads? That would make things so much easier! We will do that with message passing! It might also make things slower, but I will just assume that it is slower than the data parallelism, threads, locks and atomics we just looked at. On the other hand it allows us to work simply while maintaining a very high degree of freedom between threads, allowing us to take that to the extreme with task parallelism.</p> <p>So why is message passing easier to work with than the mutexes and atomics we just looked at? Until now, we have either had a library like Rayon split some shared data into segments which each thread could consume or we have used more or less complex locking mechanisms to synchronize which thread had access to which data. With message passing we instead use some of the things we learned earlier about move semantics. We partition the data somehow and move the data completely to whichever entity will use the data for processing. The processing thread can then return the result through message passing as well. These messages could be anything, they could be a tuple of a command, such such as \"multiply all data by 2\" along with the data to be processed.</p> <p>This is quite a simple way of doing things and we don't have to worry about a single lock. The implementation takes care of that. There are some other caveats, however. The specific version of message passing we will be looking at is called channels in Rust. What happens is that the message is basically moved into a queue from which the receiver can dequeue messages. The Rust's borrow checker would no longer need to worry about who or what owns what, as the data is now fully owned by the receiving thread. But what happens if the queue is full? We can have multiple threads enqueueing messages to the same channel (queue), if the receiving thread is unable to dequeue fast enough, should the channel overwrite the oldest messages in the queue to allow the transmitting threads to move on and no longer block on the transmission, should the enqueued message be dropped/ignored or should the threads wanting to enqueue a message wait around until space opens up in the channel? If you are designing an efficient system using message passing this is absolutely something you should think about.</p> <p>Two potential solutions are the synchronous and the asynchronous channels. With the asynchronous channel the transmitting thread will send the message to the channel and then move on, as in not block, remember blocking behavior? The asynchronous channel then has two methods of handling message overflow. It can either resize the queue (think back to dynamically sized arrays) or begin dropping messages. The synchronous channel on the other hand requires the transmitter to wait until either the message has been successfully transmitted or received, depending on the interpretation.</p> <p>Another hazard is what happens if one side of the channel stops interacting with the channel? If you imagine one thread holding a transmitter to the channel. It sends data every once in a while to be processed. On the other end a thread might have nothing to do but receive messages from the channel and process them. If then the transmitting thread moves on and no longer transmits data to the channel, we need to be aware of that possibility and handle it. It could either be the channel itself communicating that one end of the channel had been dropped or it could be the transmission of an exit message. This only works if it is the transmitting thread moving on. If the receiving thread will move on, or end, it cannot transmit an exit message, unless it has a way to send an exit message to the transmitting thread. The channel itself handling whether both ends are still alive is probably the way to go in most cases.</p> <p>Another main use of message passing is sharing data between computers. If you have multiple computers working on the same problem, they don't have any physical memory to share, so instead they can send messages to each other. This is the basis of the MPI standard.</p>"},{"location":"m2_concurrency/s4_message_passing/#3-rust-channels","title":"3\ufe0f\u20e3 Rust Channels","text":"<p>The first ways to do message passing you will meet in Rust are the <code>mpsc::channel</code> and <code>mpsc::sync_channel</code> structs. <code>mpsc</code> stands for multiple producer, single consumer. When creating an <code>mpsc::channel</code> both a sender and a receiver are returned. The sender can be cloned multiple times, to allow several threads to send messages using the same channel, whereas there can only ever be one receiver.</p> <p>mpsc::channel is the one most often used and is an asynchronous channel. It is what is known as unbounded and if given too many messages will resize to accommodate. A transmitting thread will not block.</p> <p>The mpsc::sync_channel on the other hand is bounded and won't change its size. A transmitting thread will have to block until the message as been sent successfully. This might be best if you are running at high speeds and can't drop packages. You could very quickly accummulate a massive amount of memory.</p>"},{"location":"m2_concurrency/s4_message_passing/#3-real-time-message-passing","title":"3\ufe0f\u20e3 Real-Time Message Passing","text":"<p>I made a code example for you. You can either go to <code>m2_concurrency::code::message_passing</code> or online.</p> <p>In this example I use Rust's <code>mpsc::channel</code> to get two pairs of senders/receivers. The main thread generates some data and sends the <code>Vec&lt;f32&gt;</code> to a processing thread, which performs some in-place computations and sends them back on another channel. The main thread can then receive the result and prints it out. Note that each thread does not block and instead of using <code>.rcv()</code>, which is a blocking receive, uses <code>.try_rcv()</code> and handles the case where there wasn't any new message to receive. In the case of no message being received it also prints.</p> <p>There are three values at the top.</p> <pre><code>    let max_work: usize = 100;\n    let master_wait_time: u64 = 200;\n    let worker_wait_time: u64 = 200;\n</code></pre> <p>Max work is the most amount of work messages will be processed, otherwise the program would run forever. <code>master_wait_time</code> is the amount of miliseconds the master thread will wait after sending a new work message. The <code>worker_wait_time</code> is the amount of time the processing thread will wait before attempting to receive another message.</p> <p>Try and run the code, see what happens!</p> <p>Now try and adjusting the wait times. How does it run when you reduce <code>worker_wait_time</code>. Why do you think that is?</p>"},{"location":"m2_concurrency/s4_message_passing/#additional-reading","title":"Additional Reading","text":"<p>A longer, very friendly introduction to channels and message passing in Rust can be found here.</p> <p>This is slightly larger scale, but when running code on multiple compute nodes in an HPC installation, the most used method for sharing data between nodes is a standard called MPI.</p>"},{"location":"m2_concurrency/s5_async/","title":"2\ufe0f\u20e3 Async","text":"<p>If you went into the code from the memory hierachies module you will have already seen some async operations, which was solely to interact with the GPU, as the WebGPU specification, on which WGPU is based, requires that all interactions are async. This makes sense, as the CPU we are calling it from, does not have direct control of the GPU, but makes requests. Synchronized execution is expensive. Later on, you might see async as part of GUI programming.</p> <p>Where async is most pervasive however, is when interacting with the internet. Getting stuff from the internet is quite slow compared to getting things from RAM, so it makes good sense, if we are sending and receiving lots of requests to make a note of what we did and come back later. This is the core of async. Make a note of what you just asked something to do and either wait now (blocking) until that request has been completed or come back at a later time to see whether your request has completed, perhaps even pick up the results and send them somewhere else.</p> <p>This makes async programming really shine in web servers, handling thousands of requests per second. We wouldn't want to launch thousands of threads, our system would be massively oversubscribed, unless we had at least hundreds of physical cores. Async and futures on the other hand are lightweight and can instead be executed by a handful of threads. Scheduling and keeping track of when work needs to be performed on which futures, the note that a request has been made and what will be the result, will require an async runtime. This runtime can itself be relatively big, so you have to imagine a linear function with a bias of the size of the runtime plus the amount of futures. Big runtime, small futures. Thus a big, if fast runtime, like the very popular tokio, only makes sense if we are in a minimum of 10's of requests per second. Otherwise, use a lighter runtime or whatever comes with Rust by default.</p>"},{"location":"m2_concurrency/s5_async/#3-async-in-rust","title":"3\ufe0f\u20e3 Async in Rust","text":"<p>Async is still relatively new in Rust and is likely to see significant changes. The documentation reflects that, despite there being an async Rust book, it is not complete.</p> <p>In order to call functions denoted <code>async</code> we either need to use a <code>block_on(my_async_function())</code> call in our synchronous code and the call, as surprising as that may be, blocks on the asynchronous function call. An <code>async</code> function will return <code>impl Future&lt;Output = T&gt;</code>. Basically, this means, this is a future returning type <code>T</code>... eventually! Within <code>async</code> functions we are free to call other async functions without using <code>block_on()</code>. Instead each call to an <code>async</code> function returns the <code>Future</code> I mentioned earlier. You can either use <code>.await</code> immmediately like so <code>let result: u8 = my_async_func().await;</code>, or you can store it and <code>.await</code> later, like so</p> <pre><code>    let future_result = my_async_func();\n    let result: u8 = future_result.await;\n</code></pre> <p>If you think back to the earlier page <code>m2::s1</code> about threads, you can imagine the same scenario as threading. You launch a bunch of jobs, store their handles, then when you are done launching jobs, you might even have some other work to do in the mean time, you await all of your handles until you are ready to move on. But, allow me to quote Rust's async book -</p> <p>The most common way to run a Future is to .await it. When .await is called on a Future, it will attempt to run it to completion. If the Future is blocked, it will yield control of the current thread. When more progress can be made, the Future will be picked up by the executor and will resume running, allowing the .await to resolve.</p> <p>if a future, for example representing download of a file, in which case there maybe be multiple other factors than just the system we are in control of, calling <code>.await</code> may result in the current thread yielding. Another thing we could do is the <code>join!()</code> macro. This is sort of like calling <code>.await</code> on a bunch of futures at the same time. Like so -</p> <pre><code>let future_a = download_file_async(url_a);\nlet future_b = download_file_async(url_b);\nlet future_c = download_file_async(url_c);\n\n(file_a, file_b, file_c) = join!(future_a, future_b, future_c);\n\nprintln!(\"Successfully downloaded files from {}, {} and {}, url_a, url_b, url_c);\n</code></pre> <p>You can read more about <code>join!()</code> here. Async is its own paradigm and again, in this course you are most likely to see it when interacting with the GPU and a GUI system. In the real world you might see it very pervasibely in web servers and anything to do with stuff that happens outside of your computer. In any case, you should try to limit how big the async portions of your code are, or very soon all of your code, including your main, will be async.</p>"},{"location":"m2_concurrency/s5_async/#additional-reading","title":"Additional Reading","text":"<p>For an intro to the async functionality available in the core Rust library, there's a book for that.</p> <p>Tokio is a widely used async runtime. It is suggested, by Tokio itself, to not use it for cases where you are CPU compute bound, they suggest Rayon in that case, to not use it for accessing lots of files as the operating systems generally do not have asynchronous file APIs, and to not use it for single web requests. So, when thinking about stuff like web servers, or something else making lots of web requests, Tokio might be the right tool.</p> <p>async-std is a newer library which seeks to act as an async extension of Rust. It is also mostly interesting if you need to handle lots of networking.</p> <p>Green Threads are like virtualized threads, like threads emulated in software, to make them extremely lightweight.</p>"},{"location":"m2_concurrency/s6_events/","title":"2\ufe0f\u20e3 Events","text":"<p>Events are, well, an event. Something happens, and the event is processed somehow, by one or more actors, reacting to that event. Usually, events are asynchronous (and often generated from user actions) in origin and are processed synchronously. Distributed architectures can also be based on events. There is even a type of system architecture based around events, called event driven architecture, which I won't be going into at all. I will be focusing on events in the context of real-time systems, usually on a desktop.</p> <p>If we import some event library, we could generate events, for example every time a user clicks the left mouse button, and then any actor subscribing to that event would receive the signal that that event had happened. Then each actor might each have their own way of handling such an event. Multiple actors can be subscribed to the same event, sort of like the reverse of the multiple producer, single consumer channel we looked at earlier. A single producer for an event, with potentially multiple not consumers, but listeners. This is a very reactive way of handling events. Essentially, every time you subscribed to an event, such as the left mouse button click, you would supply a callback function to be called, whenever that event was fired. This can be quite complicated, especially if you don't really need it and you might find yourself swimming in a hard-to-debug sea of callbacks and asynchronicity.</p> <p>In the 3\ufe0f\u20e3 example below, the event loop will instead be used. Every iteration of the loop, any and all events will be handled and for each type of event a branch will handle what to do given that event. In the case of the example for later on is that given a event for a mouse button press, the event handler might find out which of the two windows was the mouse hovering over and which event handler should receive a signal. If it is the main graphics window it might receive a command through a channel, if it is the GUI window, it might show an animation on a button. This is a lot easier to debug and follow with a clear separation of flow and state.</p> <p>Events are usually integral to interactive systems, if you do not have interactivity, you might want to stick to async.</p>"},{"location":"m2_concurrency/s6_events/#3-an-event-loop-in-practice","title":"3\ufe0f\u20e3 An Event Loop in Practice","text":"<p>I made some code for showing how you can work with events in practice. You can find the code in <code>m2_concurrency::code::egui-winit-wgpu-template</code> or online.</p> <p>Note that the events stem from Rust's more-or-less defacto window handling library, winit. Try and make sense of what is happening and run the code!</p>"},{"location":"m2_concurrency/s7_more_gpu/","title":"3\ufe0f\u20e3 More GPU","text":"<p>This section is about giving a bit of additional context around GPUs.</p>"},{"location":"m2_concurrency/s7_more_gpu/#the-graphics-pipeline","title":"The Graphics Pipeline","text":"<p>Originally, GPU's were made just for graphics. They were intiially quite closed in their form, with a lot of the internal pipeline hidden away from the user. The classic vertex and fragment shader setup can be seen below.</p> <p></p>  A simplified view of the traditional vertex/fragment shader setup. The geometry shader is optional and not that popular anymore.  Image credit  <p>The geometry shader is a later addition and has largely fallen out of favor due to dynamically generating more data in a way that yielded quite poor performance. But. If you look at the figure you can see that vertex data is passed in. This is transferred from the GPU. This will be all of the corners of the triangles that make up a mesh. Aside from the purely positional data, it can also have attributes like the normal vector, the texture coordinates or the color. The vertices are passed in and the vertex shader has access to one vertex at a time. Each vertex is transformed from the objects own space into the world, view and projection spaces. The vertices are sent along. A geometry shader could create additional vertices and resulting triangles, with a view of more than one vertex. This is optional. In shape assembly, vertices are assembled to full triangles, afterwhich the triangle is rasterized into fragments. Think of these fragments as potential pixel candidates. Each of them have values which are barycentric interpolations of the three vertices they are constructed from. Each fragment is then sent to the fragment shader, which can see a single fragment at a time. Lighting can then be calculated per fragment. Importantly, it is also possible for the fragment to know where its potential position is in the coordinate space of the rectangle to which it will be rendered. Once lighting and other fragment related information has been calculated the fragment can be sent further along where it will be tested whether the fragment is the fragment closest to the viewpoint of the camera, or whether it might be behind another fragment and can safely be discarded. The fragment shader could also write or read from textures or buffers. Other shaders can do that too, but I'm building to something.</p> <p>Additional shader types include mesh, task, raytracing and tesselation. Possibly more. Additional behind the scenes processes are performed to transform the data, doing things like clipping to determine whether a fragment is even in view. If it is not it can safely be discarded. This is not programmable.</p> <p>Why do you think it was important that the fragment in the fragment shader had access to its coordinate on the output surface? Take a second to think about it.</p> <p>The fragment shader is the precursor to the compute shader. Knowing where you are on the 2D output surface is tantamount to finding your thread ID. Allowing the fragment shader to write to buffers and not just send the fragment to be renddered allows you to save the calculations. Once upon a time, scientists thought it would be kind of nice if you could use the GPU, which is a massively parallel device for scientific computing. So they used fragment shaders in the early 2000's to do stuff like matrix-matrix multiplication and FFT's. There were a number of research projects in how to do GPGPU, eventually culminating in the official release of CUDA in 2007. Nowadays compute shaders are a standard part of graphics APIs and workloads, with an increasing amount of rendering pipelines using one or more compute shaders in their pipelines. These compute shaders can happen at any point. In fact, in launching the new Nanite system for Unreal Engine 5, one of the tech leads on the project, Brian Karis, revealed that for select triangles, they would prevent them from being hardware rasterized and instead write their own software based rasterizer which could outperform hardware rasterization for very small triangles.</p> <p>For a more thorough explanation of the graphics pipeline you can read this page from Learn OpenGL.</p>"},{"location":"m2_concurrency/s7_more_gpu/#spir-v-glsl","title":"SPIR-V &amp; GLSL","text":"<p>WGSL is somewhat limited in what it can do. If you would like access to more features or to have a greater codebase you can copy from directly, WGPU has increasing support for both the GPU intermediate representation SPIR-V and the shading language GLSL (which can be compiled to SPIR-V), without necessitation additional build scaffolding. It also looks like it will make use of naga directly. If you are sure your system has support for it (think back to extensions), a more mature shading language like GLSL, might let you make use of more features. I haven't had time to check it out yet, but if it can used without additional build steps and administration, this might be a nice way to get access to more features.</p>"},{"location":"m2_concurrency/s7_more_gpu/#additional-levels-in-the-memory-hierarchy","title":"Additional Levels in the Memory Hierarchy","text":"<p>Last time I introduced concepts in GPU memory to you, we had two options. A thread could either write its data to global memory (VRAM) from which other threads could read the data, or it could share data with other threads in the same workgroup, by writing to the user programmable shared memory, residing in the L1 cache. There are two other options. I know the first one is available in CUDA, it might be in some shader languages and OpenCL, but it's certainly not in WGSL. In CUDA it is known as warp shuffling. When sharing data through shared memory, the cost to access shared memory is 10 cycles, or it was last time I checked. Warp shuffling, or work group shuffling, is sharing directly between the registers of each thread in a work group. It was 1 cycle to access last time I checked. This is immensely useful in reductions and prefix sums. You would still need to write data to global memory in order to share data between work groups. Unless you are using a GPU supporting distributed shared memory. At the time of writing, the Nvidia H100 is the only card supporting it. Distributed shared memory allows accessing memory residing in the shared memory of other work groups, as if it was one big shared memory. Hopefully, this feature will make its way into other cards soon.</p>"},{"location":"m2_concurrency/s7_more_gpu/#were-you-scattering-or-were-you-gathering","title":"Were you Scattering or were you Gathering?","text":"<p>While a complete guide on GPGPU algorithm design might be... more thorough at least, one core design aspect you should definitely know is scattering and gathering.</p> <p></p>  Answer!  <p>An example of how we use either to accomplish the same thing is a 1D convolution. As you might recall, we have three elements in play. We have an input signal, an array of size N. We have a filter with weights, an array of size M. M will usually be an odd number from 1 to 19. Finally, we have an output, usually also size N. The input signal can also be padded with 0's in each end in order to minimize branching. Or the output is N-M+1 smaller in order to have an output which doesn't have (M-1)/2 0's at the ends.</p> <p>Anyways, for weight in our kernel we sample a different element in our signal, multiply it with a weight in our filter, accumulate the result, and output a new signal.</p> <p></p>  A figure showing a signal being convolved with a filter.  Image credit  <p>I hope that makes sense now. Because, scattering and gathering are two different ways of executing this fairly simple process which has quite an impact on performance. If we were doing 1D convolution on a GPU, from which point of view do we program the threads? If they are scattering, they take the point of view of each input element. Each element takes its own value, multiplies by a filter value and accumulates in the output array. Now we have a whole segment of threads which can all read from their own input element completely unobstructed, but which have a huge amount of contention as they have to synchronize their writes to either shared or global memory for the output. This also removes the ability for each thread to accumulate the result locally in a register. If they instead followed the gathering paradigm, each thread would take the point of view of their own output element. They would each do lots of reads from both the input signal and the filter weights, they would do local accumulation in a register, and each have a single write to memory with no overlap. As you might remember, you should always prefer more reads than writes. Reads are very parallelizable, writes usually require synchronization.</p> <p>This can be extended to lots of different algorithms. For matrix-matrix multiplication, it's recommended to use gathering, from the point of view of the output element for each thread. For some more material on scattering and gathering, there is a paper on it from '07.</p>"},{"location":"m2_concurrency/s7_more_gpu/#additional-reading","title":"Additional Reading","text":"<p>To learn more about the graphics pipeline you can check out Learn OpenGL or Learn WGPU.</p>"},{"location":"m2_concurrency/s8_branchless_programming/","title":"3\ufe0f\u20e3 Branchless Programming","text":"<p>For this next section, I am going to be cheating a little bit. I am going to introduce to you a concept that is mostly about optimizing for branch prediction and cache lines. These things aren't technically concurrency. But they do lead to an introduction of single instruction, multiple data (SIMD), which is... I don't know... concurrency's cousin or something. It also wasn't immediately obvious where else this would go. Anyways, branch prediction, branchless programming, data oriented programming and SIMD, here we go!</p>"},{"location":"m2_concurrency/s8_branchless_programming/#branch-prediction","title":"Branch Prediction","text":"<p>While the speed at which CPUs and memory can execute hasn't risen aggresively in these last few years, that doesn't mean they can't get smarter. One such effort is the hardware supporting branch prediction. Loading a value from memory can take quite a while, in the mean time the CPU can easily do other stuff or find our which piece of memory to load next. Below you'll see a small example of a pipelined architecture -</p> <p></p>  Pipelined Hardware Architecture  Image credit  <p>This features several sections executing different commands at the same time and then storing the result in a register, to be moved on to the next part of the pipeline. This is vastly more efficient, but requires that the hardware is able to predict which execution path should be followed. In the case of picking the wrong path, all instructions which were part of the wrong path have to be flushed from the pipeline, and the new execution path has to be begun. If you have a hard time picturing the scenario, let me paint you a picture with code -</p> <pre><code>for index in 0..data.len() {\n    if index % 1337 == 0 &amp;&amp; index != 0 {\n        data[index] -= 1; // Path A\n    } else {\n        data[index] += 1; // Path B\n    }\n}\n</code></pre> <p>Below you can see a more abstract representation of pipelined instructions -</p> <p></p>  Pipelined Instructions  Image credit  <p>If you would like to know more about the hardware side, I do recommend you check out these slides from University of California, Irvine for a cursory glance at the fairly complex topic of the hardware involved in branch prediction.  </p> <p>Going back to the code from earlier, you can see, any branch predictor worth its salt would predict that path B would be executed. In a few cases path A will be executed instead, but will be vastly more expensive.</p>"},{"location":"m2_concurrency/s8_branchless_programming/#branchless-programming","title":"Branchless Programming","text":"<p>\"Branchless programming\", is perhaps too big a promise, in all but the simplest cases, it will be \"branch mitigating programming\". Branching in code is expensive, so let's look at how we can help the computer, regardless of parallelism, attain a bit of speed. Again, please checkout the slides for an overview of various hazards with code examples. But a few of the basic highlights are control flow, through short circuiting, unrolling of for-loops (your compiler will often do this automatically), reformulation of branching through arithmetic. Another problem can be data hazards. If line A is writing to some variable in an array, which line B depends on, line B cannot commence processing until line A has completed.</p> <p>The circuiting boolean operators <code>a &amp;&amp; b</code> and <code>a || b</code> are used everywhere. The short circuiting part means that because both <code>a</code> and <code>b</code> need to be true in order for <code>&amp;&amp;</code> to evaluate as <code>true</code>, if <code>a</code> evaluates as false, <code>b</code> need not be evaluated. Do you see the problem?</p> <p>It's a branch! Supposing that <code>a</code> and <code>b</code> aren't too expensive to evaluate we can reduce the amount of branching in our code by evaluating both. One way to do so could be -</p> <pre><code>if 0 &lt; (a as u8 &amp; b as u8) {\n</code></pre> <p>or even -</p> <pre><code>if 0 &lt; (a as u8 * b as u8) {\n</code></pre> <p>For <code>||</code> options can include -</p> <pre><code>if 0 &lt; (a as u8 | b as u8)\n</code></pre> <p>or -</p> <pre><code>if 0 &lt; (a as u8 + b as u8)\n</code></pre> <p>Another way to remove a potential if-statement branch could be to multiply by 0's and 1's the data we might like to use, by reformulating our code arithmetically -</p> <pre><code>fn main() {\n    let data: Vec&lt;f32&gt; = vec![0.5; 5];\n\n    let a: bool = true;\n    let a_arithmetic: f32 = a as u8 as f32;\n    let b: bool = false;\n    let b_arithmetic: f32 = b as u8 as f32;\n\n    let calculated: f32 = a_arithmetic * data[0] + b_arithmetic * data[1];\n    println!(\"Calculated: {}\", calculated);\n}\n</code></pre> <p>As with everything else, this is something you should benchmark before deciding. In terms of readability it is usually harder to read, so just do it when you need better performance, after asserting correctness first, of course.</p> <p>Loop unrolling is the process of doing more work per loop, which also reduces the relationship between actual work done and administrative control flow work. This unroll will happen a certain amount at a time. Let's take a quick look at a loop unroll transformation, with an unroll of 4.</p> <pre><code>let mut data: Vec&lt;f32&gt; = vec![0.2; 19];\nfor index in 0..data.len() {\n    data[index] *= 2.0;\n}\n</code></pre> <p>And now for the unrolled version -</p> <pre><code>    let unroll_size: usize = 4;\n    let mut data: Vec&lt;f32&gt; = vec![0.2; 19];\n    let full_iterations: usize = data.len() / unroll_size; // 4 = 19 / 4\n\n    for index in 0..full_iterations {\n        let index: usize = index * unroll_size;\n        data[index + 0] *= 2.0;\n        data[index + 1] *= 2.0;\n        data[index + 2] *= 2.0;\n        data[index + 3] *= 2.0;\n    }\n\n    for index in (full_iterations * unroll_size)..data.len() {\n        data[index] *= 2.0;\n    }\n</code></pre> <p>Of course, the tail iterations in the second for-loop won't be as fast the main loop, but again, this usually something the compiler will do for you in release mode.</p>"},{"location":"m2_concurrency/s8_branchless_programming/#data-oriented-design","title":"Data-oriented Design","text":"<p>We can take the branchless thinking, and add in optimization for cache lines, from the micro to the macro and make it part of the way we formulate our data structures and code. This we will use things like sorting and structuring our data into bigger single objects, while at the same time pulling them apart field by field. For this, we will take a look at data-oriented design.</p> <p>A path tracer is sort of like a physics simulation for generating, sometimes physically accuracte, high quality images. From the cameras point of view, a multitude of rays are sent through each pixel. They travel forward until they intersect a reason to stop traveling directly forward. Most often this is some form of geometry, the surface of an object. Geometry is not the same as material. Once a ray intersects geometry, it needs to know what is the color of the surface that was just hit, and in which direction should it bounce next. This is dependant on the material. If it hits a perfect mirror, it will change direction much like a billiard ball hitting the side of a billiards table. If the surface was perfectly diffuse it might change to a direction in a completely random direction. This calculation is not just a simple lookup, but two entirely different mathematical functions. They ray will continue to bounce until it either reaches a maximum depth, hits a light source or is otherwise terminated. If that doesn't make sense, try out this video.</p> <p>So why spend your and my time describing path tracers?</p> <p>They are the perfect case for not just performant heterogeneous systems, but wildly divergent performant heterogeneous systems. If you launch 100 rays from different starting origins inside the area covered by a single pixel, even if they all hit the same surface, based on the material, they might scatter in different directions. They might not all hit the same surface, some leaving the scene and hitting nothing, resulting in early termination, while some hit material A requiring one scattering function, while others hit materila B requiring another scattering function. As you can imagine, this gets progressively worse as all rays scatter round and round in the scene. But we can mitigate the effects of this steadily increasing divergence by making sure to package, sort, reorder and compact our rays. Instead of looking at a ray at a time, we might structure our program in various pipeline steps. We might for example try to intersect all rays against the scene, which is quite expensive, but more on that later, followed by a material/scattering interaction, where we could group rays by material, allowing us to execute all rays needed a material A scattering event, followed by executing all rays needing a material B scattering event. It gets worse, but let's take a step back and just look at how we might handle a whole bunch of rays.</p> <p>The typical way we might handle a bunch of rays in a path tracer, would be to define each ray as a struct, such as -</p> <pre><code>struct Ray {\n    origin: Vec3,\n    direction: Vec3,\n    color: Vec3,\n    material: Material, // Enum\n}\n</code></pre> <p>This is just as useful with graphs, as long as we have chosen a formulation of them which allows us to keep them in arrays instead of floating around in pointer-based structures. Think back to cache lines now, if we are handling a bunch of rays one at a time, whenever we go to a different function we are jumping all around in different branches of code, we are loading lots of parts of this <code>Ray</code> struct in our cache line that we might not even need. If we are primarily in a part of the code where we care about geometry and intersections, we won't care about material and color. We would care about origin and direction. We could reformulate the way we kept track of rays to process several data elements at the same time, when in a relevant part of the code. If we kept a <code>Vec&lt;Ray&gt;</code> to do this, sending the <code>Vec&lt;Ray&gt;</code> around from function to function, we would call this an Array-of-Structs (AoS).</p> <p>What we could do instead, to only cache exactly what we needed is called Struct-of-Arrays (SoA). We keep all of our rays in a deconstructed fashion -</p> <pre><code>struct VectorOfRays {\n    origins: Vec&lt;Vec3&gt;,\n    directions: Vec&lt;Vec3&gt;,\n    colors: Vec&lt;Vec3&gt;,\n    materials: Vec&lt;Material&gt;,\n}\n</code></pre> <p>or we might even take it a step further -</p> <pre><code>struct VectorOfRays {\n    origins_x: Vec&lt;f32&gt;,\n    origins_y: Vec&lt;f32&gt;,\n    origins_z: Vec&lt;f32&gt;,\n    directions_x: Vec&lt;f32&gt;,\n    directions_y: Vec&lt;f32&gt;,\n    directions_z: Vec&lt;f32&gt;,\n    color_r: Vec&lt;f32&gt;,\n    color_g: Vec&lt;f32&gt;,\n    color_b: Vec&lt;f32&gt;,\n    materials: Vec&lt;Material&gt;,\n}\n</code></pre> <p>Now instead of operating on individual rays, or operating on lists of rays, we can do individual operations on each and every relevant element, getting them all in our cache line. This require our intersection function to have a series of loops for each step in the function and requires a bit of refactoring. We gain an additional advantage, we increase the probability of the compiler autovectorizing our code. More on that in a little bit.</p> <p>This all seems a bit cumbersome however. If you remember earlier, I told you that these rays might scatter, bounce, hit different materials and be terminated. Either successfully or unsuccessfully. We need some flexibility back to make it possible to intersect a bunch of threads, terminate some of them, move others to one queue for material A, move others to a queue for material B and so on.</p> <p>Enter... drumroll please... Arrays-of-Structs-of-Arrays (AoSoA).</p> <p>We could instead use a coarser granularity by storing a number of rays in a shared struct and keep a list of those structs -</p> <pre><code>struct Ray4 {\n    origins_x: [f32; 4] = [0.0; 4],\n    origins_y: [f32; 4] = [0.0; 4],\n    origins_z: [f32; 4] = [0.0; 4],\n    directions_x: [f32; 4] = [0.0; 4],\n    directions_y: [f32; 4] = [0.0; 4],\n    directions_z: [f32; 4] = [0.0; 4],\n    color_r: [f32; 4] = [0.0; 4],\n    color_g: [f32; 4] = [0.0; 4],\n    color_b: [f32; 4] = [0.0; 4],\n    materials: [Material; 4] = [Material::default(); 4],\n}\n</code></pre> <p>We can choose a coarser granularity by making this struct any size we like. Within reason, we have a good chance it might fit on the stack. Now we are free to move around rays, eliminate ones that are terminated, replace terminated rays with new ones and other such divergence limiting operations. Some libraries such as ultraviolet are designed around AoSoA containing types like <code>Vec3x8</code> and <code>f32x4</code>.</p> <p>We could even take things a step further and separate our ray structs. The fields that are necessary for intersection aren't the same as for propagating colors through bounce paths. Normally, we might connect disconnected parts of the same underlying object by indices, but as rays are a transient object which we might bounce between functions it might work better with something more static like geometry and materials.</p>"},{"location":"m2_concurrency/s8_branchless_programming/#the-road-to-simd","title":"The Road to SIMD","text":"<p>SIMD stands for single instruction multiple data. Basically this whole section has been building to it. SIMD is a single instruction that operates on an entire <code>lane</code>. A <code>lane</code> is defined by the architecture of your CPU. The name to look for is typically named something like SSE or AVX. The CPU of the system used for benchmarking this section, has multiple SIMD instruction sets - Intel\u00ae SSE4.1, Intel\u00ae SSE4.2, Intel\u00ae AVX2, Intel\u00ae AVX-512. You can program for each specific instruction set or find a cross-platform such as ultraviolet to program for multiple architectures. What I found most important is that my SIMD lanes max out at 512 bits. If I am doing <code>f32</code>-based SIMD operations I can fit up to 16 floats into a lane.</p> <p>SIMD programming is a genre unto itself and learning to program it can take a while. If you look at the figure from earlier showing pipelined instructions, picture each box as being wider. Imagine you are doing the same operations on N rays at a time. If we have a diverging workload such as in a path tracer, we have to have some way to handle when some rays are no longer relevant. To handle this SIMD makes heavy use of masks. Masks are types which distinguish which of the N elements in a lane need to be executed. Another thing SIMD cannot do, or if it is possible, you probably shouldn't force it to do, is executing different functions at the same time.</p> <p>Let's look at some code. This program looks at different ways of arranging data and executing a handful of different, very basic, functions. Note that the memory-to-compute ratio is quite high, as the functions are quite simple compared to the amount of load operations. SIMD doesn't magically make your program any less memory bound, as such you have to try and optimize your code before going fully SIMD.</p> <p>Find the code in <code>m2_concurrency::code::sorting_functions</code> or online.  </p> <p></p>  Benchmark for the program in ```m2_concurrency::code::sorting_functions```. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively. The CPU supports Intel\u00ae SSE4.1, Intel\u00ae SSE4.2, Intel\u00ae AVX2, Intel\u00ae AVX-512.  <p>As you can see, if we generate a huge list of different functions to call, we can gain an immense amount of performance simply by sorting our data. We can either sort it ourselves, or sit it as it accumulates, by putting it in different lists. The other major performance gain we get is by introducing SIMD. Note that the performance gain isn't huge when changing to SIMD. This is a case where each function is small enough that we are mostly memory bound. SIMD does nothing to alleviate that, except try to draw on even more of the memory bandwidth.</p> <p>If we instead look at code which uses an actual function from path tracing, in this case sphere intersection, we are doing much more compute per memory load and can see a significant increase in performance. Here's a sample of the code to show you what SIMD programming looks like -</p> <pre><code>fn ray_sphere_intersect_x8(\n    sphere_o: &amp;uv::Vec3x8,\n    sphere_r_sq: &amp;uv::f32x8,\n    ray_o: &amp;uv::Vec3x8,\n    ray_d: &amp;uv::Vec3x8,\n) -&gt; uv::f32x8 {\n    let oc = *ray_o - *sphere_o;\n    let b = oc.dot(*ray_d);\n    let c = oc.mag_sq() - sphere_r_sq;\n    let descrim = b * b - c;\n\n    let desc_pos = descrim.cmp_gt(0.0);\n\n    let desc_sqrt = descrim.sqrt();\n\n    let t1 = -b - desc_sqrt;\n    let t1_valid = t1.cmp_gt(0.0) &amp; desc_pos;\n\n    let t2 = -b + desc_sqrt;\n    let t2_valid = t2.cmp_gt(0.0) &amp; desc_pos;\n\n    let t = t2_valid.blend(t2, uv::f32x8::splat(std::f32::MAX));\n    let t = t1_valid.blend(t1, t);\n\n    t\n}\n</code></pre> <p>As you can see, in this case I only use functions implemented directly by utraviolet's <code>uv::Vec3x8</code> and <code>uv::f32x8</code> types.</p> <p>Be sure to check out the 4 references at the top of the code. Find the code in <code>m2_concurrency::code::sphere_intersection</code> or online.</p> <p></p>  Benchmark for the program in ```m2_concurrency::code::sphere_intersection```. This benchmark was run on my laptop boasting an Intel i7-1185G7, 3.0 GHz with 32GB of RAM. The operating system was Windows 10. The L1/L2/L3 caches were 320 KB, 5 MB and 12 MB respectively. The CPU supports Intel\u00ae SSE4.1, Intel\u00ae SSE4.2, Intel\u00ae AVX2, Intel\u00ae AVX-512.  <p>In this case, the vanilla struct-of-arrays code performed worse. In some cases you can hope for autovectorization, where the compiler, in the case where you have factored your data in a way that allows it, will create a SIMD version of your code. This is usually more likely if you turn on your compilers native targetting flag.</p> <p>With compilers you can tell it which platforms to compile for. If you tell it to compile exclusively for YOUR platform it can make use of optimizations and commands specific to your system. This might make it perform significantly worse on other systems though.,</p> <p>I will leave you with one thing if you are memory bound, SIMD won't magically make your memory bandwidth increase.  </p> <p>Also, here's a nice blog post about the performance implications of AOS, SOA and AOSOA.</p>"},{"location":"m2_concurrency/s8_branchless_programming/#additional-reading","title":"Additional Reading","text":"<p>A nice introduction video to branchless programming by Fedor Pikus. A nice introduction video to SIMD by Guillaume Endignoux. Mike Acton on Data-oriented Design. Wiki on branch prediction. Wiki on instruction pipelining. Slides on instruction pipelining from The University of Tennessee, Knoxville.  </p>"},{"location":"m2_concurrency/s8_branchless_programming/#shader-execution-reordering","title":"\ud83e\uddec Shader Execution Reordering","text":"<p>The sorting, compacting and reordering hinted at for path tracing earlier is actually a pretty hot button topic in path tracing and has recently gotten hardware support.  </p> <p>Megakernels Considered Harmful Wavefront Path Tracing Shader Execution Reordering </p>"},{"location":"m2_concurrency/s9_parallel_graphs/","title":"3\ufe0f\u20e3 Parallel Graphs","text":"<p>How to efficiently use graphs in parallel is more of an advanced topic and poses a wide range of challenges. If your graph is read-only and reasonably, sized you can optimize your performance by reducing the amount of pointer jumps you do and increase your cache coherence. One possible avenue is to construct your graph through indices.</p> <p>However, not only is it hard to construct graphs in Rust, but it's also hard to construct dynamic graphs. And even hard to construct massive graphs which can be dynamically manipulated in parallel. If you are working on a single node at a time, things become quite simple in parallel. You can wrap each node in a lock, such as a mutex. But, it is quite common to have to process bigger and bigger neighbourhoods around vertices. One way to do this is to alternative between read and write stages. In one stage, each thread reads a relevant neighbourhood, does some processing, and adds changes to a list. Once all changes have been proposed, one or more threads can execute the changes, resulting in a new graph. One such solution is described here.</p> <p>Sampling and repacking extremely huge graphs for training graph neural networks on GPUs is also a research topic on its own. Meshes, widely used in graphics, is another type of graph. In order to maintain spatial coherency, and improve performance, another active field is how to best sort meshes (graphs) and choosing the right sizes into cohesive meshlets (neighbourhoods), which can be culled individually instead of the entire mesh being culled. Check out this general intro to parallel graph processing.  </p>"},{"location":"m3_types/","title":"1\ufe0f\u20e3 Types","text":"<p>Types might not seem to be the sexiest topic of all time, but don't you worry. Knowing about types will be a good investment of your time. Whether to use a 32-bit or 64-bit number is not just a matter of whether your program has enough precision to go around, but can be a source of bugs and buggy behavior. Sometimes you may even need to guarantee that your system yields at minimum some level of precision in very real world terms for your users to be satisfied using your system.</p> <p>You can reduce the strain on your memory bandwidth by using smaller types (think back to cache lines) and get more data elements per cache line resulting in being less memory bound which might in turn increase the speed of your program. If you are on a GPU, your performance might improve significantly (factor 32) by going from 64-bit floats to 32-bits, you can even get access to tensor cores, by reducing precision even further, allowing an even greater improvement to the speed of your program. Knowing more about types allows you to sort, order and quantize your processed data in a way that has as small an impact on precision as possible while decreasing the size of your data. This could result in faster download times or you could stream your data from disk directly to the GPU, where the GPU itself might be able to unpack the data.</p> <p>Which types you are using have an impact not just on speed and size, but also the energy consumption of your programs. In general, less bits mean less energy consumed and integers cost less energy to process compared to floats. You don't have to micromanage every single variable all the time, but one of the first places to look when optimizing should be arrays. <code>u8</code> rarely matters, but <code>[u8]</code> sure does.</p> <p>Finally, knowing about types allows us to operate directly on the underlying bits, casting from one type to another, to create tightly packed information, which we couldn't otherwise have, such as packing three dimensional indices into just 32 or 64 bit integers.</p>"},{"location":"m3_types/s0_integers/","title":"2\ufe0f\u20e3 Integers","text":"<p>So far we have mainly used integers as indices into memory (arrays), but other than being good at doing integer math, integers also have a few other characteristics going for them. They have uniform precision across their entire span, they are good for compressing data and integer-based calculations are quite a bit more energy efficient than floating point calculations.</p> <p>Before I go into how a computer typically represents signed and unsigned integers, we are going to take a look at how we represent numbers in different bases without a computer. You probably know numbers in base 10. But what about base 2 and base 16?</p>"},{"location":"m3_types/s0_integers/#binary-base-2","title":"Binary - Base 2","text":"<p>Base 2 is what you'll know as binary. Binary is at the absolute core of how computers represent numbers, as each digit can be represented as either being on or off. 1 or 0. We have 2 possible values for each digit in binary numbers. In base 10 the number 18 can be represented as the following -</p> <p>1x10<sup>1</sup> + 8x10<sup>0</sup></p> <p>There is of course an infinite amount of 0's in front of 18, which we don't see because they are implied. Base 2 would represent the number as 0b10010 (note the 0b prefix).</p> <p>Let's try and represent the number 18 in base 2.</p> <p>1x2<sup>4</sup> + 0x2<sup>3</sup> + 0x2<sup>2</sup> + 1x2<sup>1</sup> + 0x2<sup>0</sup></p> <p>Or if we resolve the digits</p> <p>1x16 + 0x8 + 0x4 + 1x2 + 0x1</p> <p>Removing the zeroes -</p> <p>1x16 + 1x2  </p> <p>So 0b10010 is the same as 18 in base ten.</p>"},{"location":"m3_types/s0_integers/#hexadecimal-base-16","title":"Hexadecimal - Base 16","text":"<p>Another representation is hexadecimal. It is base 16. So what do you do when you now have 16 values per digit?</p> <p>You start including letters! Hexadecimal numbers have a range for each digit from 0 to F. So 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A (10), B(11), C(12), D(13), E(14) and F(15). Why would anyone want to use a system like this?</p> <p>We could write the same in hexadecimal by writing 0x12 (note the 0x prefix) or -</p> <p>1x16<sup>1</sup> + 2x16<sup>0</sup></p> <p>Which amounts to -</p> <p>1x16 + 2x1</p> <p>But why would you want to do this? Simple, it functions like a contraction of binary. Each F represents 4 bits. So if we wanted to indicate an entire byte (8 bits) turned on, which is the same as the number 255, we could either write 255 or 0xFF. That case is simple enough. Everyone can remember 255. We could just use base 10 for that and save us having to learn hexadecimal representations. Stress on the representation, whether you write base 2, 10 or 16, in hardware it is all base 2. But where hexadecimal really shines is when we want to manipulate individual bytes or bits.</p> <p>There is something called bitwise operators, don't worry about it, it is one of the next sections (s3). We could use the bitwise operator &amp; (AND) on two numbers. &amp; would compare every bit of the two numbers one at a time and output a 1 if both bits were 1. It would output a 0 in all other cases. So if we gave it the two binary numbers, the mask 0b00000000000000001111111100000000 and some input number like 0b01010010001001101011010110010110, we would get an output value of 0b00000000000000000011010100000000. This allows us to just isolate the active bits in the second byte from the right.</p> <p>This is really hard to read. If we had instead encoded our mask in hexadecimal, we would write 0xFF00 for our mask. We rarely have to write out our inputs, but in this case it can be represented as 0x5226B596. If you are on Windows, you can find the Calculator app. If you click in the upper left corner, you can find the view called \"Programmer\". On the left side you will see various numerical representations like HEX, DEC, OCT and BIN. Having these sorts of calculators can be very handy to quickly transfer from one representation to another.</p> <p></p>  You can find other numerical representations in calculator apps."},{"location":"m3_types/s0_integers/#unsigned-integers","title":"Unsigned Integers","text":"<p>Unsigned integers are more or less represented as a raw binary number. The range of possible numbers to represent with an unsigned integer is 2<sup>n</sup>-1. So an 8-bit number can represent the range 0..255. You might wonder why the -1 is there. It's because 0 also has to be represented.</p> <p>You can multiply, divide, add, subtract and all the other standard stuff. But what happens when you go over or under the representable range? This is called over- and underflow. It is defined for unsigned integers, but undefined for signed integers. Signed means it can also represent negative numbers, where as unsigned integers can exclusively represent non-negative numbers.</p> <p>Overflow in unsigned integers is defined as being the remaineder of dividing the desired number by the maximum representable number. So if you are adding 128 to 157, both using 8-bit unsigned integers, which has a maximum value of 255, the desired number is 285. This results in an overflowed value of 30. Read this link about some of the considerations regarding unsigned integers. The author carries the opinion that you should mainly use unsigned integers for stuff like array indices (which are all required to be <code>usize</code> in Rust) or as variables which you know are being manipulated as bitfields instead of numbers.</p> <p>A special type of unsigned integer is the <code>usize</code>. On a 32-bit operating system, where you might recall we can only address around 2 GB of memory, the <code>usize</code> will have a size of 32 bits. On most modern 64-bit operating systems, the <code>usize</code> will be a 64-bit unsigned integer. Rust demands that you convert your indices to <code>usize</code>, where as C++ implicitly converts your integers to <code>usize</code>. So what happens if you index an array with a 32-bit signed integer in C++? This is very common, and in most cases it is fine. Except when that signed integer is negative. In which case we have used a type that allows us this undesired behavior. I ran this code in an online C++ environment -</p> <pre><code>// Online C++ compiler to run C++ program online\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    // Write C++ code here\n    std::cout &lt;&lt; \"Hello world!\";\n    std::vector&lt;int&gt; array{0, 1, 2, 3, 4};\n    int some_index{-2};\n\n    std::cout &lt;&lt; array[some_index];\n\n    return 0;\n}\n</code></pre> <p>When running it for other negative indices than -2 it prints the value 0. This is probably uninitialized memory which hasn't been used yet. If we index with -2 we get the value 33. Think back to when we looked at memory hierarchies and pointer arithmetic in <code>m1::s0</code>. This is actually a pointer being manipulated. We weren't supposed to go back beyond the start of the array, but we just did!</p> <p>This is bad, and you shouldn't do it.</p>"},{"location":"m3_types/s0_integers/#signed-integers","title":"Signed Integers","text":"<p>Signed integers are a bit more complex. They can carry both positive and negative numbers. The outermost bit is reserved as a sign bit, denoting whether the number is positive or negative. Read about the range of signed integers as well as the concept two's complement here.</p> <p>In the case of signed integers, over- and underflow are undefined in general. However, overflow for signed integers is defined in Rust Debug . You can se more about the types in Rust here.</p>"},{"location":"m3_types/s0_integers/#additional-reading","title":"Additional Reading","text":"<p>You can take a look at the wiki for integers, or you might prefer lecture slides or even a video by your favourite performance YouTuber.</p>"},{"location":"m3_types/s1_floats/","title":"2\ufe0f\u20e3 Floats","text":"<p>Where as in signed integers we had two distinct areas of the bits with a uniform precision, we have 3 segments and a quite varying precision with floating point numbers. In the more-or-less standard IEEE-754 specification we also have a number of exceptions and specific behaviors such as in which situations a float locks to a NaN value. Given the use cases in machine learning new, smaller and less standard floating point formats have emerged such as bfloat16. A lot of these very small specialized types of floats, or very small integers are required to work with tensor core accelerators.</p> <p>I can't explain exactly how a float works much better than the wiki. Ever so slightly browsing the page about IEEE-754 also makes good sense.</p> <p>So, now that you've browsed the websites and your are familliar with concepts such as exponent and fraction, I have a few key concepts for you to make note of. If you can at all keep your floats as close to being between -1.0 and 1.0, you will keep as much of your precision as possible. 4.0 / 3.0 is not the same in terms of precision as 4.0 * (1.0 / 3.0), unless the inverse number that you are multiplying with is exactly 1/2^N. This is an often used optimization as a multiplication is vastly cheaper than a division. You will typically see this optimization if a division by a constant is happening in a loop -</p> <pre><code>let some_constant: f32 = 5.0;\nfor element in &amp;mut data {\n    *element = *element / some_constant;\n}\n</code></pre> <p>is not numerically equivalent to</p> <pre><code>let some_constant: f32 = 5.0;\nlet inverse_some_constant: f32 = 1.0 / some_constant;\nfor element in &amp;mut data {\n    *element = *element * inverse_some_constant;\n}\n</code></pre> <p>But if <code>some_constant</code> was 4.0, 2.0 or 256.0 or something like that, they would be. Finally, NaN's propagate. Any operation involving a NaN value returns a NaN value, including <code>NaN == NaN</code>.</p> <p>ACCUMLATING IN HIGHER PRECISION</p>"},{"location":"m3_types/s1_floats/#additional-reading-highly-recommended","title":"Additional Reading - Highly Recommended","text":"<p>Every floating point operation incurs some form of error. But if you used specialized floating point operations such as the fused multiply-add, which can be found in Rust as well, you can get a single rounding error instead of two rounding errors. If you are summing a large list of numbers you can use algorithms for compensating for the accumulated error, such as Kahan Summation. You can also keep a running estimate of the error.</p>"},{"location":"m3_types/s1_floats/#computer-graphics","title":"\ud83e\uddec Computer Graphics","text":"<p>Depth buffers are usually in need of some thought. You can take a look at depth precision visualized or the now fairly common reverse depth buffer.</p>"},{"location":"m3_types/s2_energy_efficiency/","title":"2\ufe0f\u20e3 Energy Efficiency","text":"<p>From experience, not a lot of people who daily write quite long-running code, necessarily connect the two dots computing and energy consumption/CO<sub>2</sub> emissions. Anything you do with a computer consumes some form of electricity. Including whatever you do on the internet or download from the internet. Like downloading the contents of this page for you to read. Generally, if your code is faster, it is greener. If your code downloads less, it is likely to be greener. If you use a lower precision, generally, it is likely to be greener. If you accumulate work to do, as in batching, instead of firing up the system for every single request, it is likely to be greener. If you preprocess the data at the edge before sending it to the cloud, it is likely to be greener. If your code is faster, it is likely to greener. If your code is faster, it is likely to be cheaper.</p> <p>I will primarily focus on the energy efficiency of things you have direct control over. Once you have learned that, you should be able to infer the rest. Why not make this a 3\ufe0f\u20e3 topic? Because quite a few people don't seem to be making the aforementioned connection, and I find it important that people start evaluating the value that is generated from what they are doing. Training and running neural networks do not grab energy out of the ether. They have a cost. A resource expenditure, and the value you attain from those systems should be greater than the consumption.</p> <p></p>  The estimated power consumption of some arithmetic operations on an AMD and an Intel CPU. Note that 32-bit integer is missing. The operations ending in i are integers, d for double and s for single.  Image credit  <p>Unfortunately, the 32-bit integer data wasn't there, and it is an older measurement. But, it should give you an indication of the scale of how much cheaper a 32-bit float is compared to a 64-bit float. Now this was just for compute, let's see what it's like for memory access -</p> <p></p>  The estimated power consumption of memory accesses on an AMD and an Intel CPU.  Image credit  <p>As you can see, retrieving a value all the way from RAM instead of a cache is vastly more expensive. If you think back to the cache lines from <code>m1</code> imagine lowering the precision of your data from 32-bit floats to 16-bit or even 8-bit. Imagine many additional data elements you could fit in a single cache line. Elements which could be reused and kept in cache.</p>"},{"location":"m3_types/s2_energy_efficiency/#additional-reading","title":"Additional Reading","text":"<p>Efficient Processing of Deep Neural Networks is a highly recommended tour through various concerns and techniques in reducing the energy footprint of neural networks. A recorded lecture of Vivienne Sze about energy efficiency in AI. Characterizing energy consumption of CPUs. Green Computing. What is green coding?. The somewhat controversial paper Ranking programming languages by energy efficiency .</p>"},{"location":"m3_types/s3_bitwise_operations/","title":"2\ufe0f\u20e3 Bitwise Operations","text":"<p>After that little aside, let's get back to a more direct description of how we work with types. We don't just have the mathematical operators which we can use on integers and floats, taken directly from mathematics, like *, /, +, -, %, but we can operate directly on the bits themselves, which is specific to the underlying implementation of types. Before we start, I should probably say (write?) that it is at minimum bad practice, if not outright triggering a compiler error, to do bitwise operations on floating point numbers. In most cases it is recommended to use integers, and if you do not view the variable as a number at all but as an array of bits, also known as a bit array, a bit mask, bit map, bit set, bit string or bit vector (make up your minds people!), it is recommended to use unsigned integers, or sometimes languages will have specific types just for better ergonomics when directly manipulating bits. In that case they might mimick the vector operations quite closely, such as indexing the bit array, like <code>bit_vector[3]</code> to get the fourth bit. This is quite inefficient if you are manipulating more than a single bit at a time though. A bit vector type might also add some other operations, like counting the number of zeros and ones.</p> <p>Anyways. We have 5 of the most common bitwise operations right here: &gt;&gt; &lt;&lt; &amp; | ^ ~.</p> <p>The &gt;&gt; and &lt;&lt; operators are known as bit shifts. You might hastily assume that that <code>let shifted = input &gt;&gt; 2;</code> merely shifts all of the bits 2 steps to the right. This CAN be equivalent to integer division by 4 as the bits fall off to the right. But what if <code>input</code> is a signed integer? In that case it can vary. Does it shift the sign along with it or does ignore the sign bit? The same with <code>let shifted = input &lt;&lt; 2;</code>. If input was <code>-2</code>, with a bit representation of <code>0b1111_1110</code> and we shift two places to the right we get, wait, hold on... we still get <code>-8</code>! Thank you 2's complement! What it actually does is shift the 1 to the right, but puts in another 1 into the sign bit. But shifting downwards would get us <code>0b0011_1111</code>, which is <code>63</code>. Which wasn't what we intended. If you think this is a bit messy, that's because it is and if you are playing around with bits for signed integers you should always look up the language you are using's definitions of shifts on various types. That got a bit headscratchy, because one number representation had a special bit and 2's complement. Now imagine this, but with floating point numbers with three segments of bits instead. That's why we don't do bitwise operations on floats!</p> <p>The type of shift where the sign bit gets special treatment is called an arithmetic shift, where as the one treating the whole thing as just bits, is called logical shift. You can read more about it here, but I would recommend you stick to unsigned integers for bit manipulating stuff. It's less stuff to keep in your head.</p> <p>Next, let's look at the bitwise and, usually written as bitwise AND, probably to ensure you don't mistake it for linguistic addition. Bitwise AND has the same logical implications as the boolean &amp;&amp;. Two trues, or two 1's, result in a true or a 1. All other cases result in false or 0. But, the common &amp;&amp; does something else. It short circuits. If we take the statement <code>let is_correct = eval_a() &amp;&amp; eval_b();</code> and <code>eval_a()</code> returns false, <code>eval_b()</code> is never called. This can be a boon for performance if <code>eval_b()</code> is very expensive, but can actually lead to worse performance in the case of <code>let is_correct = 0 &lt; some_number &amp;&amp; some_number &lt; max;</code>. I'll get back to that in <code>m2::s8</code>, which is about branchless programming. It's a 3\ufe0f\u20e3 topic, which requires understanding some stuff about branch prediction, and especially bitwise- and conditional operations.</p> <p>Anyways. When we do the bitwise AND we don't supply two booleans, we supply two integers or other bit representations. Don't do it with floats, even if the language allows you, it is needlessly complicated. The other number we supply is usually called a mask. We could for example have a number where we wanted to isolate the first 8 bits and not be bothered by the rest. In that case we might write something like -</p> <pre><code>let initial_value: u16 = 0b10101010_11110101;\nlet mask: u16 =                   0b11111111;\nlet masked_value: u16 = initial_value &amp; mask; // masked value should now be 0b11110101\n</code></pre> <p>Masks is also where hexadecimal numbers can really clean up. I used u16 in the example, but if we used 64-bit numbers things could get very out of hand -</p> <pre><code>// Damn you 64-bits!\nlet initial_value: u64 = 0b10101010_10101010_10101010_10101010_10101010_10101010_10101010_11110101;\nlet mask: u64 =          0b11111111_00000000_00000000_00000000_00000000_00000000_00000000_00000000;\nlet masked_value: u64 = initial_value &amp; mask; // masked value should now be 0b11110101\n</code></pre> <p>... let's use hexadecimal numbers instead -</p> <pre><code>// Damn you 64-bits!\nlet initial_value: u64 = 3_074_457_345_618_258_677; // or 0x2AAA_AAAA_AAAA_AAF5\nlet mask: u64 = 0xFF00 0000 0000 0000; // FF is the same as turning 8 bits completely on. So 0xF == 0xb1111.\nlet masked_value: u64 = initial_value &amp; mask; // Masked value should be 0x2A00 0000 0000 0000\n</code></pre> <p>Or with a slightly more sane example.</p> <pre><code>let initial_value: u16 = 43_765;\nlet mask: u16 = 0xFF;\nlet masked_value: u16 = initial_value &amp; mask; // masked value should now be 0b11110101\n</code></pre> <p>We could also use it to isolate specific bits. Like checking whether a single bit is turned on -</p> <pre><code>let initial_value: u16 = 0b10101010_11110101;\nlet mask: u16 = 0b00000100;\nlet third_bit_turned_on: bool = 0 &lt; (initial_value &amp; mask); // true\n</code></pre> <p>This is really useful in embedded systems or any system where we have lots of flags and want to conserve bandwidth and/or space, like \"the router is in an error state\" or \"package received\". All of a sudden we can have 64 flags in 64 bits! Or if we wanted to have a bunch of 3d voxels or an occupancy map which only carried a is there/is not there value, we could vastly reduce memory consumption by using bit operations. | and ^ are the same except with a different logic table. || is sort of like |, or bitwise OR. One or more values need to be true. Where as with bitwise XOR it only returns 1 if a single bit is 1.</p> <p>Finally, we have the bitwise NOT. Usually written as ~. It is the bitwise version of !. It just flips all bits from 0 to 1 or from 1 to 0.</p>"},{"location":"m3_types/s3_bitwise_operations/#3-bitwise-rust","title":"3\ufe0f\u20e3 Bitwise Rust","text":"<p>Rust does not have a bit vector implementation in the standard library, but what it does instead is implement quite a number of bitwise or bitwise-adjacent operations directly on integers. Rust does have the bitwise NOT, usually denoted ~, but it is ! like on boolean types. Like so -</p> <pre><code>let initial_value: u8 = 0b01010101;\nlet flipped_value: u8 = !initial_value; // 0b10101010;\n</code></pre>"},{"location":"m3_types/s3_bitwise_operations/#additional-reading","title":"Additional Reading","text":"<p>Bitwise Operations wiki. The Rust language reference for operators.</p>"},{"location":"m3_types/s4_bit_tricks/","title":"3\ufe0f\u20e3 Bit Tricks","text":"<p>There's a number of different tricks you can do with bits, and I won't even attempt to do all of them. But I'll briefly introduce three different things you can do. Spatial hashing, Morton codes and the legendary fast inverse square root.</p>"},{"location":"m3_types/s4_bit_tricks/#spatial-hashing","title":"Spatial Hashing","text":"<p>Ok, so first off let's look at what we can do with encoding multidimensional data into a single number. This can be really useful for encoding 2- or 3D information. In fact, so nice that my thesis was basically about how to extend this concept into how we could design point cloud rendering systems.</p> <p>First off let's try just encoding three integers into a single integer. <code>x</code> will reside in bits 40-59, <code>y</code> will reside in bits 20-39 and <code>z</code> will reside in bits 0-19.</p> <pre><code>let x: u64 = 7;\nlet y: u64 = 5;\nlet z: u64 = 2;\n\nlet x_hash: u64 = 0x0FFF_FF00_0000_0000 &amp; (x &lt;&lt; 40);\nlet y_hash: u64 = 0x0000_00FF_FFF0_0000 &amp; (y &lt;&lt; 20);\nlet z_hash: u64 = 0x0000_0000_000F_FFFF &amp; z;\nlet hash: u64 = x_hash | y_hash | z_hash;\n</code></pre> <p>So we have three usages of bitwise operators here. We shift, AND and OR. First we shift our initial number to start from the correct bits. We mask with the bitwise AND to ensure that we stick within the allotted bits. This also has the effect of truncating the 44 most significant bits of <code>x</code>, <code>y</code> and <code>z</code>. So we have to keep track of what it is we trying to hash. Finally, we bitwise OR the whole thing.</p> <p>We could also do the masking before the shift for something even cleaner -</p> <pre><code>let x: u64 = 7;\nlet y: u64 = 5;\nlet z: u64 = 2;\n\nlet x_hash: u64 = (x &amp; 0xFFFFF) &lt;&lt; 40;\nlet y_hash: u64 = (y &amp; 0xFFFFF) &lt;&lt; 20;\nlet z_hash: u64 =  z &amp; 0xFFFFF;\nlet hash: u64 = x_hash | y_hash | z_hash;\n</code></pre> <p>We can of course also just start with a 0 and continually OR with the shifted and masked values.</p> <pre><code>let x: u64 = 7;\nlet y: u64 = 5;\nlet z: u64 = 2;\n\nlet mut hash: u64 = 0;\nhash |= (x &amp; 0xFFFFF) &lt;&lt; 40;\nhash |= (y &amp; 0xFFFFF) &lt;&lt; 20;\nhash |=  z &amp; 0xFFFFF;\n</code></pre> <p>Since we have ensured we do not have any overlaps between our shifted and masked numbers, the ORs can be replaced by additions. We are also always free to use multiplication instead of the shifts, but the intent is less clear and you have to be sure that it is still doing the same as the corresponding shift, which again, is defined differently depending on the stype, direction and language.</p> <pre><code>let x: u64 = 7;\nlet y: u64 = 5;\nlet z: u64 = 2;\n\nlet mut hash: u64 = 0;\nhash += (x &amp; 0xFFFFF) * 1_099_511_627_775;\nhash += (y &amp; 0xFFFFF) * 1_048_575;\nhash +=  z &amp; 0xFFFFF;\n</code></pre> <p>Ok, so just linearizing three integers into one integer is pretty clear. We can of course easily reconstruct the three numbers by just doing the opposite operations.</p> <pre><code>let x: u64 = 7;\nlet y: u64 = 5;\nlet z: u64 = 2;\n\nlet x_hash: u64 = (x &amp; 0xFFFFF) &lt;&lt; 40;\nlet y_hash: u64 = (y &amp; 0xFFFFF) &lt;&lt; 20;\nlet z_hash: u64 =  z &amp; 0xFFFFF;\n\nlet hash: u64 = x_hash | y_hash | z_hash;\n\nlet x: u64 = hash &gt;&gt; 40 &amp; 0xFFFFF; // x == 7\nlet y: u64 = hash &gt;&gt; 20 &amp; 0xFFFFF; // y == 5\nlet z: u64 = hash &amp; 0xFFFFF; // z == 2\n</code></pre> <p>Easy peasy. We could easily split this code into encode and decode functions. Linearization of 3D coordinates would be good for encoding sparse data. If you used this for actual indices into an array you would have to allocate more memory than your machine could possibly have. Instead, you can use the u64, or you could even do a 32-bit or 16-bit encoding using less bits from the original data, as keys into a hash map.</p> <p>Now, let's do this with floats instead! This requires us to use quantization. With quantization we want to take a float and adapt it to the domain of an integer. We need to know WHERE our domain resides and how BIG it is.</p> <pre><code>// This would be given for this specific grid\n// If we aren't TOO bothered by the loss of precision we can\n// multiply by the inverse of the scaling term.\nlet x_scale: f32 = 5.0f;\nlet x_offset: f32 = 0.32f;\n\nlet y_scale: f32 = 4.0f;\nlet y_offset: f32 = 3.50f;\n\nlet z_scale: f32 = 2.0f;\nlet z_offset: f32 = 0.42f;\n\n\nlet x: f32 = 7.2f;\nlet y: f32 = 5.5f;\nlet z: f32 = 2.2f;\n\nlet x_quantized: u64 = ((x - x_offset ) / x_scale) as u64;\nlet y_quantized: u64 = ((y - y_offset ) / y_scale) as u64;\nlet z_quantized: u64 = ((z - z_offset ) / z_scale) as u64;\n\nlet x_hash: u64 = (x_quantized &amp; 0xFFFFF) &lt;&lt; 40;\nlet y_hash: u64 = (y_quantized &amp; 0xFFFFF) &lt;&lt; 20;\nlet z_hash: u64 =  z_quantized &amp; 0xFFFFF;\n\nlet hash: u64 = x_hash | y_hash | z_hash;\n</code></pre> <p>We use an offset to move all of the values to have the smallest represented value start at 0. We then scale our float to have the maximum float value be the biggest integer we want to use. Usually this will involve finding the maximum float value in our data set and knowing how many indices we at most want in our grid. I have made a small example below -</p> <p></p>  Spatial hashing implicitly defines a grid. That grid can have offsets and scales. Just be sure you don't end up not covering your domain without having a definition for points falling outside the domain.  <p>As you can see we risk having a value (C) reside outside of our quantized domain, we should have a plan for what to do if that happens or make sure that the float domain is completely covered by our quantized domain. When we quantize our floats we inevitably have some loss of precision. This loss is represented by the arrows pointing from values A and B to the nearest grid point. It is all rounding down, when done in this fashion. We could have instead chosen to round our numbers to the nearest integer. If we use this spatial hash as the key into a hash map we could keep a vector of points as the value, like <code>HashMap&lt;u64, Vec&lt;[3; f32]&gt;&gt;</code>. In that case we could have a 3D point like above. Find the spatial hash key for the 3D point, and then push our 3D point onto the Vec. This would essentially be binning. We would eventually have buckets of every point within some 3D space which we defined through our spatial hash function.</p> <p>If we wanted to get really nasty about it, we could use principal component analysis of the covariance matrix of the points to figure out a new rotated coordinate system. Then we would have to introduce a transformation term, in which case we might as well replace the scale and offset with a full 4D matrix, but it would allow for us to maximize the precision of our representation.</p>"},{"location":"m3_types/s4_bit_tricks/#morton-codes-z-order-curves","title":"Morton Codes / Z-Order Curves","text":"<p>Morton encoding is the natural extension of spatial hashing. We just saw a grid resulting from segmenting one number to represent more numbers. But what if interleaved the bits instead? This results in what is known as a Z-Order Curve, encoded and decoded through Morton coding.</p> <pre><code>let x: u32 = 8;\nlet y: u32 = 1;\nlet z: u32 = 2;\n\nlet component_count: u32 = 3;\nlet bits_of_precision: u32 = 10;\n\nlet mut encoded: u32 = 0;\nlet mut mask: u32 = 1;\nfor index in 0..bits_of_precision {\n    encoded |= ((x &amp; (1 &lt;&lt; index)) &lt;&lt; 2*index);\n    encoded |= ((y &amp; (1 &lt;&lt; index)) &lt;&lt; (2*index + 1));\n    encoded |= ((z &amp; (1 &lt;&lt; index)) &lt;&lt; (2*index + 2));\n}\n</code></pre> <p>Some mad lads looked at optimization of Morton codes. The above code snippet was just a Rust version of what was written in the aforementioned blog post under the \"For-loop based method\"-heading. The blog post also has a nice visualization of what Morton codes look like, as opposed to the grid.</p> <p></p>  A four bit, two-dimensional Morton code.  Image credit  <p>In the blog post there is even a lookup table (LUT) version mentioned which removes all of the loops. Using LUTs yields an advantage which increases with the size of the numbers as the number of iterations in the for-loops increases. It does however require you to copy paste or generate a LUT which you can reuse. You tradeoff a lot of for-loops for doing a few memory look-ups instead. It seems to be the favoured way of doing it when you search for Mortorn encoding libraries.</p> <p></p>  The spatial layout of Morton coding compared to Hilbert coding.  Image credit  <p>We can use Morton codes to do stuff like creating spatial data structures and interestingly, this is how textures are stored on GPU's. This allows for better cache coherence when working with textures. Imagine you want to draw some point between 4 different pixels of an image. You can either just go to the nearest pixel in the texture, or you can do some filtering function between the 4 nearest pixels, like bilinear interpolation. Then we need good locality to get the 4 pixels as quickly as possible. If you go back and look at the spatial ordering of the Morton layout, you'll see that if you get lucky, the four pixels can be either very close to each other in memory, or pretty close. They aren't an entire row apart.</p> <p></p>  The effect of choosing nearest neighbor interpolation instead of linear interpolation.  Image credit"},{"location":"m3_types/s4_bit_tricks/#fast-inverse-square-root","title":"Fast Inverse Square Root","text":"<p>Previously, I wrote that you should never do bitwise operations on floating point numbers. Well, now we're gonna look at a, if not the most, famous black magic methods directly manipulating the bits of a floating point number. This method comes from computer graphics where the normalization of vectors (changing their magnitudes to 1), is constantly used to ensure that the directions are correct. To do this you divide each element of the vector by the length. To find this length we need the square root of the sum of each element squared. The square root is quite expensive to compute and as you may recall, multiplication is much faster than division. In most graphics, the loss of precision in multiplying by the inverse value instead of division by the value, is acceptable. And thus if we could find the inverse square root of the sum of squared elements, it would be faster.</p> <p>This is where this gem known from the development of Quake 3 enters the scene -</p> <pre><code>float Q_rsqrt( float number )\n{\n long i;\n float x2, y;\n const float threehalfs = 1.5F;\n\n x2 = number * 0.5F;\n y  = number;\n i  = * ( long * ) &amp;y;                       // evil floating point bit level hacking\n i  = 0x5f3759df - ( i &gt;&gt; 1 );               // what the fuck?\n y  = * ( float * ) &amp;i;\n y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration\n// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed\n\n return y;\n}\n</code></pre> <p>It even has its own wikipedia page. To be honest, I watched this video about it some years ago, and can't quite remember how it works. It casts the float to an integer (long), and then manipulates the underlying bits through knowledge of the mantissa. It casts it back to a float, and then uses a single iteration of Newton's method for reducing the error. If you read the rest of the wiki, you will see there have been subsequent improvements to the accuracy, elimination of undefined behavior. The algorithm resulted in a four times faster execution than just getting the square root directly. Short after, hardware implementations started cropping up which outperformed it both in performance and in accuracy. You can even find it in WGSL through calling <code>inverseSqrt(data)</code>.</p> <p>Often you can find hardware implementations of often needed mathematical functions. Especially for GPU programming this can yield a performance boost. These are called intrinsic functions. Usually, they will have a lower precision. The instrinsic cosine might be computed in 24-bits for example, but will probably outperform the software version by a large margin, if the hardware support is available.</p>"},{"location":"m3_types/s4_bit_tricks/#additional-reading","title":"Additional Reading","text":"<p>If you've read all of this, you should now be ready to look at quantization and quantization aware training in PyTorch.</p>"},{"location":"m3_types/s5_compression/","title":"3\ufe0f\u20e3 Compression","text":"<p>Why should you care about compression? In most cases it will be handled for you, but if you are doing a real-time system, in a lot of cases you will have time for preprocessing. In preprocessing you can compress your data which, while requiring some encoding/decoding to then use your data, can relax the pressure on bandwidth, reduce download or upload times and reduce the amount of memory used. It is already heavily used in everything to do with video and audio.</p> <p>In compression you have to distinguish between lossy and lossless compression. With lossless compression the original data is recoverable down to the bit, where as with lossy compression we have some, usually variable, amount of loss of quality. This quality might not even be perceptible, or you can calculate how much is acceptable for your situation, allowing you reap a vast amount of memory savings. Choosing the right compression scheme and transforming your data, with techniques like quantization, packing and sorting, to maximize the compression scheme's effectiveness often requires domain knowledge, but the results can be spectactular.</p> <p>You will find lossy compression in formats like JPEG (2D images) and MPEG (2D videos), using codecs like H.264. You will find lossless compression in stuff like PNG and ZIP. Some of these are ubiquitous enough that dedicated hardware is available. For example, a lot of GPU's have dedicated hardware for video encoding and decoding, for stuff like streaming. Compression times and characteristics vary wildly. Some are really good with text, some are good with numerical data. Some take a really long time to compress or decompress. Some are even supported in hardware by GPU's like video encoding/decoding. Knowing how external compression libraries work can help you do transforms on your own data to maximize the performance of the compression libraries.  </p> <p>Ok, so what is compression actually? In short, exploiting the structure and repetition of data to encode it in a smaller representation. This is tightly connected to types, which is why it is in this module. For example, floats are really hard to compress well, why? Because of repetition. As you might recall, it is generally bad form to compare floats exactly. Instead you need to take the absolute difference and confirm that the two numbers are within some threshold distance of each other, not to be considered the same, but similar enough. Compression thrives on sameness. If we had a row of 1,000 pixels in an image, all of which were the exact same color, we could instead represent the entire row by the amount of pixels in the row and the value itself. Thus replacing 1,000 values by 2! Think of the savings! This is what is known as run length encoding and is one of the easiest ways of transforming your data before running it through a more general compression scheme (which might itself use run length encoding). This also somewhat echoes the various ways we tried to do jagged arrays in <code>m1::s0</code>.</p> <p>If we wanted to use the run length transformation, we could then in turn begin transforming our input data (the image) to maximize this similarity. After all, if we had a row of 1,000 pixels and they all had different values, our simple run-length encoding would balloon up to double the size. We could make ensure that the values became more similiar. If all of the colors were in 16-bit, we could lower the precision to 8-bits by either dividing all of the values by 256, or we could introduce a defacto variable precision reduction by nulling bits from the right in all of the values (I'm assuming integers here) to get the precision we could live with. If we could at most live with a reduction from 16-bit to 11-bit precision in our colors, we could set the 5 least significant bits in all of the values to 0. We have then effectively increased the likelihood of any given value being similar to its neighbor. We still have to keep it in memory as 16-bits per element, mind you, but when compressed/encoded it will take up much less space.</p> <p>We could take things even further and delta encode all of the values. We do this by having the starting value be the initial value, and every subsequent value being the difference of that value relative to the preceeding value. In that case we now have even more numbers that are similar. This does however make the decompression take longer. You now have to de-delta encode all of the values to use them.</p> <p>For a brief introduction to both the classic Lempel-Ziv schemes look under week 10 on this page. Another classic is Huffman Encoding.</p>"},{"location":"m3_types/s5_compression/#point-cloud-compression","title":"\ud83e\uddec Point Cloud Compression","text":"<p>This is mostly for people interested in graphics. I will showcase a few examples of transformations you can do to compress point clouds, which is based on some real-world experience.</p> <p>First up, let's set the scene. A user uploads a point cloud to your system. You have time to preprocess the point cloud and at some point the user will see that the point cloud has been processed and is ready for viewing. The point clouds can be massive. Much bigger than can fit in the VRAM of a GPU, or even the RAM of the users system. Progressive rendering is chosen to render. In that case the frame buffer is cleared every time the user moves the camera and the relevant parts of the scene is accumulated into the frame buffer, using the depth buffer to discriminate between which points should be in view. Frustum culling is also implemented.</p> <p>To prioritize which points to render first, and which points to not render at all, we need a data structure which lets us make decisions about which packages to even download to the user's system before we download them. We choose a type of level-of-detail octree. The regular structure of the octree allows us to make some nice decisions later on, which are encoded implcitly by our knowledge of the chosen structure. A level-of-detail structure for point clouds means that the top node of the octree will cover the entire scene. We will choose some maximum amount of points which can reside in this root node. Those points have to cover as much of the scene as possible, preferably in as visually nice a way as possible. The fewer nodes at as low levels-of-detail as we can get away with, will allow for a very high frame rate during navigation of the scene so the user can find the angle they need and then set the camera to rest so we can accumulate the relevant points and converge on the \"true\" image.</p> <p>We will build this level-of-detail octree by using spatial hashing. Instead of starting with a tree representation, think back to nodes and pointers, we will start with a list of hash maps. In Rust the pseudo tree representation would look something like <code>let mut pseudo_tree : Vec&lt;HashMap&lt;u64, Vec&lt;Point&gt;&gt; = ...</code>. Alternatively, given that we know how much each list maxes out at we could just use a fixed size array and an index to which points are active. So for each level-of-detail we will keep a hash map. Each level-of-detail will have the same offset vector, but a diminishing scale. We can pick any factor, but if we choose 2.0 we get some nice side effects later on. So if we set the scale of the root node to 100.0 meters for each axis, level-of-detail 1 would have a scale of 50.0 meters per node for each axis. This will continue on until level-of-detail 10 where we have a scale of 0.09765625 meters, or 9.7 cm per node. Clearly, we now have a very small area, being covered by quite a lot of bits, 20! And how we will turn that to an advantage I will get back to.</p> <p>We just have to clear up one thing, subsampling. It is not that important in terms of compression, but this data structure will reuse the spatial hashing concept quite a bit, and we will do so too when choosing which points go where.</p> <p>To build our tree we start finding the minimum and maximum of the scene covered by the given point cloud. We create a node defined by the minimum and maximum. The offset becomes the minimum vector and the scale vector will be defined by the vector from the minimum to the maximum value. Sometimes, the subsampling can come out nice if we just use isometric axes. In which case the scale vector would be the same as the maximum value in the difference between the minimum and maximum values. Then we might as well not use a scale vector, but a single scale value.</p> <p>Ok, so we found the area that our root node needs to cover, now we start adding points until we hit a chosen threshold value. Lets say 128k points. Once we hit that limit we subsample and keep only half that amount of points. The subsampling strategy should be reasonably performant and make for a reasonably aesthetic coverage of the scene. We will start with an empty hash map to subsample the points. Having the cells only be occupied by one point at a time is a good basis for subsampling. The optimum would be a 3D poisson disc subsampling. It looks very nice, but to minimize the distances between neighbors, we would need to query for any active neighboring cells, which in 3D is 27 cells we need to query for every single one of our initial 128k points. Instead, if we need a bit of speed, we can get a much better performance and a reasonable aesthetic by minimizing the distance to the center of the cell.</p> <p>So. Our algorithm for subsampling a list of points will be this. Get list of points, offset and scale. Create an empty hash map, almost like the one we saw earlier, the keys are <code>u64</code> and the values are <code>Point</code>. For each point in our list we compute the spatial hash of that point. We query the hash map for whether that cell has already been created. If it has not, we create it and insert the point as the point held by the cell. If there was already a point in the cell, we compare their distances to the center of the cell. Whichever point is rejected, is added to a list of subsampled points. Once we are done with this process we have to lists of points. Once list we keep in the node, the other list we send to the next level-of-detail. The second list will then be distributed among the up to 8 child nodes. The child nodes are implicitly linked by their places in the list of hash maps and not by pointers. Once we have done this for all nodes until we are done filling out our implicit octree, we can finalize the structure. We do this by now enforcing a maximum amount of points and continue adding and subsampling until each node holds at most N points.</p> <p>Phew, that was a lot, but now we are ready to do some compression! Throughout this process we have kept all of the points in floating point coordinates. Ideally, as close to their original precision as possible. But, once we are done. We know where each point should be and we can act accordingly and only once. This data will be streamed to a users computer. They will be viewing the model through the web and won't have all of the model on their own system. So we will zip all of the points with a web friendly lossless compression scheme. Brotli is well supported by browser and was tried, this was some years ago, so the performance may be different now, but it required a lot more compression and decompression time. Instead the venerable gzip was chosen.</p> <p>So we know we are going to compress each node in our implicit octree. The rendering details aren't important/relevant to this section, but there is a limit to the resolution a user can resonably demand to see in a web browser, while at the same time we don't want to induce too big of an error as users may want to use the system for measurements. A maximum induced error of 0.5 mm is probably reasonable. Our area covered per node at level-of-detail 10 was 9.7 cm. Each node can have up to 64k points, so let's start off with the easiest optimization we can always make. Doing less. Having 64k points covering a 9.7x9.7x9.7 cm cube, is quite a bit. So let's just throw away all points after that. We enforce the limitation that we have at most 11 levels-of-detail. We could go even further and start merging all points which were sufficiently close to each other whenever we subsample, some users are bound to be mad lads and send you point clouds with a greater than 0.1mm precision. But we would need some form of policy for doing so which the user would have to find acceptable. Anyways, if we know we level-of-detail 10 will be our most precise level, we can make another observation.</p> <p>Do users really care if you have better precision, due to the smaller area covered by the same amount of bits, at higher levels-of-detail compared to the root node?</p> <p>The answer is of course, no. Most users will not care. And given that we previously chose that each successive level-of-detail will have a scale that is exactly half of the preceeding level-of-detail, we can now do bit nulling. First we will quantize all of the floats to 16-bit unsigned integers specific to the node, using the nodes scale and offset. The node will carry around some information, but that isn't too important. We won't do bit nulling with level-of-detail 0, but for level-of-detail 1 we will set the single least significant bit to 0. For level-of-detail 2, we will set the two least significant bits to zero. In the end, we will only have 6 bits of precision per coordinate axis for level-of-detail 10. So 6 bits, 64 values to cover 9.7 cm each. This leaves us with a precision of 0.1515625 cm, or 1.515 mm. Given that we quantize through flooring, we end up with at most inducing an error of 1.515mm. If we instead started our bit nulling at level-of-detail 2, we would have double the precision and have a maximum induced error of 0.75mm.</p> <p>But hold up, we can only guarantee this precision if our scene is at most 100 m in scale. What if your scene is bigger? This is where we veer off the standard octree path, once again. We will just have more than one root node.</p> <p>Think back to the part about making numbers more similar. We can do this for our list of points by sorting the points with techniques like Morton codes, Radix sorting or other schemes. As found in this paper by Sch\u00fctz, et al., sorting the points, which we can do in preprocessing, can not only have an effect on the effectiveness of compression, but also the effectiveness of rendering.</p> <p>We could also do delta encoding of all of the points, which would work better now that the points were all sorted. This would be fine in the preprocessing step, but incurred too much CPU work to de-delta encode on the users' systems. Once the each node has been received by the users' systems, unzipped and sent to the GPU, the 16-bit unsigned integers will be dequantized back into floats using a transformation matrix. This is a rendering detail.</p> <p>Another option to make the numbers more similar would be to deinterleave the list of points and putting all of the X values next to each other, followed by all of the Y values, all of the Z values and all of the colors. This again, requires the users' system to touch every single point before uploading to the GPU, or to use 4 bindings per list of points in the shader.</p> <p>And here we are. This was a small example of how domain knowledge can help you compress your data, even if you aren't writing the compressor yourself.</p>"},{"location":"m3_types/s5_compression/#additional-reading","title":"Additional Reading","text":"<p>Information Theory Data Compression Lempel-Ziv 77 &amp; 78 Huffman Coding </p> <p>Zip Gzip - HTTP Compression </p> <p>Lots of interesting things are happening in real-time decompression using neural networks.</p>"},{"location":"m3_types/s6_cooperative_matrices/","title":"\ud83e\uddec3\ufe0f\u20e3 Cooperative Matrices","text":"<p>This is mostly for people interested in deep learning or programming GPUs to the fullest. This should also include people interested in graphics due to DLSS, denoisers and upscaling becoming so prevalent. Cooperative matrices are known as tensor cores on Nvidia systems.</p> <p>Anyways, a GPU is a complex beast, but this section will specifically talk about one area of the chip that was introduced into the mainstream consumer cards starting with the RTX 20-series. Ironically, the GPU started off as quite specialized for graphics workloads, as in drawing triangles fast, but was opened up and made increasingly flexible. With the introduction of tensor cores and ray tracing cores it somehow became both more flexible and more specialized at the same time. I won't get into the ray tracing cores in this section, but it's pretty awesome!</p> <p>Cooperative matrices are basically ALU's made for linear operations on small tiles of matrices. They are really good at multiplying a small matrix by another small matrix and adding another small matrix, while accumulating the result. It can do it in a number of different levels of precision.</p> <p></p>  Nvidia's visualization of the mathematical operation a tensor core can perform.  Image credit  <p>While the tensor cores support matrix-matrix multiplication, they are much more limited in the size of the multiplication. For a general linear operation, you might need to still uses loops, but you would then be tiling your matrix instead, sending a 4x4 tile at a time of your matrices and keep track of your precisions, such as accumulating in a higher level of precision. You can read more about it here and for Vulkan here.</p> <p>If you keep the calculations numerically stable you can even keep all of your weights during the training of a neural network in 8-bit floating point, while accumulating in 16-bit floating point or greater, which will greatly reduce the bandwidth needed for training. For inference, it can also yield a big speedup all the way down to 8-bit integers. Remember, that integers compress better than floating point numbers. So if you do quantization of your entire network before inference, you can get faster inference and lower power consumption. The cooperative matrix was first available in CUDA, but has since been made available in Vulkan, although the usage of it is not that wide spread yet as it is a fairly recent addition.</p> <p></p>  Nvidia's visualization of a cooperative matrix.  Image credit"},{"location":"m3_types/s6_cooperative_matrices/#additional-reading","title":"Additional Reading","text":"<p>Tensor Cores Programming Tensor Cores Vulkan Cooperative Matrix Machine Learning in Vulkan Accelerating Inference with Sparsity A series of videos regarding how to use tensor cores for mixed precision training .</p>"},{"location":"m3_types/s7_exercises/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Exercises","text":"<p>Fma Radix Morton Spatial Hashing? Quantization Loss  </p>"},{"location":"m4_optimization/","title":"3\ufe0f\u20e3 Optimization","text":"<p>Much like the memory hierarchies there's levels to measuring performance. The easiest is just using your system's own task manager. On Windows you just click \"ctrl+shift+esc\" and click on the performance tab. No putting in print statements, no timing of functions, no installation, no nothing. You can see many of your cores are being used, how much memory (RAM) is being used, how is you GPU doing, how much is the disk being used. All of that. You will have to deduce what is happening, but if you are training a neural network, and you wonder why training is so slow, whip up task managers performance tab, see that the disk is being used quite aggresively while you are using hardly any of the RAM or GPU RAM, then you are probably loading all of your data from disk at all times without trying to save it in any type of RAM. Keeping your data in a higher level of the memory hierarchy should probably be your first move without spending too much additional time figuring out what the problem is.</p> <p>Using something like task manager requires very little work and is easy to do, but it will get you a view of all of the programs currently running on your system. At best you can check what the system is using without your program and deduce what your program is doing. But much like the black box approach to understanding the performance of someone else's library, trying to deduce the characteristics of your program through the task manager might not be enough.</p> <p>Next up, you can insert print statements and measure the timing of functions yourself. This won't give you any information about how many threads are in play, how much memory is used, or how many L2 cache misses you have, but it will allow you to measure the functions in your own code directly. This might require you to radically change your code in order to get adequate measurements and you might have to examine more closely what it is you want to measure. If you are developing a real-time system, the variance in the time to compute each frame might be relevant, pipeline bubbles could be relevant, memory usage over time (to ensure you don't have a memory leak) could be important, you may want to automatically create graphs of the system's performance or you might decide to benchmark functions individually, perhaps even automatically as part of your automated CI to ensure that none of your nifty, high performing, functions get slower with new commits. Creating benchmarks for individual functions can take a bit more work as you also have to generate some test cases to use as input.</p> <p>Depending on what system you are on, you can have easy access to programs like perf, which is available on Linux. This command line tool allows you to run your full program and can output performance statistics like which functions are the \"hottest\", as in, in which functions does your program spend the most amount of time. To get meaningful output in situations like these your program needs to be compiled with debug symbols. This means that every function in your program doesn't just have a mangled named like <code>xYz289421</code>, but a human readable name like <code>my_function()</code>. The computer doesn't care, that name is for you.</p> <p>Things like that, and which line is run in which order can easily get mangled based on which flags you gave the compiler. You should always debug in debug mode, but for measuring performance, you should in most cases measure it as close to the final program as possible, but with the caveat of retaining symbols and other likely relevant information. What tools like <code>perf</code> might not be able to help you with (not quite sure) is the nitty gritty details of exactly what is going on in your CPU. In that case you may need to get use a profiler from the specific hardware vendor who made the CPU, or in the case of GPU programming (which <code>perf</code> is not attuned to), a GPU profiler from the GPU vendor. Thus for GPU programs you might end up profiling twice, separately. Once for your CPU code and once for your GPU code. These sorts of profilers, which can be great, awesome and very visual tools, can be a pain to install and are so platform specific that I won't go into too great detail. They are specific tools you have to install and learn for your specific platform. You should make sure that you really need that extra information before spending the extra time, and possibly frustration.</p> <p>Finally, some frameworks might even have their own profiler, such as the profiler that is in your browser for web programming, or the profiler that is provided with deep learning frameworks such as PyTorch.</p>"},{"location":"m4_optimization/#additional-reading","title":"Additional Reading","text":"<p>Full-Stack, GPU-based Acceleration of Deep Learning Writing Performant Concurrent Data Structures </p>"},{"location":"m4_optimization/s0_timing_and_printing/","title":"3\ufe0f\u20e3 Timing and Printing","text":"<p>As mentioned in the previous section there's a plethora of options in how to measure performance. But in this section, let's look at what to measure.</p> <p>Let's assume we have a basic program like the following -</p> <pre><code>fn version_0() {\n    a();\n    b();\n    c();\n    d();\n}\n</code></pre> <p>We have written the full program, you can check it out in <code>m4_optimization::code::measuring_performance</code>. We have verified the correctness of our program. We have set up tests to continually ensure the continued correctness of our program. We are now ready to improve the runtime of our program. I haven't actually done this as I don't actually care about whether this specific program is correct. This is not the norm of course. The fastest thing right now would be to either put in timing and print statements or to open up our system wide performance monitor and see if anything glaringly wrong is happening. Because I have something else coming up later where the performance monitor will be the most relevant, let's go down the time and print path.</p> <p>In this case we will include some timing functionality from the standard library at the top of the file and time the whole program.</p> <pre><code>use std::time::{Duration, Instant};\n</code></pre> <p>Except, hold up. While doing this, I forgot a critical property that almost invariably happens in education about performance. Because we are running very simplified programs, dead code elimination by the compiler has a huge effect. I didn't properly use any of the results of the functions, so the Rust compiler, the helpful helper that it is, decided to make my program much faster, by not doing any of the work as the work was used nowhere. This was apparent when I timed all 4 functions, as the result was always either 100 or 200 nanoseconds. This sort of reeks of noise floor. So our new <code>version_0()</code> will look like this -</p> <pre><code>fn version_0() {\n    let mut sum: f32 = 0.0;\n    sum += a();\n    sum += b();\n    sum += c();\n    sum += d();\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>This forces the Rust compiler to not eliminate our function calls. So now, we can time the entire program as such -</p> <pre><code>fn version_1() {\n    let mut sum: f32 = 0.0;\n    let now: Instant = Instant::now();\n    sum += a();\n    sum += b();\n    sum += c();\n    sum += d();\n    let elapsed_time: Duration = now.elapsed();\n\n    println!(\"version_1 ran for {} milliseconds\", elapsed_time.as_millis());\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>Now we get an output of 1 miliseconds. Clearly, we have chosen a time resolution which was too course. If we switch to microseconds we get 1794 microseconds. But we don't know which function is taking the most time. I am going to keep it in miliseconds in the outer function, but the time resolution will probably get finer the further down we go.</p> <p>So let's time each function individually.</p> <pre><code>fn version_2() {\n    let mut sum: f32 = 0.0;\n\n    let now_a: Instant = Instant::now();\n    sum += a();\n    let elapsed_time_a: Duration = now_a.elapsed();\n    println!(\"version_2_a ran for {} milliseconds\", elapsed_time_a.as_millis());\n\n    let now_b: Instant = Instant::now();\n    sum += b();\n    let elapsed_time_b: Duration = now_b.elapsed();\n    println!(\"version_2_b ran for {} milliseconds\", elapsed_time_b.as_millis());\n\n    let now_c: Instant = Instant::now();\n    sum += c();\n    let elapsed_time_c: Duration = now_c.elapsed();\n    println!(\"version_2_c ran for {} milliseconds\", elapsed_time_c.as_millis());\n\n    let now_d: Instant = Instant::now();\n    sum += d();\n    let elapsed_time_d: Duration = now_d.elapsed();\n    println!(\"version_2_d ran for {} milliseconds\", elapsed_time_d.as_millis());\n\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>Which gives us this output -</p> <pre><code>version_2_a ran for 0 milliseconds\nversion_2_b ran for 0 milliseconds\nversion_2_c ran for 1 milliseconds\nversion_2_d ran for 0 milliseconds\n</code></pre> <p>Ok, so <code>c()</code> is likely to be taking up more of the execution time than the others. But we don't know how much. Every other function could be 999 microseconds and function c could be 1000 microseconds. We could again increase the resolution of the timing, but in this case we have something else we should think about. We have a background usage of the system. We should try and turn off as many other processes as possible. Other processes might jump in and take some time to do something which can disturb your measurements. Your system itself might also have a ramping up period. As the load increases it might boost the clock frequency of the CPU to accomodate your program's needs. A general rule of thumb is that your program should run for at least two seconds. We can either do this per function or for the whole program. Let's just do this per function. We can then either keep track of how many executions of the function were attained during a two second window or just do N loops of each function. Let's try both.</p> <pre><code>fn version_3() {\n    let mut sum: f32 = 0.0;\n    let iteration_count: u32 = 2_000;\n\n    let now_a: Instant = Instant::now();\n    for _ in 0..iteration_count {\n        sum += a();\n    }\n    let elapsed_time_a: Duration = now_a.elapsed();\n    println!(\"version_3_a ran for {} milliseconds\", elapsed_time_a.as_millis());\n\n    let now_b: Instant = Instant::now();\n    for _ in 0..iteration_count {\n        sum += b();\n    }\n    let elapsed_time_b: Duration = now_b.elapsed();\n    println!(\"version_3_b ran for {} milliseconds\", elapsed_time_b.as_millis());\n\n    let now_c: Instant = Instant::now();\n    for _ in 0..iteration_count {\n        sum += c();\n    }\n    let elapsed_time_c: Duration = now_c.elapsed();\n    println!(\"version_3_c ran for {} milliseconds\", elapsed_time_c.as_millis());\n\n    let now_d: Instant = Instant::now();\n    for _ in 0..iteration_count {\n        sum += d();\n    }\n    let elapsed_time_d: Duration = now_d.elapsed();\n    println!(\"version_3_d ran for {} milliseconds\", elapsed_time_d.as_millis());\n\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>Getting the output -</p> <pre><code>version_3_a ran for 22 milliseconds\nversion_3_b ran for 23 milliseconds\nversion_3_c ran for 2101 milliseconds\nversion_3_d ran for 2 milliseconds\n</code></pre> <p>As you can see, measuring things this way, yields a quite unbalanced view of each function. So let's try number of executions per second -</p> <pre><code>fn version_4() {\n    let mut sum: f32 = 0.0;\n    let millisecond_limit: u128 = 2_000;\n\n    let mut execution_count_a: u128 = 0;\n    let now_a: Instant = Instant::now();\n    while now_a.elapsed().as_millis() &lt; millisecond_limit {\n        sum += a();\n        execution_count_a += 1;\n    }\n    println!(\"version_4_a executed {} times in {} milliseconds\", execution_count_a, millisecond_limit);\n\n    let mut execution_count_b: u128 = 0;\n    let now_b: Instant = Instant::now();\n    while now_b.elapsed().as_millis() &lt; millisecond_limit {\n        sum += b();\n        execution_count_b += 1;\n    }\n    println!(\"version_4_b executed {} times in {} milliseconds\", execution_count_b, millisecond_limit);\n\n    let mut execution_count_c: u128 = 0;\n    let now_c: Instant = Instant::now();\n    while now_c.elapsed().as_millis() &lt; millisecond_limit {\n        sum += c();\n        execution_count_c += 1;\n    }\n    println!(\"version_4_c executed {} times in {} milliseconds\", execution_count_c, millisecond_limit);\n\n    let mut execution_count_d: u128 = 0;\n    let now_d: Instant = Instant::now();\n    while now_d.elapsed().as_millis() &lt; millisecond_limit {\n        sum += d();\n        execution_count_d += 1;\n    }\n    println!(\"version_4_d executed {} times in {} milliseconds\", execution_count_d, millisecond_limit);\n\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>Now we get the executions per second -</p> <pre><code>version_4_a executed 188668 times in 2000 milliseconds\nversion_4_b executed 187431 times in 2000 milliseconds\nversion_4_c executed 1888 times in 2000 milliseconds\nversion_4_d executed 1804447 times in 2000 milliseconds\n</code></pre> <p>Clearly, it's no secret that function c takes a lot more time than the other functions. One of the real-life applications we can compare this to is a real-time system where we care about the frames per second. We can either measure the time it takes to process or render a single frame or we can measure how many frames per second we can sustain. One crucial bit in an interactive real-time system is the variance. If you are churning out 60 frames per second, but frame 0 takes 0.9999 seconds and the other frames each take 0.0000001 seconds, the user is getting a very choppy experience. Even if it results in a lower frames per second, like going from 60 to 50, distributing the processing time a bit would be a smoother experience. You could also measure the average frames per second in a moving window fashion while moving through a scene to see which areas of a scene might need tweaking performance wise.</p> <p>What we'll look at instead is getting the variance calculated alongside our other metrics. This requires a bit more admin work and we have to keep track of individual measurements in order to get a resonably accurate measurement. In order to not disturb the characteristics of the function we are measuring too much we should probably pre-allocate space for our measurements, which becomes our sample size. Continually pushing new values into a dynamic array would result in lots of allocations, which would be bad. We are also now going to measure the mean execution time of each function as well as the variance.</p> <pre><code>fn get_mean_and_variance(measurements: &amp;Vec&lt;u128&gt;) -&gt; (f32, f32) {\n    let sum: u128 = measurements.iter().sum();\n    let mean: f32 = sum as f32 / measurements.len() as f32;\n\n    let mut variance: f32 = 0.0;\n    for element in measurements {\n        let squared: f32 = (*element as f32 - mean) * (*element as f32 - mean);\n        variance += squared;\n    }\n\n    variance /= measurements.len() as f32;\n\n    (mean, variance)\n}\n\nfn version_5() {\n    let mut sum: f32 = 0.0;\n    let measurement_count: usize = 2_000;\n    let mut measurements: Vec&lt;u128&gt; = vec![0; measurement_count];\n\n    for measurement_index in 0..measurement_count {\n        let now: Instant = Instant::now();\n        sum += a();\n        let elapsed_time: Duration = now.elapsed();\n        measurements[measurement_index] = elapsed_time.as_micros();\n    }\n    let (mean, variance) : (f32, f32) = get_mean_and_variance(&amp;measurements);\n    println!(\"version_5_a ran with mean {} microseconds and variance {} microseconds squared\", mean, variance);\n\n    for measurement_index in 0..measurement_count {\n        let now: Instant = Instant::now();\n        sum += b();\n        let elapsed_time: Duration = now.elapsed();\n        measurements[measurement_index] = elapsed_time.as_micros();\n    }\n    let (mean, variance) : (f32, f32) = get_mean_and_variance(&amp;measurements);\n    println!(\"version_5_b ran with mean {} microseconds and variance {} microseconds squared\", mean, variance);\n\n    for measurement_index in 0..measurement_count {\n        let now: Instant = Instant::now();\n        sum += c();\n        let elapsed_time: Duration = now.elapsed();\n        measurements[measurement_index] = elapsed_time.as_micros();\n    }\n    let (mean, variance) : (f32, f32) = get_mean_and_variance(&amp;measurements);\n    println!(\"version_5_c ran with mean {} microseconds and variance {} microseconds squared\", mean, variance);\n\n    for measurement_index in 0..measurement_count {\n        let now: Instant = Instant::now();\n        sum += d();\n        let elapsed_time: Duration = now.elapsed();\n        measurements[measurement_index] = elapsed_time.as_micros();\n    }\n    let (mean, variance) : (f32, f32) = get_mean_and_variance(&amp;measurements);\n    println!(\"version_5_d ran with mean {} microseconds and variance {} microseconds squared\", mean, variance);\n\n    println!(\"sum was {}\", sum);\n}\n</code></pre> <p>And we get the following output -</p> <pre><code>version_5_a ran with mean 10.116 microseconds and variance 1.1695418 microseconds squared\nversion_5_b ran with mean 10.315 microseconds and variance 0.94678104 microseconds squared\nversion_5_c ran with mean 1046.928 microseconds and variance 1914.6855 microseconds squared\nversion_5_d ran with mean 1.217 microseconds and variance 0.17091024 microseconds squared\n</code></pre> <p>If we simulate an uneven workload across executions by making a more erratic function for <code>a()</code>. A random number is generated for each call to the <code>a_erratic()</code>, 1 out of 60 calls will result in an execution time which should be approximately be 100 times slower. We get the following results -</p> <pre><code>version_6_a ran with mean 23.2655 microseconds and variance 13236.803 microseconds squared\nversion_6_b ran with mean 10.359 microseconds and variance 0.77912927 microseconds squared\nversion_6_c ran with mean 1066.4165 microseconds and variance 2758.1538 microseconds squared\nversion_6_d ran with mean 1.0035 microseconds and variance 0.0034878778 microseconds squared\n</code></pre> <p>Clearly, we should be getting the hint that something is irregular about the execution of our function <code>a_erratic()</code>. Another thing we could do, if these functions took input data, would be to benchmark the execution times across various input data sizes and graph it. I won't do it in this function, it is a bit more involved. But it is how the graphs were generated in the framework for computational graphs. You can find the code for making the graph based on vectors of performance measurements in <code>m1_memory_hierarchies::code::computational_graphs::src::shared::benchmark_plot.rs</code>.</p> <p></p>  An example of a timing plot, comparing different implementations across different input sizes.  <p>This way of benchmarking different implementations won't help us find bottlenecks in our specific system. To do that we can either keep timing and printing, going deeper and deeper. Or get a profiler to measure which functions in our code are the biggest hot spots. These profilers are system and hardware dependent, so I will suggest ones you can try out in the next section.</p>"},{"location":"m4_optimization/s1_profilers/","title":"3\ufe0f\u20e3 Profilers","text":"<p>So we just did some preliminary timing of your program. It would have taken even less time to just look at your systems performance monitor, although that would only allow you to guess about what was taking most of the processing time. Once you have tried both timing your code, and you've gotten what you could out of the system performance monitor, it might be time to install and use a profiler.</p> <p>If you are profiling Rust, make sure you leave in the debug symbols when compiling. Otherwise the profiler won't be able to tell you which function is which.</p>"},{"location":"m4_optimization/s1_profilers/#system-monitors","title":"System Monitors","text":"<p>Let's start off looking at your system monitor to see where something might be obviously wrong. I will be using Task Manager on Windows to exemplify where you might look for the various bottlenecks. The quickest way to check is following the memory hierarchy. Disk, memory, CPU and then optionally GPU. If your disk is maxed out, then a good idea for your next optimization might be to focus on disk utilization.</p>"},{"location":"m4_optimization/s1_profilers/#disk","title":"Disk","text":"<p>In the disk tab we can see the name of the disk, its maximum read and write speeds, usually read will be faster than write. You can also see whether you are mostly using read or write bandwidth.</p> <p></p>  The Disk tab of Task Manager.  <p>This is where you want to start, but disk can suffer some spillover from memory. If you have used all of your memory, the operating system will use swapping. If you had say, 16 GB of memory, and you are full up, but try to allocate more memory in your progran, the operating system can either throw a \"out of memory\" error or it can make use of the disk to supplement the memory. It will write some of the data currently residing in memory to a swap file and write it to disk. It is possible to configure how big a swap file the operating system will create at most, but in any case, if your system has to use a swap file, your program is going to be slow. So either put in more RAM or optimize your program to use less memory.</p>"},{"location":"m4_optimization/s1_profilers/#memory","title":"Memory","text":"<p>In the memory tab, we can see how much physical memory is present and how much is currently being used. Interestingly, we can also see what the frequency of the memory is. It's named \"Speed\" in Task Manager.</p> <p></p>  The Memory tab of Task Manager.  <p>Other than that, you mainly need to worry about one thing. If one of the numbers on the page is greater than the physical amount of memory you have, you are probably swapping to disk. If the memory usage is also constantly rising, you should probably think carefully about whether you might have a memory leak, or you are allocating when you weren't intending to.</p>"},{"location":"m4_optimization/s1_profilers/#cpu","title":"CPU","text":"<p>In the upper corner we can easily checkout which model of CPU we have, going to the bottom we can see the size of our caches, the base clock frequency of cores and how many cores are available in our CPU. We can see how many processes and threads are running. When you see processes think apps. There can be multiple threads running for each process. We won't get an exact indication of how well we are using each core in this configuration, but if we at no point see utilization above 50%, then we probably aren't using all cores to the fullest.</p> <p></p>  The CPU tab of Task Manager.  <p>But, I do have to remind you of one thing. High utilization is not the same as well performing code. You can very easily max out the utilization of all cores on your system, but sometimes, optimizing your code will result in less utilization, as doing less is often a good strategy. An indicator that the program dominating resource usage is single threaded, or at least not very well parallelized will be if you have a fairly constant use of 1/N, N being the amount of cores. So, if we have a system with 4 cores, and no hyperthreading, an almost constant usage of 25%, with a few spikes here and there, we have very poor parallelization of our CPU code. To get a more detailed per core view, you can click the resource monitor button in the bottom left.</p>"},{"location":"m4_optimization/s1_profilers/#gpu","title":"GPU","text":"<p>If we are using the GPU in our application, we can also go to the GPU tab. Once again, in the upper right we can see which GPU we are using. In the bottom, we can see the driver version. DirectX version refers to the version of DirectX (Windows graphics and compute API) that the system supports. The laptop I used to take the screenshot with has what is called an integrated GPU. It shares its memory with the CPU. But in this case, the entire system has 32 GB of memory, 16 of which is shared by the GPU. While this may result in higher transfer rates to the GPU, the bandwidth internally (when you access memory in a buffer you have already transferred) is usually less than a dedicated GPU.</p> <p></p>  The GPU tab of Task Manager.  <p>In this configuration, we have 4 windows for utilization monitoring. We can actually click on the titles of the windows to choose what to show. The one called 3D will show utilization, both for graphics and pure compute workloads. Copy is our transfer utilization. On dedicated cards there will often be dedicated sections for CPU-GPU transfers and for GPU-CPU transfers. Video decode also has dedicated hardware and is typically something you might see used during streaming video, such as watch videos streamed from the internet. Or video processing if you need to encode and compress video directly from your GPU.</p>"},{"location":"m4_optimization/s1_profilers/#choosing-a-profiler","title":"Choosing a Profiler","text":"<p>If your own system's performance monitor was not detailed enough, you can make use of dedicated profilers. Profilers can be hard to install and something that depends on your use case. Below is a non-exhaustive list of profilers you can try out.</p> <ul> <li>A list of Rust profilers from the Rust Performance Book</li> <li>Web Browser</li> <li>perf</li> <li>RenderDoc</li> <li>AMD Radeon GPU Profiler</li> <li>Nvidia Visual Profiler</li> <li>Nsight Systems</li> <li>PyTorch profiler</li> <li>tracy</li> </ul> <p>Profiling on Windows is a bad experience if you are outside Visual Studio. You can try using Linux tools like perf through WSL2 on Windows. How to profile your specific GPU on Windows through WSL2, I am not quite sure. You might just be able to profile outside of WSL2 using the GPU profiler provided by the GPU vendor (AMD, Nvidia, Intel).</p>"},{"location":"m4_optimization/s1_profilers/#how-to-work-with-profilers","title":"How to work with Profilers","text":"<p>So what happens when you finally have a working profiler?</p> <p>One view you can go for is the sorted hot spot list. A lot of profilers will measure a list of the functions taking the most significant part of the execution time. Sometimes, it will by default be a cumulative list, resulting in the main function being on top and you having to explore down the list to find likely candidate functions for optimization. It can be a good idea to find where to turn this off to just show which functions individually are using the most time. You might have to switch between the two views. Sometimes you can also get the profiler to show the hot spots of individual lines of code, which will be highlighted both by color and a nifty percentage.</p> <p>If use a GPU profiler instead, you can get views like your occupancy and speed relative to the theoretical peak usage and what your memory and compute utilization is. In profiling, at least pure compute workloads, not necessarily graphics, you need to keep track of two core metrics. Compute and memory utilization. If the memory utilization is high, but the compute utilization is low, the program is memory bound. You can mitigate this by optimizing the way you work with memory. You could do less memory accesses, make increased use of shared memory or registers, or even do warp shuffling. If you are compute bound, because the compute utilization is high, but the memory utilization is low, you might need to work on the actual algorithm you are using or the mathematical functions you are using. If you are dividing in a loop, you can perhaps multiply by the inverse (division is expensive), if you don't mind the possible precision loss. You can also exchange full precision math functions such as <code>cosf(x)</code> for an intrinsic (hardware supported) version which might only be in 24-bit precision, such as <code>__cosf(x)</code>. Now remember, hardware supported, is a hint that it will likely be faster.</p> <p></p>  A view in the Nsight Compute profiler showing whether the application is memory or compute bound.  Image credit  <p>If you optimize away your compute bound application, you might end up being memory bound, and then compute bound, and so on and so on, until the memory and compute are hopefully balanced, both with a high utilization. Or low if your workload ends up not actually being that big.</p> <p>Another interesting view in the Nsight Compute profiler is the one below.</p> <p></p>  A view in the Nsight Compute profiler showing the \"heat\" in various parts of the memory hierarchy.  Image credit  <p>Here we are shown the utilization of different parts of the memory hierarchy. If you don't see the obvious way to get out of being compute bound, this, more visual, way of communicating profile statistics is a great way to see what you aren't using.</p> <p>I will leave you with this - at a celebration I was fortunate to sit next to the rendering lead of the renowned game company Playdead Games, Mikkel Gj\u00f8l. His go-to profiling advice was \"Always go straight for the L2 cache hits and misses\". I don't embroider, but if I did... I would embroider that on a pillow.</p>"},{"location":"m4_optimization/s2_optimizing_deep_learning_training/","title":"\ud83e\uddec3\ufe0f\u20e3 Optimizing Deep Learning Training","text":"<p>Now, let's go through a small example of optimization through picking the lowest hanging fruits. I'll do this on a different system, why will make sense later. This system has an Nvidia RTX2070 GPU with 8 GB of VRAM, 16 GB of 2666MHz RAM, 1TB SSD (not NVMe) disk and an Intel i7-9700F CPU with 8 cores and a base speed of 3.0 GHz. The operating system is Windows 10.</p> <p>On to the actual case. I found a very nice tutorial about super resolution networks. They take in a lower resolution image and produce another image with a higher resolution. I won't link to the tutorial, although the original author is very welcome to contact me and approve, in which case I of course will. I won't link to it as I don't want to risk this case study coming off as shaming or finger pointing. Performance wasn't the goal of the tutorial, it was to teach the reader about super resolution networks, which it does very well.</p> <p>Anyways, in the tutorial there are two different networks to train. Either a super resolution GAN or a super resolution ResNet. I chose the ResNet. The data set is 13 GB for training and 6 GB for validation. Normally, I would expect to see the epochs, where you might recall we will sample the entire training data set, get faster after the first one, as more data is loaded from disk. There is a snag which can make optimization of this problem difficult. We usually random sample batches from the entire training data set. Finally, we are limited by being unable to fit the entire training data set in either type of RAM.</p> <p>To start, let's run the code for two epochs and see what the run time is.</p> <p></p>  The baseline timing for running two epochs.  <p>So, we ran two epochs and it is murderously slow. The timing did not improve noticeably for the second epoch. Let's go over to task manager and look at the system's characteristics before we rerun the two epochs.</p> <p></p>  The system baseline in the CPU tab.  <p></p>  The system baseline in the GPU tab.  <p>We can see not a lot of stuff is going on aside from small peaks here and there. Our baseline RAM is 5.8 GB and on the GPU we use just about 1 GB. Let's run two epochs and see what comes out.</p> <p></p>  The CPU tab during unoptimized training.  <p></p>  The GPU tab during unoptimized training.  <p></p>  The disk tab during unoptimized training.  <p>As you can see, ww are not maxing out the RAM or VRAM, the GPU is barely doing anything, but using a total of 3GB VRAM now. Additionally, we can see that we are at a total of 10 GB RAM used, with a reasonably heavy load on both disk and CPU. There aren't performance statistics for the memory, but my guess is that it is covered by the CPU usage as well. Time spent waiting for memory access is under the CPU load. So let's take a look at the data loader.</p> <p></p>  The data loader.  <p>It does some setup, but most importantly... it doesn't reuse data. It ALWAYS reloads it from disk and redoes some preprocessing. In a perfect world, we would keep the entire data set loaded on the GPU, at which point we would probably optimize the GPU compute part, but what could we do about the disk and CPU load? While we could make a more complex hierarchical data loader which kept track of which data was on the GPU, which data was in memory and which was on disk, this is more work than I have time for, and given the randomized data set sampling, the probability of getting a \"cache hit\" on the GPU is around 1/3 at the start of an epoch and decreasing over time. Short of upgrading to a GPU with more RAM or compressing and preprocessing the data, there are probably other and cheaper low hanging fruit. And by cheap I mean either in money or in your/my time. Which is also a valuable resource. Instead I propose the following: upgrade the RAM and cache the data. Upgrading the RAM to an amount that allows us to keep most or all of the data set in memory will allow us to write a data cache. We will still only load the data on demand, but first we will look in our list of loaded data, if it is not in the cache, we will load the data from the disk and preprocess it. Then we will put the data in our cache after which we will yield the data. We save the preprocessing and hopefully significantly decrease the load on our disk.</p> <p>Currently, I have two 8 GB sticks of 2666 MHz RAM in the machine. I will now shut off the system and replace the two sticks with a single 32 GB stick. Also running at 2666 MHz, because that's the greatest speed my motherboard will allow. So hold on while I do that.</p> <p>Ok, so I installed the 32 GB stick and changed the data loader to lazy caching. As it turns out it ends up using around 64 GB if you want to cache all of the data. Which results in heavy disk activity and swapping. As it were, I forgot to take into account that if you load N GB of data, it will take up KN GB of memory. A rule of thumb is that K is between 2 and 3. The new function looks like this -</p> <p></p>  The data loader.  <p>This results in a severe amount of thrashing where a part of the memory is kept on disk and a continual swapping between memory and disk and disk and memory takes place, which absolutely tanks performance. Let's see whether putting in another stick of RAM yields better results.</p> <p>As it turns out, I ordered another 32GB stick, but the online store cancelled by order as it was no longer being made. This is probably the end of the road for this case study. Training on less data, a reduction of 50% might make it cacheable in memory, seems like cheating. I'll try and come back with a different one at some point. One thing I would like to say is, that while the code we started looking at was unoptimized, that wouldn't matter unless you were either running a very beefy desktop rig or actual HPC equipment, in which case having all of the data in memory would be possible. So a point in favour of the person who wrote that tutorial. He catered the code to his audience and made it possible to run on the widest range of platforms.</p>"},{"location":"m4_optimization/s3_exercises/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb Exercises","text":"<p>Try out the profilers relevant to your own system with some sample programs. Now try it with some of your own code from before you started on the guide!</p>"},{"location":"m4_optimization/s3_exercises/#setting-up-a-profiler","title":"Setting Up a Profiler","text":"<p>Find and setup a profiler that works on your system. Find some Rust code that uses the GPU and run it. I recommend either the egui-winit-template or the computational graphs framework.</p> <ul> <li>Does it work with Rust?</li> <li>Which metrics can you get out of it?</li> <li>Can you see what is happening on the GPU?</li> <li>Do you need a different profiler to see what is going on on the GPU?</li> <li>Can you see L2 cache hits/misses?</li> <li>Can it show you the hot spots in your code?</li> </ul>"},{"location":"m4_optimization/s3_exercises/#group-discussion-and-presentation","title":"\ud83e\uddec Group discussion and presentation","text":"<p>Pick one of the following topics. Read and understand it, then present and discuss the topic with one or more other people. Preferably classmates.</p> <ul> <li>Inverse depth buffers</li> <li>Bit tricks, atomic operators, packing normals and colors</li> <li>Morton codes / Z-order curves, tiling and GPU textures</li> <li>PyTorch 2.0 Compiler</li> <li>Graph Sampling</li> <li>DLSS</li> <li>Real-Time Texture Decompression and Upsampling</li> <li>2:4 sparsity with Tensor Cores</li> </ul>"},{"location":"m5_real_time_systems/","title":"3\ufe0f\u20e3 Real-Time Systems","text":"<ul> <li>Events</li> <li>Key and Mouse events</li> <li>Event Loops</li> <li>GUIs &amp; egui</li> <li>Bindings - PyO3 and cxx, maturin</li> <li>Architecture and Design</li> <li>Logging</li> <li>Tips and Tricks</li> </ul>"},{"location":"m5_real_time_systems/sx_tips_and_tricks/","title":"Tips and Tricks","text":"<ul> <li>memcpy</li> <li>Check/validate everything before the hot loop</li> <li>Unitialized memory</li> <li>Hot loops, event loops</li> <li>Allocations in a hot loop</li> <li>Object Pools</li> <li>System calls - hoist out of the hot loop</li> <li>Logging and printing</li> <li>Walk, don't run, testing for correctness before optimization</li> <li>Don't use abbreviations</li> <li>Don't talk moon man language to me, ya Blargon!</li> <li>Don't use postfix incrementation++</li> <li>When to care about software engineering and when to care about performance</li> <li>Don't use a string key/identifier or integer, when a type safe enum will do the job</li> <li>Hard coding types</li> <li>Cognitive load, and delaying errors to after the first draft - deliberate development vs. debugging</li> <li>Prefer stateless programming, minimize stateful programming (functional inspiration)</li> <li>Implicit casting</li> <li>Templating</li> <li>Know your system - mobile, laptop, desktop, integrated memory, which GPU</li> <li>Use version control even for solo development</li> <li>Am I copying/cloning things that don't need to be copied?</li> <li>Anything that can be immutable, should be immutable - aliasing!</li> <li>Testing and Seeding RNG's</li> <li>Faster RNG</li> <li>Timing real-time systems and how to escape or offload compute</li> <li>Multi-resolution computing for making your real-time target, video streaming and image loading</li> <li>Pressure testing and worst cases</li> <li>Static/Dynamic Dispatch - dyn, enum, trait</li> <li>The Markov Chain</li> <li>If using recursion and risking stack overflow, use a loop and a queue</li> <li>If you can, always prefer an array over more complicated structures</li> <li>Sorting and random access - GyroDropout, Sorting Functions benchmark</li> <li>Is there contention around synchronization primtives such as mutexes?</li> </ul>"},{"location":"m6_projects/","title":"\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb\ud83e\uddec Projects","text":""},{"location":"m6_projects/#how-to-create-real-time-systems-good-frameworks-for-the-different-fields-and-project-proposals","title":"How to create real time systems, good frameworks for the different fields and project proposals","text":"<ul> <li>Starting with a simple prototype</li> <li>Identify your components</li> <li>Single threaded correct implementation -&gt; Testing to avoid regression</li> <li>Optimize</li> </ul>"},{"location":"m6_projects/#what-makes-for-a-good-project","title":"What makes for a good project?","text":"<ul> <li>What is your concept/project?</li> <li>Which concepts from the previous material do you think are relevant to your project and why?</li> <li>Preprocessing your data?</li> <li>How do you adapt to your chosen/available platform?</li> <li>Which libraries did you choose for this problem?</li> <li>How fast did you get to your minimum viable product?</li> <li>Which steps did you take from there and why?</li> <li>How did you determine which parts of your system to optimize?</li> <li>What else would you like to do with your system?</li> </ul>"},{"location":"m6_projects/#project-proposals","title":"Project Proposals","text":"<ul> <li>Virtual 3D scanner for a point cloud dataset</li> <li>EEG system</li> <li>Change the latent variables in a network using GUI, optimize the network</li> <li>Point cloud renderer</li> <li>Real-time style transfer on a web cam feed</li> <li>Rendering fractals influenced by a web cam feed</li> <li>Eye tracking -&gt; Present to screen and read from web cam -&gt; feature extraction -&gt; classifier -&gt; intervention signal -&gt; reading app (Wolfgang Fuhl, PISTOL, fixation detection)</li> <li>Bird classification from sound / Real-time classification of sound (Xeno-canto database)</li> <li>Who is talking? Real-time classification of sound</li> <li>Are you dyslexic? Eye tracking classifier</li> <li>Cognitive load tracker - Eyes &amp; pupil dilation and online estimation of signal strength (pupils vs. sound for the hearing impaired)</li> </ul>"},{"location":"m6_projects/#components-librariesframeworks","title":"Components - libraries/frameworks","text":"<p>blessed rayon egui wonnx tch winit cv ultraviolet arewelearningyet burn time chrono hifitime </p>"}]}